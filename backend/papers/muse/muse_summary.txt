The paper presents Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space, where it predicts randomly masked image tokens given the text embedding extracted from a pre-trained large language model (LLM). The model consists of several sub-models, including a pair of VQGAN "tokenizer" models, a base masked image model, and a super-resolution model. The base model takes a sequence of partially masked low-res tokens and predicts the marginal distribution for each masked token, conditioned on the unmasked tokens and a T5-XXL text embedding. The super-resolution model translates the lower-resolution latent map to the higher-resolution latent map, followed by decoding through the higher-resolution VQGAN to give the final high-resolution image. Muse achieves a new state-of-the-art on CC3M, with an FID score of 6.06, and a CLIP score of 0.32 on zero-shot COCO evaluation. The model is also faster than comparable models, with an inference time of 0.5s for 256x256 images and 1.3s for 512x512 images.