text,summary
"Journal of Arti(cid:12)cial Intelligence Research 1 (1993) 25-46

Submitted 7/93; published 8/93

Dynamic Backtracking

Matthew L. Ginsberg

ginsberg@cs.uoregon.edu

CIRL, University of Oregon,

Eugene, OR 97403-1269 USA

Abstract

Because of their occasional need to return to shallow points in a search tree, existing

backtracking methods can sometimes erase meaningful progress toward solving a search

problem.

In this paper, we present a method by which backtrack points can be moved

deeper in the search space, thereby avoiding this di(cid:14)culty. The technique developed is

a variant of dependency-directed backtracking that uses only polynomial space while still

providing useful control information and retaining the completeness guarantees provided

by earlier approaches.

1. Introduction

Imagine that you are trying to solve some constraint-satisfaction problem, or csp. In the

interests of de(cid:12)niteness, I will suppose that the csp in question involves coloring a map of

the United States sub ject to the restriction that adjacent states be colored di(cid:11)erently.

Imagine we begin by coloring the states along the Mississippi, thereby splitting the

remaining problem in two. We now begin to color the states in the western half of the

country, coloring perhaps half a dozen of them before deciding that we are likely to be able

to color the rest. Suppose also that the last state colored was Arizona.

At this point, we change our focus to the eastern half of the country. After all, if we can't

color the eastern half because of our coloring choices for the states along the Mississippi,

there is no point in wasting time completing the coloring of the western states.

We successfully color the eastern states and then return to the west. Unfortunately, we

color New Mexico and Utah and then get stuck, unable to color (say) Nevada. What's more,

backtracking doesn't help, at least in the sense that changing the colors for New Mexico

and Utah alone does not allow us to proceed farther. Depth-(cid:12)rst search would now have

us backtrack to the eastern states, trying a new color for (say) New York in the vain hope

that this would solve our problems out West.

This is obviously pointless; the blockade along the Mississippi makes it impossible for

New York to have any impact on our attempt to color Nevada or other western states.

What's more, we are likely to examine every possible coloring of the eastern states before

addressing the problem that is actually the source of our di(cid:14)culties.

The solutions that have been proposed to this involve (cid:12)nding ways to backtrack directly

to some state that might actually allow us to make progress, in this case Arizona or earlier.

Dependency-directed backtracking (Stallman & Sussman, 1977) involves a direct backtrack

to the source of the di(cid:14)culty; backjumping (Gaschnig, 1979) avoids the computational over-

head of this technique by using syntactic methods to estimate the point to which backtrack

is necessary.

(cid:13)1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Ginsberg

In both cases, however, note that although we backtrack to the source of the problem,

we backtrack over our successful solution to half of the original problem, discarding our

solution to the problem of coloring the states in the East. And once again, the problem is

worse than this { after we recolor Arizona, we are in danger of solving the East yet again

before realizing that our new choice for Arizona needs to be changed after all. We won't

examine every possible coloring of the eastern states, but we are in danger of rediscovering

our successful coloring an exponential number of times.

This hardly seems sensible; a human problem solver working on this problem would

simply ignore the East if possible, returning directly to Arizona and proceeding. Only if the

states along the Mississippi needed new colors would the East be reconsidered { and even

then only if no new coloring could be found for the Mississippi that was consistent with the

eastern solution.

In this paper we formalize this technique, presenting a modi(cid:12)cation to conventional

search techniques that is capable of backtracking not only to the most recently expanded

node, but also directly to a node elsewhere in the search tree. Because of the dynamic way

in which the search is structured, we refer to this technique as dynamic backtracking.

A more speci(cid:12)c outline is as follows: We begin in the next section by introducing a

variety of notational conventions that allow us to cast both existing work and our new

ideas in a uniform computational setting. Section 3 discusses backjumping, an intermediate

between simple chronological backtracking and our ideas, which are themselves presented

in Section 4. An example of the dynamic backtracking algorithm in use appears in Section

5 and an experimental analysis of the technique in Section 6. A summary of our results and

suggestions for future work are in Section 7. All proofs have been deferred to an appendix

in the interests of continuity of exposition.

2. Preliminaries

De(cid:12)nition 2.1 By a constraint satisfaction problem (I ; V ; (cid:20)) we wil l mean a set I of vari-

ables; for each i 2 I , there is a set V

of possible values for the variable i. (cid:20) is a set of

i

constraints, each a pair (J; P ) where J = (j

; . . . ; j

) is an ordered subset of I and P is a

1

k

subset of V

(cid:2) (cid:1) (cid:1) (cid:1) (cid:2) V

.

j

1

j

k

A solution to the csp is a set v

of values for each of the variables in I such that v

2 V

i

i

i

for each i and for every constraint (J; P ) of the above form in (cid:20), (v

; . . . ; v

) 2 P .

j

j

1

k

In the example of the introduction, I is the set of states and V

is the set of possible

i

colors for the state i. For each constraint, the (cid:12)rst part of the constraint is a pair of adjacent

states and the second part is a set of allowable color combinations for these states.

Our basic plan in this paper is to present formal versions of the search algorithms

described in the introduction, beginning with simple depth-(cid:12)rst search and proceeding to

backjumping and dynamic backtracking. As a start, we make the following de(cid:12)nition of a

partial solution to a csp:

De(cid:12)nition 2.2 Let (I ; V ; (cid:20)) be a csp. By a partial solution to the csp we mean an ordered

subset J (cid:18) I and an assignment of a value to each variable in J .

26

Dynamic Backtracking

We wil l denote a partial solution by a tuple of ordered pairs, where each ordered pair

(i; v ) assigns the value v to the variable i. For a partial solution P , we wil l denote by P the

set of variables assigned values by P .

Constraint-satisfaction problems are solved in practice by taking partial solutions and

extending them by assigning values to new variables. In general, of course, not any value can

be assigned to a variable because some are inconsistent with the constraints. We therefore

make the following de(cid:12)nition:

De(cid:12)nition 2.3 Given a partial solution P to a csp, an eliminating explanation for a

variable i is a pair (v ; S ) where v 2 V

and S (cid:18) P . The intended meaning is that i

i

cannot take the value v because of the values already assigned by P to the variables in S .

An elimination mechanism (cid:15) for a csp is a function that accepts as arguments a partial

solution P , and a variable i 62 P . The function returns a (possibly empty) set (cid:15)(P; i) of

eliminating explanations for i.

For a set E of eliminating explanations, we will denote by

E the values that have been

b

identi(cid:12)ed as eliminated, ignoring the reasons given. We therefore denote by

(cid:15)(P; i) the set

b

of values eliminated by elements of (cid:15)(P; i).

Note that the above de(cid:12)nition is somewhat (cid:13)exible with regard to the amount of work

done by the elimination mechanism { all values that violate completed constraints might

be eliminated, or some amount of lookahead might be done. We will, however, make the

following assumptions about all elimination mechanisms:

1. They are correct. For a partial solution P , if the value v

62

(cid:15)(P; i), then every

b

i

constraint (S; T ) in (cid:20) with S (cid:18) P [ fig is satis(cid:12)ed by the values in the partial solution

and the value v

for i. These are the constraints that are complete after the value v

i

i

is assigned to i.

2. They are complete. Suppose that P is a partial solution to a csp, and there is some

solution that extends P while assigning the value v to i. If P

is an extension of P

0

with (v ; E ) 2 (cid:15)(P

; i), then

0

E \ (P

(cid:0) P ) 6= (cid:31)

(1)

0

In other words, whenever P can be successfully extended after assigning v to i but

0

0

P

cannot be, at least one element of P

(cid:0) P is identi(cid:12)ed as a possible reason for the

problem.

3. They are concise. For a partial solution P , variable i and eliminated value v , there

is at most a single element of the form (v ; E ) 2 (cid:15)(P; i). Only one reason is given why

the variable i cannot have the value v .

Lemma 2.4 Let (cid:15) be a complete elimination mechanism for a csp, let P be a partial solu-

tion to this csp and let i 62 P . Now if P can be successful ly extended to a complete solution

after assigning i the value v , then v 62

(cid:15)(P; i).

b

I apologize for the swarm of de(cid:12)nitions, but they allow us to give a clean description of

depth-(cid:12)rst search:

27

Ginsberg

Algorithm 2.5 (Depth-(cid:12)rst search) Given as inputs a constraint-satisfaction problem

and an elimination mechanism (cid:15):

1. Set P = (cid:31). P is a partial solution to the csp. Set E

= (cid:31) for each i 2 I ; E

is the

i

i

set of values that have been eliminated for the variable i.

2. If P = I , so that P assigns a value to every element in I , it is a solution to the

original problem. Return it. Otherwise, select a variable i 2 I (cid:0) P . Set E

=

(cid:15)(P; i),

b

i

the values that have been eliminated as possible choices for i.

3. Set S = V

(cid:0) E

, the set of remaining possibilities for i. If S is nonempty, choose an

i

i

element v 2 S . Add (i; v ) to P , thereby setting i's value to v , and return to step 2.

4. If S is empty, let (j; v

) be the last entry in P ; if there is no such entry, return failure.

j

Remove (j; v

) from P , add v

to E

, set i = j and return to step 3.

j

j

j

We have written the algorithm so that it returns a single answer to the csp; the modi-

(cid:12)cation to accumulate all such answers is straightforward.

The problem with Algorithm 2.5 is that it looks very little like conventional depth-(cid:12)rst

search, since instead of recording the unexpanded children of any particular node, we are

keeping track of the failed siblings of that node. But we have the following:

Lemma 2.6 At any point in the execution of Algorithm 2.5, if the last element of the partial

solution P assigns a value to the variable i, then the unexplored siblings of the current node

are those that assign to i the values in V

(cid:0) E

.

i

i

Proposition 2.7 Algorithm 2.5 is equivalent to depth-(cid:12)rst search and therefore complete.

As we have remarked, the basic di(cid:11)erence between Algorithm 2.5 and a more conven-

tional description of depth-(cid:12)rst search is the inclusion of the elimination sets E

. The

i

conventional description expects nodes to include pointers back to their parents; the sib-

lings of a given node are found by examining the children of that node's parent. Since we

will be reorganizing the space as we search, this is impractical in our framework.

It might seem that a more natural solution to this di(cid:14)culty would be to record not the

values that have been eliminated for a variable i, but those that remain to be considered.

The technical reason that we have not done this is that it is much easier to maintain

elimination information as the search progresses. To understand this at an intuitive level,

note that when the search backtracks, the conclusion that has implicitly been drawn is

that a particular node fails to expand to a solution, as opposed to a conclusion about the

currently unexplored portion of the search space. It should be little surprise that the most

e(cid:14)cient way to manipulate this information is by recording it in approximately this form.

3. Backjumping

How are we to describe dependency-directed backtracking or backjumping in this setting?

In these cases, we have a partial solution and have been forced to backtrack; these more

sophisticated backtracking mechanisms use information about the reason for the failure to

identify backtrack points that might allow the problem to be addressed. As a start, we need

to modify Algorithm 2.5 to maintain the explanations for the eliminated values:

28

Dynamic Backtracking

Algorithm 3.1 Given as inputs a constraint-satisfaction problem and an elimination mech-

anism (cid:15):

1. Set P = E

= (cid:31) for each i 2 I . E

is a set of eliminating explanations for i.

i

i

2. If P = I , return P . Otherwise, select a variable i 2 I (cid:0) P . Set E

= (cid:15)(P; i):

i

3. Set S = V

(cid:0)

E

. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and

i

i

b

return to step 2.

4. If S is empty, let (j; v

) be the last entry in P ; if there is no such entry, return failure.

j

Remove (j; v

) from P . We must have

E

= V

, so that every value for i has been

j

i

i

b

eliminated; let E be the set of al l variables appearing in the explanations for each

eliminated value. Add (v

; E (cid:0) fj g) to E

, set i = j and return to step 3.

j

j

Lemma 3.2 Let P be a partial solution obtained during the execution of Algorithm 3.1,

and let i 2 P be a variable assigned a value by P . Now if P

(cid:18) P can be successful ly

0

extended to a complete solution after assigning i the value v but (v ; E ) 2 E

, we must have

i

E \ (P (cid:0) P

) 6= (cid:31)

0

In other words, the assignment of a value to some variable in P (cid:0)P

is correctly identi(cid:12)ed

0

as the source of the problem.

Note that in step 4 of the algorithm, we could have added (v

; E \ P ) instead of (v

; E (cid:0)

j

j

fj g) to E

; either way, the idea is to remove from E any variables that are no longer assigned

j

values by P .

In backjumping, we now simply change our backtrack method; instead of removing a

single entry from P and returning to the variable assigned a value prior to the problematic

variable i, we return to a variable that has actually had an impact on i. In other words, we

return to some variable in the set E .

Algorithm 3.3 (Backjumping) Given as inputs a constraint-satisfaction problem and an

elimination mechanism (cid:15):

1. Set P = E

= (cid:31) for each i 2 I .

i

2. If P = I , return P . Otherwise, select a variable i 2 I (cid:0) P . Set E

= (cid:15)(P; i):

i

3. Set S = V

(cid:0)

E

. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and

i

i

b

return to step 2.

4. If S is empty, we must have

E

= V

. Let E be the set of al l variables appearing in

i

i

b

the explanations for each eliminated value.

5. If E = (cid:31), return failure. Otherwise, let (j; v

) be the last entry in P such that j 2 E .

j

Remove from P this entry and any entry fol lowing it. Add (v

; E \ P ) to E

, set i = j

j

j

and return to step 3.

29

Ginsberg

In step 5, we add (v

; E \ P ) to E

, removing from E any variables that are no longer

j

j

assigned values by P .

Proposition 3.4 Backjumping is complete and always expands fewer nodes than does depth-

(cid:12)rst search.

Let us have a look at this in our map-coloring example. If we have a partial coloring

P and are looking at a speci(cid:12)c state i, suppose that we denote by C the set of colors that

are obviously illegal for i because they con(cid:13)ict with a color already assigned to one of i's

neighbors.

One possible elimination mechanism returns as (cid:15)(P; i) a list of (c; P ) for each color

c 2 C that has been used to color a neighbor of i. This reproduces depth-(cid:12)rst search, since

we gradually try all possible colors but have no idea what went wrong when we need to

backtrack since every colored state is included in P . A far more sensible choice would take

(cid:15)(P; i) to be a list of (c; fng) where n is a neighbor that is already colored c. This would

ensure that we backjump to a neighbor of i if no coloring for i can be found.

If this causes us to backjump to another state j , we will add i's neighbors to the elim-

inating explanation for j 's original color, so that if we need to backtrack still further, we

consider neighbors of either i or j . This is as it should be, since changing the color of one of

i's other neighbors might allow us to solve the coloring problem by reverting to our original

choice of color for the state j .

We also have:

Proposition 3.5 The amount of space needed by backjumping is o(i

v ), where i = jI j is

2

the number of variables in the problem and v is the number of values for that variable with

the largest value set V

.

i

This result contrasts sharply with an approach to csps that relies on truth-maintenance

techniques to maintain a list of nogoods (de Kleer, 1986). There, the number of nogoods

found can grow linearly with the time taken for the analysis, and this will typically be

exponential in the size of the problem. Backjumping avoids this problem by resetting the

set E

of eliminating explanations in step 2 of Algorithm 3.3.

i

The description that we have given is quite similar to that developed in (Bruynooghe,

1981). The explanations there are somewhat coarser than ours, listing all of the variables

that have been involved in any eliminating explanation for a particular variable in the csp,

but the idea is essentially the same. Bruynooghe's eliminating explanations can be stored

in o(i

) space (instead of o(i

v )), but the associated loss of information makes the technique

2

2

less e(cid:11)ective in practice. This earlier work is also a description of backjumping only, since

intermediate information is erased as the search proceeds.

4. Dynamic backtracking

We (cid:12)nally turn to new results. The basic problem with Algorithm 3.3 is not that it back-

jumps to the wrong place, but that it needlessly erases a great deal of the work that has

been done thus far. At the very least, we can retain the values selected for variables that

are backjumped over, in some sense moving the backjump variable to the end of the partial

30

Dynamic Backtracking

solution in order to replace its value without modifying the values of the variables that

followed it.

There is an additional modi(cid:12)cation that will probably be clearest if we return to the

example of the introduction. Suppose that in this example, we color only some of the eastern

states before returning to the western half of the country. We reorder the variables in order

to backtrack to Arizona and eventually succeed in coloring the West without disturbing the

colors used in the East.

Unfortunately, when we return East backtracking is required and we (cid:12)nd ourselves

needing to change the coloring on some of the eastern states with which we dealt earlier.

The ideas that we have presented will allow us to avoid erasing our solution to the problems

out West, but if the search through the eastern states is to be e(cid:14)cient, we will need to

retain the information we have about the portion of the East's search space that has been

eliminated. After all, if we have determined that New York cannot be colored yellow, our

changes in the West will not reverse this conclusion { the Mississippi really does isolate one

section of the country from the other.

The machinery needed to capture this sort of reasoning is already in place. When we

backjump over a variable k, we should retain not only the choice of value for k, but also k's

elimination set. We do, however, need to remove from this elimination set any entry that

involves the eventual backtrack variable j , since these entries are no longer valid { they

depend on the assumption that j takes its old value, and this assumption is now false.

Algorithm 4.1 (Dynamic backtracking I) Given as inputs a constraint-satisfaction prob-

lem and an elimination mechanism (cid:15):

1. Set P = E

= (cid:31) for each i 2 I .

i

2. If P = I , return P . Otherwise, select a variable i 2 I (cid:0) P . Set E

= E

[ (cid:15)(P; i).

i

i

3. Set S = V

(cid:0)

E

. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and

i

i

b

return to step 2.

4. If S is empty, we must have

E

= V

; let E be the set of al l variables appearing in the

i

i

b

explanations for each eliminated value.

5. If E = (cid:31), return failure. Otherwise, let (j; v

) be the last entry in P such that j 2 E .

j

Remove (j; v

) from P and, for each variable k assigned a value after j , remove from

j

E

any eliminating explanation that involves j . Set

k

E

= E

[ (cid:15)(P; j ) [ f(v

; E \ P )g

(2)

j

j

j

so that v

is eliminated as a value for j because of the values taken by variables in

j

E \ P . The inclusion of the term (cid:15)(P; j ) incorporates new information from variables

that have been assigned values since the original assignment of v

to j . Now set i = j

j

and return to step 3.

Theorem 4.2 Dynamic backtracking always terminates and is complete. It continues to

satisfy Proposition 3.5 and can be expected to expand fewer nodes than backjumping provided

that the goal nodes are distributed randomly in the search space.

31

Ginsberg

The essential di(cid:11)erence between dynamic and dependency-directed backtracking is that

the structure of our eliminating explanations means that we only save nogood information

based on the current values of assigned variables; if a nogood depends on outdated infor-

mation, we drop it. By doing this, we avoid the need to retain an exponential amount of

nogood information. What makes this technique valuable is that (as stated in the theorem)

termination is still guaranteed.

There is one trivial modi(cid:12)cation that we can make to Algorithm 4.1 that is quite useful

in practice. After removing the current value for the backtrack variable j , Algorithm 4.1

immediately replaces it with another. But there is no real reason to do this; we could

instead pick a value for an entirely di(cid:11)erent variable:

Algorithm 4.3 (Dynamic backtracking) Given as inputs a constraint-satisfaction prob-

lem and an elimination mechanism (cid:15):

1. Set P = E

= (cid:31) for each i 2 I .

i

2. If P = I , return P . Otherwise, select a variable i 2 I (cid:0) P . Set E

= E

[ (cid:15)(P; i).

i

i

3. Set S = V

(cid:0)

E

. If S is nonempty, choose an element v 2 S . Add (i; v ) to P and

i

i

b

return to step 2.

4. If S is empty, we must have

E

= V

; let E be the set of al l variables appearing in the

i

i

b

explanations for each eliminated value.

5. If E = (cid:31), return failure. Otherwise, let (j; v

) be the last entry in P that binds a

j

variable appearing in E . Remove (j; v

) from P and, for each variable k assigned

j

a value after j , remove from E

any eliminating explanation that involves j . Add

k

(v

; E \ P ) to E

and return to step 2.

j

j

5. An example

In order to make Algorithm 4.3 a bit clearer, suppose that we consider a small map-

coloring problem in detail. The map is shown in Figure 1 and consists of (cid:12)ve countries:

Albania, Bulgaria, Czechoslovakia, Denmark and England. We will assume (wrongly!) that

the countries border each other as shown in the (cid:12)gure, where countries are denoted by nodes

and border one another if and only if there is an arc connecting them.

In coloring the map, we can use the three colors red, yellow and blue. We will typically

abbreviate the country names to single letters in the obvious way.

We begin our search with Albania, deciding (say) to color it red. When we now look at

Bulgaria, no colors are eliminated because Albania and Bulgaria do not share a border; we

decide to color Bulgaria yellow. (This is a mistake.)

We now go on to consider Czechoslovakia; since it borders Albania, the color red is

eliminated. We decide to color Czechoslovakia blue and the situation is now this:

32

Dynamic Backtracking

Denmark

s

(cid:0)@

(cid:0)

(cid:0)

@

(cid:0)

@

(cid:0)

@

(cid:0)

@

s

(cid:0)
s

@

s

Bulgaria

Albania

(cid:0)

@

@

Czechoslovakia

@

(cid:0)

(cid:0)

@

@

(cid:0)

(cid:0)

@

(cid:0)

@

(cid:0)

@

(cid:0)

@

@(cid:0)

s

England

Figure 1: A small map-coloring problem

country

color

red

yellow blue

Albania

red

Bulgaria

yellow

Czechoslovakia

blue

A

Denmark

England

For each country, we indicate its current color and the eliminating explanations that mean

it cannot be colored each of the three colors (when such explanations exist). We now look

at Denmark.

Denmark cannot be colored red because of its border with Albania and cannot be colored

yellow because of its border with Bulgaria; it must therefore be colored blue. But now

England cannot be colored any color at all because of its borders with Albania, Bulgaria

and Denmark, and we therefore need to backtrack to one of these three countries. At this

point, the elimination lists are as follows:

country

color

red

yellow blue

Albania

red

Bulgaria

yellow

Czechoslovakia

blue

A

Denmark

blue

A

B

England

A

B

D

We backtrack to Denmark because it is the most recent of the three possibilities, and

begin by removing any eliminating explanation involving Denmark from the above table to

get:

33

Ginsberg

country

color

red

yellow blue

Albania

red

Bulgaria

yellow

Czechoslovakia

blue

A

Denmark

A

B

England

A

B

Next, we add to Denmark's elimination list the pair

(blue; fA; Bg)

This indicates correctly that because of the current colors for Albania and Bulgaria, Den-

mark cannot be colored blue (because of the subsequent dead end at England). Since every

color is now eliminated, we must backtrack to a country in the set fA; Bg. Changing

Czechoslovakia's color won't help and we must deal with Bulgaria instead. The elimination

lists are now:

country

color

red

yellow blue

Albania

red

Bulgaria

Czechoslovakia

blue

A

Denmark

A

B

A,B

England

A

B

We remove the eliminating explanations involving Bulgaria and also add to Bulgaria's elim-

ination list the pair

(yellow; A)

indicating correctly that Bulgaria cannot be colored yellow because of the current choice of

color for Albania (red).

The situation is now:

country

color

red

yellow blue

Albania

red

Czechoslovakia

blue

A

Bulgaria

A

Denmark

A

England

A

We have moved Bulgaria past Czechoslovakia to re(cid:13)ect the search reordering in the algo-

rithm. We can now complete the problem by coloring Bulgaria red, Denmark either yellow

or blue, and England the color not used for Denmark.

This example is almost trivially simple, of course; the thing to note is that when we

changed the color for Bulgaria, we retained both the blue color for Czechoslovakia and the

information indicating that none of Czechoslovakia, Denmark and England could be red.

In more complex examples, this information may be very hard-won and retaining it may

save us a great deal of subsequent search e(cid:11)ort.

Another feature of this speci(cid:12)c example (and of the example of the introduction as

well) is that the computational bene(cid:12)ts of dynamic backtracking are a consequence of

34

Dynamic Backtracking

the automatic realization that the problem splits into disjoint subproblems. Other authors

have also discussed the idea of applying divide-and-conquer techniques to csps (Seidel, 1981;

Zabih, 1990), but their methods su(cid:11)er from the disadvantage that they constrain the order in

which unassigned variables are assigned values, perhaps at odds with the common heuristic

of assigning values (cid:12)rst to those variables that are most tightly constrained. Dynamic

backtracking can also be expected to be of use in situations where the problem in question

does not split into two or more disjoint subproblems.

1

6. Experimentation

Dynamic backtracking has been incorporated into the crossword-puzzle generation program

described in (Ginsberg, Frank, Halpin, & Torrance, 1990), and leads to signi(cid:12)cant perfor-

mance improvements in that restricted domain. More speci(cid:12)cally, the method was tested

on the problem of generating 19 puzzles of sizes ranging from 2 (cid:2) 2 to 13 (cid:2) 13; each puzzle

was attempted 100 times using both dynamic backtracking and simple backjumping. The

dictionary was shu(cid:15)ed between solution attempts and a maximum of 1000 backtracks were

permitted before the program was deemed to have failed.

In both cases, the algorithms were extended to include iterative broadening (Ginsberg

& Harvey, 1992), the cheapest-(cid:12)rst heuristic and forward checking. Cheapest-(cid:12)rst has

also been called \most constrained (cid:12)rst"" and selects for instantiation that variable with

the fewest number of remaining possibilities (i.e., that variable for which it is cheapest to

enumerate the possible values (Smith & Genesereth, 1985)). Forward checking prunes the

set of possibilities for crossing words whenever a new word is entered and constitutes our

experimental choice of elimination mechanism: at any point, words for which there is no legal

crossing word are eliminated. This ensures that no word will be entered into the crossword

if the word has no potential crossing words at some point. The cheapest-(cid:12)rst heuristic

would identify the problem at the next step in the search, but forward checking reduces

the number of backtracks substantially. The \least-constraining"" heuristic (Ginsberg et al.,

1990) was not used; this heuristic suggests that each word slot be (cid:12)lled with the word that

minimally constrains the subsequent search. The heuristic was not used because it would

invalidate the technique of shu(cid:15)ing the dictionary between solution attempts in order to

gather useful statistics.

The table in Figure 2 indicates the number of successful solution attempts (out of 100)

for each of the two methods on each of the 19 crossword frames. Dynamic backtracking is

more successful in six cases and less successful in none.

With regard to the number of nodes expanded by the two methods, consider the data

presented in Figure 3, where we graph the average number of backtracks needed by the

two methods.

Although initially comparable, dynamic backtracking provides increasing

2

computational savings as the problems become more di(cid:14)cult. A somewhat broader set of

experiments is described in (Jonsson & Ginsberg, 1993) and leads to similar conclusions.

There are some examples in (Jonsson & Ginsberg, 1993) where dynamic backtracking

leads to performance degradation, however; a typical case appears in Figure 4.

In this

3

1. I am indebted to David McAllester for these observations.

2. Only 17 points are shown because no point is plotted where backjumping was unable to solve the problem.

3. The worst performance degradation observed was a factor of approximately 4.

35

Ginsberg

Dynamic

Dynamic

Frame backtracking Backjumping Frame backtracking Backjumping

1

100

100

11

100

98

2

100

100

12

100

100

3

100

100

13

100

100

4

100

100

14

100

100

5

100

100

15

99

14

6

100

100

16

100

26

7

100

100

17

100

30

8

100

100

18

61

0

9

100

100

19

10

0

10

100

100

Figure 2: Number of problems solved successfully

400

dynamic

200

backtracking

r

r

r

r

r

r

r

r

r

r

rr

r

r

r

r

r

200

400

600

800

1000

backjumping

Figure 3: Number of backtracks needed

36

Dynamic Backtracking

Region 1

s

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

(cid:0)

s

(cid:0)
s

a

a

a

A

a

@

B

(cid:0)

a

@

a

a

a

a

@

a

@

a

a

a

@

a

a

@

a

a

@

a

@
a

s

Region 2

Figure 4: A di(cid:14)cult problem for dynamic backtracking

(cid:12)gure, we (cid:12)rst color A, then B , then the countries in region 1, and then get stuck in region

2.

We now presumably backtrack directly to B , leaving the coloring of region 1 alone. But

this may well be a mistake { the colors in region 1 will restrict our choices for B , perhaps

making the subproblem consisting of A, B and region 2 more di(cid:14)cult than it might be. If

region 1 were easy to color, we would have been better o(cid:11) erasing it even though we didn't

need to.

This analysis suggests that dependency-directed backtracking should also fare worse

on those coloring problems where dynamic backtracking has trouble, and we are currently

extending the experiments of (Jonsson & Ginsberg, 1993) to con(cid:12)rm this. If this conjecture

is borne out, a variety of solutions come to mind. We might, for example, record how

many backtracks are made to a node such as B in the above (cid:12)gure, and then use this to

determine that (cid:13)exibility at B is more important than retaining the choices made in region

1. The di(cid:14)culty of (cid:12)nding a coloring for region 1 can also be determined from the number

of backtracks involved in the search.

7. Summary

7.1 Why it works

There are two separate ideas that we have exploited in the development of Algorithm 4.3

and the others leading up to it. The (cid:12)rst, and easily the most important, is the notion

that it is possible to modify variable order on the (cid:13)y in a way that allows us to retain the

results of earlier work when backtracking to a variable that was assigned a value early in

the search.

37

Ginsberg

This reordering should not be confused with the work of authors who have suggested a

dynamic choice among the variables that remain to be assigned values (Dechter & Meiri,

1989; Ginsberg et al., 1990; P. Purdom & Robertson, 1981; Zabih & McAllester, 1988); we

are instead reordering the variables that have been assigned values in the search thus far.

Another way to look at this idea is that we have found a way to \erase"" the value given

to a variable directly as opposed to backtracking to it. This idea has also been explored

by Minton et.al. in (Minton, Johnston, Philips, & Laird, 1990) and by Selman et.al. in

(Selman, Levesque, & Mitchell, 1992); these authors also directly replace values assigned

to variables in satis(cid:12)ability problems. Unfortunately, the heuristic repair method used is

incomplete because no dependency information is retained from one state of the problem

solver to the next.

There is a third way to view this as well. The space that we are examining is really a

graph, as opposed to a tree; we reach the same point by coloring Albania blue and then

Bulgaria red as if we color them in the opposite order. When we decide to backjump from a

particular node in the search space, we know that we need to back up until some particular

property of that node ceases to hold { and the key idea is that by backtracking along a

path other than the one by which the node was generated, we may be able to backtrack

only slightly when we would otherwise need to retreat a great deal. This observation is

interesting because it may well apply to problems other than csps. Unfortunately, it is not

clear how to guarantee completeness for a search that discovers a node using one path and

backtracks using another.

The other idea is less novel. As we have already remarked, our use of eliminating

explanations is quite similar to the use of nogoods in the atms community; the principal

di(cid:11)erence is that we attach the explanations to the variables they impact and drop them

when they cease to be relevant. (They might become relevant again later, of course.) This

avoids the prohibitive space requirements of systems that permanently cache the results of

their nogood calculations; this observation also may be extensible beyond the domain of

csps speci(cid:12)cally. Again, there are other ways to view this { Gashnig's notion of backmarking

(Gaschnig, 1979) records similar information about the reason that particular portions of a

search space are known not to contain solutions.

7.2 Future work

There are a variety of ways in which the techniques we have presented can be extended; in

this section, we sketch a few of the more obvious ones.

7.2.1 Backtracking to older culprits

One extension to our work involves lifting the restriction in Algorithm 4.3 that the variable

erased always be the most recently assigned member of the set E .

In general, we cannot do this while retaining the completeness of the search. Consider

the following example:

Imagine that our csp involves three variables, x, y and z , that can each take the value 0

or 1. Further, suppose that this csp has no solutions, in that after we pick any two values

for x and for y , we realize that there is no suitable choice for z .

38

Dynamic Backtracking

We begin by taking x = y = 0; when we realize the need to backtrack, we introduce the

nogood

x = 0 (cid:27) y 6= 0

(3)

and replace the value for y with y = 1.

This fails, too, but now suppose that we were to decide to backtrack to x, introducing

the new nogood

y = 1 (cid:27) x 6= 0

(4)

We change x's value to 1 and erase (3).

This also fails. We decide that y is the problem and change its value to 0, introducing

the nogood

x = 1 (cid:27) y 6= 1

but erasing (4). And when this fails, we are in danger of returning to x = y = 0, which we

eliminated at the beginning of the example. This loop may cause a modi(cid:12)ed version of the

dynamic backtracking algorithm to fail to terminate.

In terms of the proof of Theorem 4.2, the nogoods discovered already include information

about all assigned variables, so there is no di(cid:11)erence between (7) and (8). When we drop

(3) in favor of (4), we are no longer in a position to recover (3).

We can deal with this by placing conditions on the variables to which we choose to

backtrack; the conditions need to be de(cid:12)ned so that the proof of Theorem 4.2 continues to

4

hold.

Experimentation indicates that loops of the form we have described are extremely

rare in practice; it may also be possible to detect them directly and thereby retain more

substantial freedom in the choice of backtrack point.

This freedom of backtrack raises an important question that has not yet been addressed

in the literature: When backtracking to avoid a di(cid:14)culty of some sort, to where should one

backtrack?

Previous work has been constrained to backtrack no further than the most recent choice

that might impact the problem in question; any other decision would be both incomplete and

ine(cid:14)cient. Although an extension of Algorithm 4.3 need not operate under this restriction,

we have given no indication of how the backtrack point should be selected.

There are several easily identi(cid:12)ed factors that can be expected to bear on this choice.

The (cid:12)rst is that there remains a reason to expect backtracking to chronologically recent

choices to be the most e(cid:11)ective { these choices can be expected to have contributed to

the fewest eliminating explanations, and there is obvious advantage to retaining as many

eliminating explanations as possible from one point in the search to the next. It is pos-

sible, however, to simply identify that backtrack point that a(cid:11)ects the fewest number of

eliminating explanations and to use that.

Alternatively, it might be important to backtrack to the choice point for which there

will be as many new choices as possible; as an extreme example, if there is a variable i

for which every value other than its current one has already been eliminated for other

reasons, backtracking to i is guaranteed to generate another backtrack immediately and

should probably be avoided if possible.

4. Another solution appears in (McAllester, 1993).

39

Ginsberg

Finally, there is some measure of the \directness"" with which a variable bears on a

problem. If we are unable to (cid:12)nd a value for a particular variable i, it is probably sensible

to backtrack to a second variable that shares a constraint with i itself, as opposed to some

variable that a(cid:11)ects i only indirectly.

How are these competing considerations to be weighed? I have no idea. But the frame-

work we have developed is interesting because it allows us to work on this question. In

more basic terms, we can now \debug"" partial solutions to csps directly, moving laterally

through the search space in an attempt to remain as close to a solution as possible. This

sort of lateral movement seems central to human solution of di(cid:14)cult search problems, and

it is encouraging to begin to understand it in a formal way.

7.2.2 Dependency pruning

It is often the case that when one value for a variable is eliminated while solving a csp,

others are eliminated as well. As an example, in solving a scheduling problem a particular

choice of time (say t = 16) may be eliminated for a task A because there then isn't enough

time between A and a subsequent task B ; in this case, all later times can obviously be

eliminated for A as well.

Formalizing this can be subtle; after all, a later time for A isn't uniformly worse than an

earlier time because there may be other tasks that need to precede A and making A later

makes that part of the schedule easier. It's the problem with B alone that forces A to be

earlier; once again, the analysis depends on the ability to maintain dependency information

as the search proceeds.

We can formalize this as follows. Given a csp (I ; V ; (cid:20)), suppose that the value v has

been assigned to some i 2 I . Now we can construct a new csp (I

; V

; (cid:20)

) involving the

0

0

0

remaining variables I

= I (cid:0) fig, where the new set V

need not mention the possible values

0

0

V

for i, and where (cid:20)

is generated from (cid:20) by modifying the constraints to indicate that i

i

0

has been assigned the value v . We also make the following de(cid:12)nition:

De(cid:12)nition 7.1 Given a csp, suppose that i is a variable that has two possible values u and

v . We wil l say that v is stricter than u if every constraint in the csp induced by assigning

u to i is also a constraint in the csp induced by assigning i the value v .

The point, of course, is that if v is stricter than u is, there is no point to trying a

solution involving v once u has been eliminated. After all, (cid:12)nding such a solution would

involve satisfying all of the constraints in the v restriction, these are a superset of those in

the u restriction, and we were unable to satisfy the constraints in the u restriction originally.

The example with which we began this section now generalizes to the following:

Proposition 7.2 Suppose that a csp involves a set S of variables, and that we have a

partial solution that assigns values to the variables in some subset P (cid:18) S . Suppose further

that if we extend this partial solution by assigning the value u to a variable i 62 P , there is

no further extension to a solution of the entire csp. Now consider the csp involving the

variables in S (cid:0) P that is induced by the choices of values for variables in P . If v is stricter

than u as a choice of value for i in this problem, the original csp has no solution that both

assigns v to i and extends the given partial solution on P .

40

Dynamic Backtracking

This proposition isn't quite enough; in the earlier example, the choice of t = 17 for A

will not be stricter than t = 16 if there is any task that needs to be scheduled before A is.

We need to record the fact that B (which is no longer assigned a value) is the source of the

di(cid:14)culty. To do this, we need to augment the dependency information with which we are

working.

More precisely, when we say that a set of variables fx

g eliminates a value v for a variable

i

x, we mean that our search to date has allowed us to conclude that

where the v

are the current choices for the x

. We can obviously rewrite this as

i

i

(v

= x

) ^ (cid:1) (cid:1) (cid:1) ^ (v

= x

) (cid:27) v 6= x

1

1

k

k

(v

= x

) ^ (cid:1) (cid:1) (cid:1) ^ (v

= x

) ^ (v = x) (cid:27) F

(5)

1

1

k

k

where F indicates that the csp in question has no solution.

Let's be more speci(cid:12)c still, indicating in (5) exactly which csp has no solution:

(v

= x

) ^ (cid:1) (cid:1) (cid:1) ^ (v

= x

) ^ (v = x) (cid:27) F (I )

(6)

1

1

k

k

where I is the set of variables in the complete csp.

Now we can address the example with which we began this section; the csp that is

known to fail in an expression such as (6) is not the entire problem, but only a subset of it.

In the example, we are considering, the subproblem involves only the two tasks A and B .

In general, we can augment our nogoods to include information about the subproblems on

which they fail, and then measure strictness with respect to these restricted subproblems

only. In our example, this will indeed allow us to eliminate t = 17 from consideration as a

possible time for A.

The additional information stored with the nogoods doubles their size (we have to store a

second subset of the variables in the csp), and the variable sets involved can be manipulated

easily as the search proceeds. The cost involved in employing this technique is therefore that

of the strictness computation. This may be substantial given the data structures currently

used to represent csps (which typically support the need to check if a constraint has been

violated but little more), but it seems likely that compile-time modi(cid:12)cations to these data

structures can be used to make the strictness question easier to answer.

In scheduling

problems, preliminary experimental work shows that the idea is an important one; here,

too, there is much to be done.

The basic lesson of dynamic backtracking is that by retaining only those nogoods that

are still relevant given the partial solution with which we are working, the storage di(cid:14)culties

encountered by full dependency-directed methods can be alleviated. This is what makes

all of the ideas we have proposed possible { erasing values, selecting alternate backtrack

points, and dependency pruning. There are surely many other e(cid:11)ective uses for a practical

dependency maintenance system as well.

Acknowledgements

This work has been supported by the Air Force O(cid:14)ce of Scienti(cid:12)c Research under grant

number 92-0693 and by DARPA/Rome Labs under grant number F30602-91-C-0036. I

41

Ginsberg

would like to thank Rina Dechter, Mark Fox, Don Geddis, Will Harvey, Vipin Kumar,

Scott Roy and Narinder Singh for helpful comments on these ideas. Ari Jonsson and

David McAllester provided me invaluable assistance with the experimentation and proofs

respectively.

A. Proofs

Lemma 2.4 Let (cid:15) be a complete elimination mechanism for a csp, let P be a partial solution

to this csp and let i 62 P . Now if P can be successful ly extended to a complete solution after

assigning i the value v , then v 62

(cid:15)(P; i).

b

Proof. Suppose otherwise, so that (v ; E ) 2 (cid:15)(P; i). It follows directly from the completeness

of (cid:15) that

a contradiction.

E \ (P (cid:0) P ) 6= (cid:31)

Lemma 2.6 At any point in the execution of Algorithm 2.5, if the last element of the partial

solution P assigns a value to the variable i, then the unexplored siblings of the current node

are those that assign to i the values in V

(cid:0) E

.

i

i

Proof. We (cid:12)rst note that when we decide to assign a value to a new variable i in step 2

of the algorithm, we take E

=

(cid:15)(P; i) so that V

(cid:0) E

is the set of allowed values for this

b

i

i

i

variable. The lemma therefore holds in this case. The fact that it continues to hold through

each repetition of the loop in steps 3 and 4 is now a simple induction; at each point, we

add to E

the node that has just failed as a possible value to be assigned to i.

i

Proposition 2.7 Algorithm 2.5 is equivalent to depth-(cid:12)rst search and therefore complete.

Proof. This is an easy consequence of the lemma. Partial solutions correspond to nodes

in the search space.

Lemma 3.2 Let P be a partial solution obtained during the execution of Algorithm 3.1, and

let i 2 P be a variable assigned a value by P . Now if P

(cid:18) P can be successful ly extended

0

to a complete solution after assigning i the value v but (v ; E ) 2 E

, we must have

i

E \ (P (cid:0) P

) 6= (cid:31)

0

Proof. As in the proof of Lemma 2.6, we show that no step of Algorithm 3.1 can cause

Lemma 3.2 to become false.

That the lemma holds after step 2, where the search is extended to consider a new

variable, is an immediate consequence of the assumption that the elimination mechanism

is complete.

In step 4, when we add (v

; E (cid:0) fj g) to the set of eliminating explanations for j , we

j

are simply recording the fact that the search for a solution with j set to v

failed because

j

we were unable to extend the solution to i. It is a consequence of the inductive hypothesis

that as long as no variable in E (cid:0) fj g changes, this conclusion will remain valid.

Proposition 3.4 Backjumping is complete and always expands fewer nodes than does depth-

(cid:12)rst search.

42

Dynamic Backtracking

Proof. That fewer nodes are examined is clear; for completeness, it follows from Lemma

3.2 that the backtrack to some element of E in step 5 will always be necessary if a solution

is to be found.

Proposition 3.5 The amount of space needed by backjumping is o(i

v ), where i = jI j is

2

the number of variables in the problem and v is the number of values for that variable with

the largest value set V

.

i

Proof. The amount of space needed is dominated by the storage requirements of the elim-

ination sets E

; there are i of these. Each one might refer to each of the possible values for

j

a particular variable j ; the space needed to store the reason that the value j is eliminated

is at most jI j, since the reason is simply a list of variables that have been assigned values.

There will never be two eliminating explanations for the same variable, since (cid:15) is concise

and we never rebind a variable to a value that has been eliminated.

Theorem 4.2 Dynamic backtracking always terminates and is complete. It continues to

satisfy Proposition 3.5 and can be expected to expand fewer nodes than backjumping provided

that the goal nodes are distributed randomly in the search space.

Proof. There are four things we need to show: That dynamic backtracking needs o(i

v )

2

space, that it is complete, that it can be expected to expand fewer nodes than backjumping,

and that it terminates. We prove things in this order.

Space This is clear; the amount of space needed continues to be bounded by the structure

of the eliminating explanations.

Completeness This is also clear, since by Lemma 3.2, all of the eliminating explanations

retained in the algorithm are obviously still valid. The new explanations added in (2) are

also obviously correct, since they indicate that j cannot take the value v

as in backjumping

j

and that j also cannot take any values that are eliminated by the variables being backjumped

over.

E(cid:14)ciency To see that we expect to expand fewer nodes, suppose that the subproblem

involving only the variables being jumped over has s solutions in total, one of which is given

by the existing variable assignments. Assuming that the solutions are distributed randomly

in the search space, there is at least a 1=s chance that this particular solution leads to a

solution of the entire csp; if so, the reordered search { which considers this solution earlier

than the other { will save the expense of either assigning new values to these variables or

repeating the search that led to the existing choices. The reordered search will also bene(cid:12)t

from the information in the nogoods that have been retained for the variables being jumped

over.

Termination This is the most di(cid:14)cult part of the proof.

As we work through the algorithm, we will be generating (and then discarding) a variety

of eliminating explanations. Suppose that e is such an explanation, saying that j cannot

take the value v

because of the values currently taken by the variables in some set e

.

j

V

We will denote the variables in e

by x

; . . . ; x

and their current values by v

; . . . ; v

. In

V

1

k

1

k

declarative terms, the eliminating explanation is telling us that

(x

= v

) ^ (cid:1) (cid:1) (cid:1) ^ (x

= v

) (cid:27) j 6= v

(7)

1

1

k

k

j

43

Ginsberg

Dependency-directed backtracking would have us accumulate all of these nogoods; dynamic

backtracking allows us to drop any particular instance of (7) for which the antecedent is no

longer valid.

The reason that dependency-directed backtracking is guaranteed to terminate is that

the set of accumulated nogoods eliminates a monotonically increasing amount of the search

space. Each nogood eliminates a new section of the search space because the nature of the

search process is such that any node examined is consistent with the nogoods that have been

accumulated thus far; the process is monotonic because all nogoods are retained throughout

the search. These arguments cannot be applied to dynamic backtracking, since nogoods are

forgotten as the search proceeds. But we can make an analogous argument.

To do this, suppose that when we discover a nogood like (7), we record with it all of the

variables that precede the variable j in the partial order, together with the values currently

assigned to these variables. Thus an eliminating explanation becomes essentially a nogood

n of the form (7) together with a set S of variable/value pairs.

We now de(cid:12)ne a mapping (cid:21)(n; S ) that changes the antecedent of (7) to include assump-

tions about al l the variables bound in S , so that if S = fs

; v

g,

i

i

(cid:21)(n; S ) = [(s

= v

) ^ (cid:1) (cid:1) (cid:1) ^ (s

= v

) (cid:27) j 6= v

]

(8)

1

1

l

l

j

At any point in the execution of the algorithm, we denote by N the conjunction of the

modi(cid:12)ed nogoods of the form (8).

We now make the following claims:

1. For any eliminating explanation (n; S ), n j= (cid:21)(n; S ) so that (cid:21)(n; S ) is valid for the

problem at hand.

2. For any new eliminating explanation (n; S ), (cid:21)(n; S ) is not a consequence of N .

3. The deductive consequences of N grow monotonically as the dynamic backtracking

algorithm proceeds.

The theorem will follow from these three observations, since we will know that N is a valid

set of conclusions for our search problem and that we are once again making monotonic

progress toward eliminating the entire search space and concluding that the problem is

unsolvable.

That (cid:21)(n; S ) is a consequence of (n; S ) is clear, since the modi(cid:12)cation used to obtain

(8) from (7) involves strengthening that antecedent of (7). It is also clear that (cid:21)(n; S ) is

not a consequence of the nogoods already obtained, since we have added to the antecedent

only conditions that hold for the node of the search space currently under examination. If

(cid:21)(n; S ) were a consequence of the nogoods we had obtained thus far, this node would not

be being considered.

The last observation depends on the following lemma:

Lemma A.1 Suppose that x is a variable assigned a value by our partial solution and that

x appears in the antecedent of the nogood n in the pair (n; S ). Then if S

is the set of

0

variables assigned values no later than x, S

(cid:18) S .

0

44

Dynamic Backtracking

Proof. Consider a y 2 S

, and suppose that it were not in S . We cannot have y = x, since

0

y would then be mentioned in the nogood n and therefore in S . So we can suppose that

y is actually assigned a value earlier than x is. Now when (n; S ) was added to the set of

eliminating explanations, it must have been the case that x was assigned a value (since it

appears in the antecedent of n) but that y was not. But we also know that there was a

later time when y was assigned a value but x was not, since y precedes x in the current

partial solution. This means that x must have changed value at some point after (n; S ) was

added to the set of eliminating explanations { but (n; S ) would have been deleted when this

happened. This contradiction completes the proof.

Returning to the proof the Theorem 4.2, suppose that we eventually drop (n; S ) from

our collection of nogoods and that when we do so, the new nogood being added is (n

; S

). It

0

0

follows from the lemma that S

(cid:18) S . Since x

= v

is a clause in the antecedent of (cid:21)(n; S ), it

i

i

0

follows that (cid:21)(n

; S

) will imply the negation of the antecedent of (cid:21)(n; S ) and will therefore

0

0

imply (cid:21)(n; S ) itself. Although we drop (cid:21)(n; S ) when we drop the nogood (n; S ), (cid:21)(n; S )

continues to be entailed by the modi(cid:12)ed set N , the consequences of which are seen to be

growing monotonically.

References

Bruynooghe, M. (1981). Solving combinatorial search problems by intelligent backtracking.

Information Processing Letters, 12 (1), 36{39.

de Kleer, J. (1986). An assumption-based truth maintenance system. Arti(cid:12)cial Intel ligence,

28, 127{162.

Dechter, R., & Meiri, I. (1989). Experimental evaluation of preprocessing techniques in

constraint satisfaction problems. In Proceedings of the Eleventh International Joint

Conference on Arti(cid:12)cial Intel ligence, pp. 271{277.

Gaschnig, J. (1979). Performance measurement and analysis of certain search algorithms.

Tech. rep. CMU-CS-79-124, Carnegie-Mellon University.

Ginsberg, M. L., Frank, M., Halpin, M. P., & Torrance, M. C. (1990). Search lessons learned

from crossword puzzles. In Proceedings of the Eighth National Conference on Arti(cid:12)cial

Intel ligence, pp. 210{215.

Ginsberg, M. L., & Harvey, W. D. (1992). Iterative broadening. Arti(cid:12)cial Intel ligence, 55,

367{383.

Jonsson, A. K., & Ginsberg, M. L. (1993). Experimenting with new systematic and non-

systematic search techniques. In Proceedings of the AAAI Spring Symposium on AI

and NP-Hard Problems Stanford, California.

McAllester, D. A. (1993). Partial order backtracking. Journal of Arti(cid:12)cial Intel ligence

Research, 1. Submitted.

Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1990). Solving large-scale con-

straint satisfaction and scheduling problems using a heuristic repair method. In Pro-

ceedings of the Eighth National Conference on Arti(cid:12)cial Intel ligence, pp. 17{24.

45

Ginsberg

P. Purdom, C. B., & Robertson, E. (1981). Backtracking with multi-level dynamic search

rearrangement. Acta Informatica, 15, 99{114.

Seidel, R. (1981). A new method for solving constraint satisfaction problems. In Proceedings

of the Seventh International Joint Conference on Arti(cid:12)cial Intel ligence, pp. 338{342.

Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satis(cid:12)ability

problems. In Proceedings of the Tenth National Conference on Arti(cid:12)cial Intel ligence.

Smith, D. E., & Genesereth, M. R. (1985). Ordering conjunctive queries. Arti(cid:12)cial Intel li-

gence, 26 (2), 171{215.

Stallman, R. M., & Sussman, G. J. (1977). Forward reasoning and dependency-directed

backtracking in a system for computer-aided circuit analysis. Arti(cid:12)cial Intel ligence,

9 (2), 135{196.

Zabih, R. (1990). Some applications of graph bandwidth to constraint satisfaction problems.

In Proceedings of the Eighth National Conference on Arti(cid:12)cial Intel ligence, pp. 46{51.

Zabih, R., & McAllester, D. A. (1988). A rearrangement search strategy for determining

propositional satis(cid:12)ability.

In Proceedings of the Seventh National Conference on

Arti(cid:12)cial Intel ligence, pp. 155{160.

46

","Because of their occasional need to return to shallow points in a search
tree, existing backtracking methods can sometimes erase meaningful progress
toward solving a search problem. In this paper, we present a method by which
backtrack points can be moved deeper in the search space, thereby avoiding this
difficulty. The technique developed is a variant of dependency-directed
backtracking that uses only polynomial space while still providing useful
control information and retaining the completeness guarantees provided by
earlier approaches."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1993) 1-23

Submitted 5/93; published 8/93

A Market-Oriented Programming Environment and its

Application to Distributed Multicommodity Flow Problems

Michael P. Wellman

wellman@engin.umich.edu

University of Michigan, Dept. of Electrical Engineering and Computer Science,

Ann Arbor, MI 48109 USA

Abstract

Market price systems constitute a well-understood class of mechanisms that under

certain conditions provide e(cid:11)ective decentralization of decision making with minimal com-

munication overhead. In a market-oriented programming approach to distributed problem

solving, we derive the activities and resource allocations for a set of computational agents

by computing the competitive equilibrium of an arti(cid:12)cial economy. Walras provides basic

constructs for de(cid:12)ning computational market structures, and protocols for deriving their

corresponding price equilibria. In a particular realization of this approach for a form of

multicommodity (cid:13)ow problem, we see that careful construction of the decision process ac-

cording to economic principles can lead to e(cid:14)cient distributed resource allocation, and that

the behavior of the system can be meaningfully analyzed in economic terms.

1. Distributed Planning and Economics

In a distributed or multiagent planning system, the plan for the system as a whole is a com-

posite of plans produced by its constituent agents. These plans may interact signi(cid:12)cantly in

both the resources required by each of the agents' activities (preconditions) and the prod-

ucts resulting from these activities (postconditions). Despite these interactions, it is often

advantageous or necessary to distribute the planning process because agents are separated

geographically, have di(cid:11)erent information, possess distinct capabilities or authority, or have

been designed and implemented separately.

In any case, because each agent has limited

competence and awareness of the decisions produced by others, some sort of coordination is

required to maximize the performance of the overall system. However, allocating resources

via central control or extensive communication is deemed infeasible, as it violates whatever

constraints dictated distribution of the planning task in the (cid:12)rst place.

The task facing the designer of a distributed planning system is to de(cid:12)ne a computa-

tionally e(cid:14)cient coordination mechanism and its realization for a collection of agents. The

agent con(cid:12)guration may be given, or may itself be a design parameter. By the term agent,

I refer to a module that acts within the mechanism according to its own knowledge and

interests. The capabilities of the agents and their organization in an overall decision-making

structure determine the behavior of the system as a whole. Because it concerns the collec-

tive behavior of self-interested decision makers, the design of this decentralized structure is

fundamentally an exercise in economics or incentive engineering. The problem of developing

architectures for distributed planning (cid:12)ts within the framework of mechanism design (Hur-

wicz, 1977; Reiter, 1986), and many ideas and results from economics are directly applicable.

In particular, the class of mechanisms based on price systems and competition has been

deeply investigated by economists, who have characterized the conditions for its e(cid:14)ciency

(cid:13)1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Wellman

and compatibility with other features of the economy. When applicable, the competitive

mechanism achieves coordination with minimal communication requirements (in a precise

sense related to the dimensionality of messages transmitted among agents (Reiter, 1986)).

The theory of general equilibrium (Hildenbrand & Kirman, 1976) provides the founda-

tion for a general approach to the construction of distributed planning systems based on

price mechanisms. In this approach, we regard the constituent planning agents as consumers

and producers in an arti(cid:12)cial economy, and de(cid:12)ne their individual activities in terms of pro-

duction and consumption of commodities. Interactions among agents are cast as exchanges,

the terms of which are mediated by the underlying economic mechanism, or protocol. By

specifying the universe of commodities, the con(cid:12)guration of agents, and the interaction

protocol, we can achieve a variety of interesting and often e(cid:11)ective decentralized behaviors.

Furthermore, we can apply economic theory to the analysis of alternative architectures, and

thus exploit a wealth of existing knowledge in the design of distributed planners.

I use the phrase market-oriented programming to refer to the general approach of de-

riving solutions to distributed resource allocation problems by computing the competitive

equilibrium of an arti(cid:12)cial economy.

In the following, I describe this general approach

1

and a primitive programming environment supporting the speci(cid:12)cation of computational

markets and derivation of equilibrium prices. An example problem in distributed trans-

portation planning demonstrates the feasibility of decentralizing a problem with nontrivial

interactions, and the applicability of economic principles to distributed problem solving.

2. WALRAS: A Market-Oriented Programming Environment

To explore the use of market mechanisms for the coordination of distributed planning mod-

ules, I have developed a prototype environment for specifying and simulating computational

markets. The system is called walras, after the 19th-century French economist L(cid:19)eon Wal-

ras, who was the (cid:12)rst to envision a system of interconnected markets in price equilibrium.

Walras provides basic mechanisms implementing various sorts of agents, auctions, and

bidding protocols. To specify a computational economy, one de(cid:12)nes a set of goods and

instantiates a collection of agents that produce or consume those goods. Depending on the

context, some of the goods or agents may be (cid:12)xed exogenously, for example, they could cor-

respond to real-world goods or agents participating in the planning process. Others might

be completely arti(cid:12)cial ones invented by the designer to decentralize the problem-solving

process in a particular way. Given a market con(cid:12)guration, walras then runs these agents

to determine an equilibrium allocation of goods and activities. This distribution of goods

and activities constitutes the market solution to the planning problem.

1. The name was inspired by Shoham's use of agent-oriented programming to refer to a specialization of

ob ject-oriented programming where the entities are described in terms of agent concepts and interact

via speech acts (Shoham, 1993). Market-oriented programming is an analogous specialization, where the

entities are economic agents that interact according to market concepts of production and exchange. The

phrase has also been invoked by Lavoie, Baetjer, and Tulloh (1991) to refer to real markets in software

components.

2

Market-Oriented Programming

2.1 General Equilibrium

The walras framework is patterned directly after general-equilibrium theory. A brief expo-

sition, glossing over many (cid:12)ne points, follows; for elaboration see any text on microeconomic

theory (e.g., (Varian, 1984)).

We start with k goods and n agents. Agents fall in two general classes. Consumers can

buy, sell, and consume goods, and their preferences for consuming various combinations or

bund les of goods are speci(cid:12)ed by their utility function. If agent i is a consumer, then its

utility function, u

: <

! <, ranks the various bundles of goods according to preference.

i

+

k

Consumers may also start with an initial allocation of some goods, termed their endow-

ment. Let e

denote agent i's endowment of good j , and x

the amount of good j that i

i;j

i;j

ultimately consumes. The ob jective of consumer i is to choose a feasible bundle of goods,

(x

; : : : ; x

) (rendered in vector notation as x

), so as to maximize its utility. A bundle

i;1

i;k

i

is feasible for consumer i if its total cost at the going prices does not exceed the value of

i's endowment at these prices. The consumer's choice can be expressed as the following

constrained optimization problem:

max

u

(x

) s.t. p (cid:1) x

(cid:20) p (cid:1) e

;

(1)

i

i

i

i

x

i

where p = (p

; : : : ; p

) is the vector of prices for the k goods.

1

k

Agents of the second type, producers, can transform some sorts of goods into some

others, according to their technology. The technology speci(cid:12)es the feasible combinations of

inputs and outputs for the producer. Let us consider the special case where there is one

output good, indexed j , and the remaining goods are potential inputs. In that case, the

technology for producer i can be described by a production function,

y

= (cid:0)x

= f

(x

; : : : ; x

; x

; : : : ; x

);

i

i;j

i

i;1

i;j(cid:0)1

i;j+1

i;k

specifying the maximum output producible from the given inputs. (When a good is an

input in its own production, the production function characterizes net output.) In this

case, the producer's ob jective is to choose a production plan that maximizes pro(cid:12)ts sub ject

to its technology and the going price of its output and input goods. This involves choosing a

production level, y

, along with the levels of inputs that can produce y

at the minimum cost.

i

i

Let x

and p

denote the consumption and prices, respectively, of the input goods. Then

i;(cid:22)|

(cid:22)|

the corresponding constrained optimization problem is to maximize pro(cid:12)ts, the di(cid:11)erence

between revenues and costs:

(cid:20)

(cid:20)

(cid:21)(cid:21)

max

p

y

(cid:0)

min

p

(cid:1) x

s.t. y

(cid:20) f

(x

)

;

j

i

(cid:22)|

i;(cid:22)|

i

i

i;(cid:22)|

y

i

x

i;(cid:22)|

or equivalently,

min

p (cid:1) x

s.t. (cid:0) x

(cid:20) f

(x

):

(2)

i

i;j

i

i;(cid:22)|

x

i

An agent acts competitively when it takes prices as given, neglecting any impact of its

own behavior on prices. The above formulation implicitly assumes perfect competition, in

that the prices are parameters of the agents' constrained optimization problems. Perfect

competition realistically re(cid:13)ects individual rationality when there are numerous agents, each

small with respect to the entire economy. Even when this is not the case, however, we can

3

Wellman

implement competitive behavior in individual agents if we so choose. The implications of

the restriction to perfect competition are discussed further below.

A pair (p; x) of a price vector and vector of demands for each agent constitutes a

competitive equilibrium for the economy if and only if:

1. For each agent i, x

is a solution to its constrained optimization problem|(1) or

i

(2)|at prices p, and

2. the net amount of each good produced and consumed equals the total endowment,

n

n

X

X

x

=

e

; for j = 1; : : : ; k:

(3)

i;j

i;j

i=1

i=1

In other words, the total amount consumed equals the total amount produced (counted

as negative quantities in the consumption bundles of producers), plus the total amount

the economy started out with (the endowments).

Under certain \classical"" assumptions (essentially continuity, monotonicity, and concav-

ity of the utility and production functions; see, e.g., (Hildenbrand & Kirman, 1976; Varian,

1984)), competitive equilibria exist, and are unique given strictness of these conditions.

From the perspective of mechanism design, competitive equilibria possess several desirable

properties, in particular, the two fundamental welfare theorems of general equilibrium the-

ory: (1) all competitive equilibria are Pareto optimal (no agent can do better without some

other doing worse), and (2) any feasible Pareto optimum is a competitive equilibrium for

some initial allocation of the endowments. These properties seem to o(cid:11)er exactly what

we need: a bound on the quality of the solution, plus the prospect that we can achieve

the most desired behavior by carefully engineering the con(cid:12)guration of the computational

market. Moreover, in equilibrium, the prices re(cid:13)ect exactly the information required for

distributed agents to optimally evaluate perturbations in their behavior without resorting

to communication or reconsideration of their full set of possibilities (Koopmans, 1970).

2.2 Computing Competitive Equilibria

Competitive equilibria are also computable, and algorithms based on (cid:12)xed-point meth-

ods (Scarf, 1984) and optimization techniques (Nagurney, 1993) have been developed. Both

sorts of algorithms in e(cid:11)ect operate by collecting and solving the simultaneous equilib-

rium equations (1), (2), and (3)). Without an expressly distributed formulation, however,

these techniques may violate the decentralization considerations underlying our distributed

problem-solving context. This is quite acceptable for the purposes these algorithms were

originally designed, namely to analyze existing decentralized structures, such as transporta-

tion industries or even entire economies (Shoven & Whalley, 1992). But because our purpose

is to implement a distributed system, we must obey computational distributivity constraints

not relevant to the usual purposes of applied general-equilibrium analysis. In general, ex-

plicitly examining the space of commodity bundle allocations in the search for equilibrium

undercuts our original motive for decomposing complex activities into consumption and

production of separate goods.

4

Market-Oriented Programming

Another important constraint is that internal details of the agents' state (such as utility

or production functions and bidding policy) should be considered private in order to maxi-

mize modularity and permit inclusion of agents not under the designers' direct control. A

consequence of this is that computationally exploiting global properties arising from spe-

cial features of agents would not generally be permissible for our purposes. For example,

the constraint that pro(cid:12)ts be zero is a consequence of competitive behavior and constant-

returns production technology. Since information about the form of the technology and

bidding policy is private to producer agents, it could be considered cheating to embed the

zero-pro(cid:12)t condition into the equilibrium derivation procedure.

Walras's procedure is a decentralized relaxation method, akin to the mechanism of

tatonnement originally sketched by L(cid:19)eon Walras to explain how prices might be derived.

In the basic tatonnement method, we begin with an initial vector of prices, p

. The agents

0

determine their demands at those prices (by solving their corresponding constrained op-

timization problems), and report the quantities demanded to the \auctioneer"". Based on

these reports, the auctioneer iteratively adjusts the prices up or down as there is an excess

of demand or supply, respectively. For instance, an adjustment proportional to the excess

could be modeled by the di(cid:11)erence equation

n

n

X

X

p

= p

+ (cid:11)(

x

(cid:0)

e

):

t+1

t

i

i

i=1

i=1

If the sequence p

; p

; : : : converges, then the excess demand in each market approaches zero,

0

1

and the result is a competitive equilibrium. It is well known, however, that tatonnement

processes do not converge to equilibrium in general (Scarf, 1984). The class of economies in

which tatonnement works are those with so-called stable equilibria (Hicks, 1948). A su(cid:14)cient

condition for stability is gross substitutability (Arrow & Hurwicz, 1977): that if the price

for one good rises, then the net demands for the other goods do not decrease. Intuitively,

gross substitutability will be violated when there are complementarities in preferences or

technologies such that reduced consumption for one good will cause reduced consumption

in others as well (Samuelson, 1974).

2.3 WALRAS Bidding Protocol

The method employed by walras successively computes an equilibrium price in each sep-

arate market, in a manner detailed below. Like tatonnement, it involves an iterative ad-

justment of prices based on reactions of the agents in the market. However, it di(cid:11)ers from

traditional tatonnement procedures in that (1) agents submit supply and demand curves

rather than single point quantities for a particular price, and (2) the auction adjusts in-

dividual prices to clear, rather than adjusting the entire price vector by some increment

(usually a function of summary statistics such as excess demand).

2

Walras associates an auction with each distinct good. Agents act in the market by

submitting bids to auctions. In walras, bids specify a correspondence between prices and

2. This general approach is called progressive equilibration by Dafermos and Nagurney (1989), who applied

it to a particular transportation network equilibrium problem. Although this model of market dynamics

does not appear to have been investigated very extensively in general-equilibrium theory, it does seem

to match the kind of price adjustment process envisioned by Hicks in his pioneering study of dynamics

and stability (Hicks, 1948).

5

Wellman

quantities of the good that the agent o(cid:11)ers to demand or supply. The bid for a particular

good corresponds to one dimension of the agent's optimal demand, which is parametrized

by the prices for all relevant goods. Let x

(p) be the solution to equation (1) or (2), as

i

appropriate, for prices p. A walras agent bids for good j under the assumption that prices

for the remaining goods are (cid:12)xed at their current values, p

. Formally, agent i's bid for

(cid:22)|

good j is a function x

: <

! <, from prices to quantities satisfying

i;j

+

x

(p

) = x

(p

; p

)

;

i;j

j

i

j

(cid:22)|

j

where the subscript j on the right-hand side selects the quantity demanded of good j from

the overall demand vector. The agent computes and sends this function (encoded in any of

a variety of formats) to the auction for good j .

Given bids from all interested agents, the auction derives a market-clearing price, at

which the quantity demanded balances that supplied, within some prespeci(cid:12)ed tolerance.

This clearing price is simply the zero crossing of the aggregate demand function, which is the

sum of the demands from all agents. Such a zero crossing will exist as long as the aggregate

demand is su(cid:14)ciently well-behaved, in particular, if it is continuous and decreasing in price.

Gross substitutability, along with the classical conditions for existence of equilibrium, is

su(cid:14)cient to ensure the existence of a clearing price at any stage of the bidding protocol.

Walras calculates the zero crossing of the aggregate demand function via binary search.

If aggregate demand is not well-behaved, the result of the auction may be a non-clearing

price.

When the current price is clearing with respect to the current bids, we say the market

for that commodity is in equilibrium. We say that an agent is in equilibrium if its set of

outstanding bids corresponds to the solution of its optimization problem at the going prices.

If all the agents and commodity markets are in equilibrium, the allocation of goods dictated

by the auction results is a competitive equilibrium.

Figure 1 presents a schematic view of the walras bidding process. There is an auction

for each distinct good, and for each agent, a link to all auctions in which it has an interest.

There is also a \tote board"" of current prices, kept up-to-date by the various auctions. In

the current implementation the tote board is a global data structure, however, since price

change noti(cid:12)cations are explicitly transmitted to interested agents, this central information

could be easily dispensed with.

Each agent maintains an agenda of bid tasks, specifying the markets in which it must

update its bid or compute a new one. In Figure 1, agent A

has pending tasks to submit

i

bids to auctions G

, G

, and G

. The bidding process is highly distributed, in that each

1

7

4

agent need communicate directly only with the auctions for the goods of interest (those in

the domain of its utility or production function, or for which it has nonzero endowments).

Each of these interactions concerns only a single good; auctions never coordinate with each

other. Agents need not negotiate directly with other agents, nor even know of each other's

existence.

As new bids are received at auction, the previously computed clearing price becomes

obsolete. Periodically, each auction computes a new clearing price (if any new or updated

bids have been received) and posts it on the tote board. When a price is updated, this

may invalidate some of an agent's outstanding bids, since these were computed under the

assumption that prices for remaining goods were (cid:12)xed at previous values. On (cid:12)nding out

6

Market-Oriented Programming

G1

G2

Gk

A1

A2

Ai

Task Agenda
[1], [7], [4]

tote board

}

An

}

p1
p2

pk

Figure 1: Walras's bidding process. G

denotes the auction for the j th good, and A

the

j

i

ith trading agent. An item [j ] on the task agenda denotes a pending task to

compute and submit a bid for good j .

about a price change, an agent augments its task agenda to include the potentially a(cid:11)ected

bids.

At all times, walras maintains a vector of going prices and quantities that would be

exchanged at those prices. While the agents have nonempty bid agendas or the auctions new

bids, some or all goods may be in disequilibrium. When all auctions clear and all agendas

are exhausted, however, the economy is in competitive equilibrium (up to some numeric

tolerance). Using a recent result of Milgrom and Roberts (1991, Theorem 12), it can be

shown that the condition su(cid:14)cient for convergence of tatonnement|gross substitutability|

is also su(cid:14)cient for convergence of walras's price-adjustment process. The key observation

is that in progressive equilibration (synchronous or not) the price at each time is based on

some set of previous supply and demand bids.

Although I have no precise results to this e(cid:11)ect, the computational e(cid:11)ort required for

convergence to a (cid:12)xed tolerance seems highly sensitive to the number of goods, and much

less so to the number of agents. Eydeland and Nagurney (1989) have analyzed in detail

the convergence pattern of progressive equilibration algorithms related to walras for par-

ticular special cases, and found roughly linear growth in the number of agents. However,

general conclusions are di(cid:14)cult to draw as the cost of computing the equilibrium for a par-

ticular computational economy may well depend on the interconnectedness and strength of

interactions among agents and goods.

2.4 Market-Oriented Programming

As described above, walras provides facilities for specifying market con(cid:12)gurations and

computing their competitive equilibrium. We can also view walras as a programming

environment for decentralized resource allocation procedures. The environment provides

constructs for specifying various sorts of agents and de(cid:12)ning their interactions via their

7

Wellman

relations to common commodities. After setting up the initial con(cid:12)guration, the market

can be run to determine the equilibrium level of activities and distribution of resources

throughout the economy.

To cast a distributed planning problem as a market, one needs to identify (1) the goods

traded, (2) the agents trading, and (3) the agents' bidding behavior. These design steps

are serially dependent, as the de(cid:12)nition of what constitutes an exchangeable or producible

commodity severely restricts the type of agents that it makes sense to include. And as

mentioned above, sometimes we have to take as (cid:12)xed some real-world agents and goods

presented as part of the problem speci(cid:12)cation. Once the con(cid:12)guration is determined, it

might be advantageous to adjust some general parameters of the bidding protocol. Below, I

illustrate the design task with a walras formulation of the multicommodity (cid:13)ow problem.

2.5 Implementation

Walras is implemented in Common Lisp and the Common Lisp Ob ject System (CLOS).

The current version provides basic infrastructure for running computational economies,

including the underlying bidding protocol and a library of CLOS classes implementing a

variety of agent types. The ob ject-oriented implementation supports incremental develop-

ment of market con(cid:12)gurations. In particular, new types of agents can often be de(cid:12)ned as

slight variations on existing types, for example by modifying isolated features of the demand

behavior, bidding strategies (e.g., management of task agenda), or bid format. Wang and

Slagle (1993) present a detailed case for the use of ob ject-oriented languages to represent

general-equilibrium models. Their proposed system is similar to walras with respect to

formulation, although it is designed as an interface to conventional model-solving packages,

rather than to support a decentralized computation of equilibrium directly.

Although it models a distributed system, walras runs serially on a single processor.

Distribution constraints on information and communication are enforced by programming

and speci(cid:12)cation conventions rather than by fundamental mechanisms of the software en-

vironment. Asynchrony is simulated by randomizing the bidding sequences so that agents

are called on unpredictably. Indeed, arti(cid:12)cial synchronization can lead to an undesirable

oscillation in the clearing prices, as agents collectively overcompensate for imbalances in

the preceding iteration.

3

The current experimental system runs transportation models of the sort described be-

low, as well as some abstract exchange and production economies with parametrized utility

and production functions (including the expository examples of Scarf (1984) and Shoven

and Whalley (1984)). Customized tuning of the basic bidding protocol has not been nec-

essary.

In the process of getting walras to run on these examples, I have added some

generically useful building blocks to the class libraries, but much more is required to (cid:12)ll out

a comprehensive taxonomy of agents, bidding strategies, and auction policies.

3. In some formal dynamic models (Huberman, 1988; Kephart, Hogg, & Huberman, 1989), homogeneous

agents choose instantaneously optimal policies without accounting for others that are simultaneously

making the same choice. Since the value of a particular choice varies inversely with the number of agents

choosing it, this delayed feedback about the others' decisions leads to systematic errors, and hence

oscillation. I have also observed this phenomenon empirically in a synchronized version of WALRAS.

By eliminating the synchronization, agents tend to work on di(cid:11)erent markets at any one time, and hence

do not su(cid:11)er as much from delayed feedback about prices.

8

Market-Oriented Programming

3. Example: Multicommodity Flow

In a simple version of the multicommodity (cid:13)ow problem, the task is to allocate a given

set of cargo movements over a given transportation network. The transportation network

is a collection of locations, with links (directed edges) identifying feasible transportation

operations. Associated with each link is a speci(cid:12)cation of the cost of moving cargo along it.

We suppose further that the cargo is homogeneous, and that amounts of cargo are arbitrarily

divisible. A movement requirement associates an amount of cargo with an origin-destination

pair. The planning problem is to determine the amount to transport on each link in order to

move all the cargo at the minimum cost. This simpli(cid:12)cation ignores salient aspects of real

transportation planning. For instance, this model is completely atemporal, and is hence

more suitable for planning steady-state (cid:13)ows than for planning dynamic movements.

A distributed version of the problem would decentralize the responsibility for trans-

porting separate cargo elements. For example, planning modules corresponding to geo-

graphically or organizationally disparate units might arrange the transportation for cargo

within their respective spheres of authority. Or decision-making activity might be decom-

posed along hierarchical levels of abstraction, gross functional characteristics, or according

to any other relevant distinction. This decentralization might result from real distribution

of authority within a human organization, from inherent informational asymmetries and

communication barriers, or from modularity imposed to facilitate software engineering.

Consider, for example, the abstract transportation network of Figure 2, taken from

Harker (1988). There are four locations, with directed links as shown. Consider two move-

ment requirements. The (cid:12)rst is to transport cargo from location 1 to location 4, and the

second in the reverse direction. Suppose we wish to decentralize authority so that separate

agents (called shippers) decide how to allocate the cargo for each movement. The (cid:12)rst ship-

per decides how to split its cargo units between the paths 1 ! 2 ! 4 and 1 ! 2 ! 3 ! 4,

while the second (cid:12)gures the split between paths 4 ! 2 ! 1 and 4 ! 2 ! 3 ! 1. Note that

the latter paths for each shipper share a common resource: the link 2 ! 3.

1

2

3

4

Figure 2: A simple network (from Harker (1988)).

Because of their overlapping resource demands, the shippers' decisions appear to be

necessarily intertwined. In a congested network, for example, the cost for transporting a

unit of cargo over a link is increasing in the overall usage of the link. A shipper planning

its cargo movements as if it were the only user on a network would thus underestimate its

costs and potentially misallocate transportation resources.

9

Wellman

For the analysis of networks such as this, transportation researchers have developed

equilibrium concepts describing the collective behavior of the shippers. In a system equi-

librium, the overall transportation of cargo proceeds as if there were an omniscient central

planner directing the movement of each shipment so as to minimize the total aggregate

cost of meeting the requirements.

In a user equilibrium, the overall allocation of cargo

movements is such that each shipper minimizes its own total cost, sharing proportionately

the cost of shared resources. The system equilibrium is thus a global optimum, while the

user equilibrium corresponds to a composition of locally optimal solutions to subproblems.

There are also some intermediate possibilities, corresponding to game-theoretic equilibrium

concepts such as the Nash equilibrium, where each shipper behaves optimally given the

transportation policies of the remaining shippers (Harker, 1986).

4

From our perspective as designer of the distributed planner, we seek a decentralization

mechanism that will reach the system equilibrium, or come as close as possible given the

distributed decision-making structure. In general, however, we cannot expect to derive a

system equilibrium or globally optimal solution without central control. Limits on coordi-

nation and communication may prevent the distributed resource allocation from exploiting

all opportunities and inhibiting agents from acting at cross purposes. But under certain

conditions decision making can indeed be decentralized e(cid:11)ectively via market mechanisms.

General-equilibrium analysis can help us to recognize and take advantage of these opportu-

nities.

Note that for the multicommodity (cid:13)ow problem, there is an e(cid:11)ective distributed solution

due to Gallager (1977). One of the market structures described below e(cid:11)ectively mimics this

solution, even though Gallager's algorithm was not formulated expressly in market terms.

The point here is not to crack a hitherto unsolved distributed optimization problem (though

that would be nice), but rather to illustrate a general approach on a simply described yet

nontrivial task.

4. WALRAS Transportation Market

In this section, I present a series of three transportation market structures implemented in

walras. The (cid:12)rst and simplest model comprises the basic transportation goods and shipper

agents, which are augmented in the succeeding models to include other agent types. Com-

parative analysis of the three market structures reveals the qualitatively distinct economic

and computational behaviors realized by alternate walras con(cid:12)gurations.

4.1 Basic Shipper Model

The resource of primary interest in the multicommodity (cid:13)ow problem is movement of cargo.

Because the value and cost of a cargo movement depends on location, we designate as a

distinct good the capacity on each origin-destination pair in the network (see Figure 2). To

capture the cost or input required to move cargo, we de(cid:12)ne another good denoting generic

transportation resources. In a more concrete model, these might consist of vehicles, fuel,

labor, or other factors contributing to transportation.

4. In the Nash solution, shippers correctly anticipate the e(cid:11)ect of their own cargo movements on the average

cost on each link. The resulting equilibrium converges to the user equilibrium as the number of shippers

increases and the e(cid:11)ect of any individual's behavior on prices diminishes (Haurie & Marcotte, 1985).

10

Market-Oriented Programming

To decentralize the decision making, we identify each movement requirement with a

distinct shipper agent. These shippers, or consumers, have an interest in moving various

units of cargo between speci(cid:12)ed origins and destinations.

The interconnectedness of agents and goods de(cid:12)nes the market con(cid:12)guration. Figure 3

depicts the walras con(cid:12)guration for the basic shipper model corresponding to the example

network of Figure 2. In this model there are two shippers, S

and S

, where S

denotes

1;4

4;1

i;j

a shipper with a requirement to move goods from origin i to destination j . Shippers connect

to goods that might serve their ob jectives: in this case, movement along links that belong to

some simple path from the shipper's origin to its destination. In the diagram, G

denotes

i;j

the good representing an amount of cargo moved over the link i ! j . G

denotes the special

0

transportation resource good. Notice that the only goods of interest to both shippers are

G

, for which they both have endowments, and G

, transportation on the link serving

0

2;3

both origin-destination pairs.

G

3,4

G 2,4

G

1,2

G 2,3

G 3,1

S

1,4

S4,1

G

2,1

G

0

G

4,2

Figure 3: Walras basic shipper market con(cid:12)guration for the example transportation net-

work.

The model we employ for transportation costs is based on a network with congestion,

thus exhibiting diseconomies of scale. In other words, the marginal and average costs (in

terms of transportation resources required) are both increasing in the level of service on a

link. Using Harker's data, we take costs to be quadratic. The quadratic cost model is posed

simply for concreteness, and does not represent any substantive claim about transportation

networks. The important qualitative feature of this model (and the only one necessary

for the example to work) is that it exhibits decreasing returns, a de(cid:12)ning characteristic of

congested networks. Note also that Harker's model is in terms of monetary costs, whereas

we introduce an abstract input good.

Let c

(x) denote the cost in transportation resources (good G

) required to transport

i;j

0

x units of cargo on the link from i to j . The complete cost functions are:

c

(x) = c

(x) = c

(x) = c

(x) = x

+ 20x;

1;2

2;1

2;4

4;2

2

c

(x) = c

(x) = c

(x) = 2x

+ 5x:

3;1

2;3

3;4

2

Finally, each shipper's ob jective is to transport 10 units of cargo from its origin to its

destination.

11

Wellman

In the basic shipper model, we assume that the shippers pay proportionately (in units

of G

) for the total cost on each link. This amounts to a policy of average cost pricing.

0

We take the shipper's ob jective to be to ship as much as possible (up to its movement

requirement) in the least costly manner. Notice that this ob jective is not expressible in

terms of the consumer's optimization problem, equation (1), and hence this model is not

technically an instance of the general-equilibrium framework.

Given a network with prices on each link, the cheapest cargo movement corresponds to

the shortest path in the graph, where distances are equated with prices. Thus, for a given

link, a shipper would prefer to ship its entire quota on the link if it is on the shortest path,

and zero otherwise. In the case of ties, it is indi(cid:11)erent among the possible allocations. To

bid on link i; j , the shipper can derive the threshold price that determines whether the link

is on a shortest path by taking the di(cid:11)erence in shortest-path distance between the networks

where link i; j 's distance is set to zero and in(cid:12)nity, respectively.

In incrementally changing its bids, the shipper should also consider its outstanding bids

and the current prices. The value of reserving capacity on a particular link is zero if it

cannot get service on the other links on the path. Similarly, if it is already committed to

shipping cargo on a parallel path, it does not gain by obtaining more capacity (even at a

lower price) until it withdraws these other bids.

Therefore, the actual demand policy of

5

a shipper is to spend its uncommitted income on the potential (cid:13)ow increase (derived from

maximum-(cid:13)ow calculations) it could obtain by purchasing capacity on the given link. It is

willing to spend up to the threshold value of the link, as described above. This determines

one point on its demand curve. If it has some unsatis(cid:12)ed requirement and uncommitted

income it also indicates a willingness to pay a lower price for a greater amount of capacity.

Boundary points such as this serve to bootstrap the economy; from the initial conditions it

is typically the case that no individual link contributes to overall (cid:13)ow between the shipper's

origin and destination. Finally, the demand curve is completed by a smoothing operation

on these points.

Details of the boundary points and smoothing operation are rather arbitrary, and I

make no claim that this particular bidding policy is ideal or guaranteed to work for a broad

class of problems. This crude approach appears su(cid:14)cient for the present example and some

similar ones, as long as the shippers' policies become more accurate as the prices approach

equilibrium.

Walras successfully computes the competitive equilibrium for this example, which

in the case of the basic shipper model corresponds to a user equilibrium (UE) for the

transportation network. In the UE for the example network, each shipper sends 2.86 units

of cargo over the shared link 2 ! 3, and the remaining cargo over the direct link from

location 2 to the destination. This allocation is ine(cid:14)cient, as its total cost is 1143 resource

5. Even if a shipper could simultaneously update its bids in all markets, it would not be a good idea to do

so here. A competitive shipper would send all its cargo on the least costly path, neglecting the possibility

that this demand may increase the prices so that it is no longer cheapest. The outstanding bids provide

some sensitivity to this e(cid:11)ect, as they are functions of price. But they cannot respond to changes in

many prices at once, and thus the policy of updating all bids simultaneously can lead to perpetual

oscillation. For example, in the network considered here, the unique competitive equilibrium has each

shipper splitting its cargo between two di(cid:11)erent paths. Policies allocating all cargo to one path can never

lead to this result, and hence convergence to competitive equilibrium depends on the incrementality of

bidding behavior.

12

Market-Oriented Programming

units, which is somewhat greater than the global minimum-cost solution of 1136 units. In

economic terms, the cause of the ine(cid:14)ciency is an externality with respect to usage of the

shared link. Because the shippers are e(cid:11)ectively charged average cost|which in the case

of decreasing returns is below marginal cost|the price they face does not re(cid:13)ect the full

incremental social cost of additional usage of the resource. In e(cid:11)ect, incremental usage of

the resource by one agent is subsidized by the other. The steeper the decreasing returns,

the more the agents have an incentive to overutilize the resource.

This is a simple example

6

of the classic tragedy of the commons.

The classical remedy to such problems is to internalize the externality by allocating

ownership of the shared resource to some decision maker who has the proper incentives to

use it e(cid:14)ciently. We can implement such a solution in walras by augmenting the market

structure with another type of agent.

4.2 Carrier Agents

We extend the basic shipper model by introducing carriers, agents of type producer who

have the capability to transport cargo units over speci(cid:12)ed links, given varying amounts

of transportation resources.

In the model described here, we associate one carrier with

each available link. The production function for each carrier is simply the inverse of the

cost function described above. To achieve a global movement of cargo, shippers obtain

transportation services from carriers in exchange for the necessary transportation resources.

Let C

denote the carrier that transports cargo from location i to location j . Each

i;j

carrier C

is connected to the auction for G

, its output good, along with G

|its input

i;j

i;j

0

in the production process. Shipper agents are also connected to G

, as they are endowed

0

with transportation resources to exchange for transportation services. Figure 4 depicts the

walras market structure when carriers are included in the economy.

G

3,4

G 2,4

C 3,4

S

1,4

G

1,2

C 1,2

G 2,3

C2,3

G

0

C 2,4

C3,1

G 3,1

S4,1

G

2,1

C 2,1

C

4,2

G

4,2

Figure 4: Walras market con(cid:12)guration for the example transportation network in an econ-

omy with shippers and carriers.

6. Average-cost pricing is perhaps the most common mechanism for allocating costs of a shared resource.

Shenker (1991) points out problems with this scheme|with respect to both e(cid:14)ciency and strategic

behavior|in the context of allocating access to congested computer networks, a problem analogous to

our transportation task.

13

Wellman

In the case of a decreasing returns technology, the producer's (carrier's) optimization

problem has a unique solution. The optimal level of activity maximizes revenues minus costs,

which occurs at the point where the output price equals marginal cost. Using this result,

carriers submit supply bids specifying transportation services as a function of link prices

(with resource price (cid:12)xed), and demand bids specifying required resources as a function of

input prices (for activity level computed with output price (cid:12)xed).

For example, consider carrier C

. At output price p

and input price p

, the carrier's

1;2

1;2

0

pro(cid:12)t is

p

y (cid:0) p

c

(y );

1;2

0

1;2

where y is the level of service it chooses to supply. Given the cost function above, this

expression is maximized at y = (p

(cid:0) 20p

)=2p

. Taking p

as (cid:12)xed, the carrier submits a

1;2

0

0

0

supply bid with y a function of p

. On the demand side, the carrier takes p

as (cid:12)xed and

1;2

1;2

submits a demand bid for enough good G

to produce y , where y is treated as a function

0

of p

.

0

With the revised con(cid:12)guration and agent behaviors described, walras derives the sys-

tem equilibrium (SE), that is, the cargo allocation minimizing overall transportation costs.

The derived cargo movements are correct to within 10% in 36 bidding cycles, and to 1%

in 72, where in each cycle every agent submits an average of one bid to one auction. The

total cost (in units of G

), its division between shippers' expenditures and carriers' pro(cid:12)ts,

0

and the equilibrium prices are presented in Table 1. Data for the UE solution of the ba-

sic shipper model are included for comparison. That the decentralized process produces a

global optimum is perfectly consistent with competitive behavior|the carriers price their

outputs at marginal cost, and the technologies are convex.

pricing

TC expense

pro(cid:12)t

p

p

p

p

p

p

p

1;2

2;1

2;3

2;4

3;1

3;4

4;2

MC (SE)

1136

1514

378

40.0

35.7

22.1

35.7

13.6

13.6

40.0

AC (UE)

1143

1143

0

30.0

27.1

16.3

27.1

10.7

10.7

30.0

Table 1: Equilibria derived by walras for the transportation example. TC, MC, and AC

stand for total, marginal, and average cost, respectively. TC = shipper expense (cid:0)

carrier pro(cid:12)t.

As a simple check on the prices of Table 1, we can verify that p

+ p

= p

and

2;3

3;4

2;4

p

+ p

= p

. Both these relationships must hold in equilibrium (assuming all links have

2;3

3;1

2;1

nonzero movements), else a shipper could reduce its cost by rerouting some cargo. Indeed,

for a simple (small and symmetric) example such as this, it is easy to derive the equilibrium

analytically using global equations such as these. But as argued above, it would be improper

to exploit these relationships in the implementation of a truly distributed decision process.

The lesson from this exercise is that we can achieve qualitatively distinct results by sim-

ple variations in the market con(cid:12)guration or agent policies. From our designers' perspective,

we prefer the con(cid:12)guration that leads to the more transportation-e(cid:14)cient SE. Examination

of Table 1 reveals that we can achieve this result by allowing the carriers to earn nonzero

pro(cid:12)ts (economically speaking, these are really rents on the (cid:12)xed factor represented by the

14

Market-Oriented Programming

congested channel) and redistributing these pro(cid:12)ts to the shippers to cover their increased

expenditures. (In the model of general equilibrium with production, consumers own shares

in the producers' pro(cid:12)ts. This closes the loop so that all value is ultimately realized in

consumption. We can specify these shares as part of the initial con(cid:12)guration, just like the

endowment.) In this example, we distribute the pro(cid:12)ts evenly between the two shippers.

4.3 Arbitrageur Agents

The preceding results demonstrate that walras can indeed implement a decentralized

solution to the multicommodity (cid:13)ow problem. But the market structure in Figure 4 is not

as distributed as it might be, in that (1) all agents are connected to G

, and (2) shippers

0

need to know about all links potentially serving their origin-destination pair. The (cid:12)rst of

these concerns is easily remedied, as the choice of a single transportation resource good was

completely arbitrary. For example, it would be straightforward to consider some collection

of resources (e.g., fuel, labor, vehicles), and endow each shipper with only subsets of these.

The second concern can also be addressed within walras. To do so, we introduce yet

another sort of producer agent. These new agents, called arbitrageurs, act as specialized

middlemen, monitoring isolated pieces of the network for ine(cid:14)ciencies. An arbitrageur

A

produces transportation from i to k by buying capacity from i to j and j to k. Its

i;j;k

production function simply speci(cid:12)es that the amount of its output good, G

, is equal to

i;k

the minimum of its two inputs, G

and G

.

If p

+ p

< p

, then its production

i;j

j;k

i;j

j;k

i;k

is pro(cid:12)table.

Its bidding policy in walras is to increment its level of activity at each

iteration by an amount proportional to its current pro(cid:12)tability (or decrement proportional

to the loss). Such incremental behavior is necessary for all constant-returns producers in

walras, as the pro(cid:12)t maximization problem has no interior solution in the linear case.

7

To incorporate arbitrageurs into the transportation market structure, we (cid:12)rst create new

goods corresponding to the transitive closure of the transportation network. In the example

network, this leads to goods for every location pair. Next, we add an arbitrageur A

for

i;j;k

every triple of locations such that (1) i ! j is in the original network, and (2) there exists a

path from j to k that does not traverse location i. These two conditions ensure that there

is an arbitrageur A

for every pair i; k connected by a path with more than one link, and

i;j;k

eliminate some combinations that are either redundant or clearly unpro(cid:12)table.

The revised market structure for the running example is depicted in Figure 5, with new

goods and agents shaded. Some goods and agents that are inactive in the market solution

have been omitted from the diagram to avoid clutter.

Notice that in Figure 5 the connectivity of the shippers has been signi(cid:12)cantly decreased,

as the shippers now need be aware of only the good directly serving their origin-destination

pair. This dramatically simpli(cid:12)es their bidding problem, as they can avoid all analysis of the

price network. The structure as a whole seems more distributed, as no agent is concerned

with more than three goods.

7. Without such a restriction on its bidding behavior, the competitive constant-returns producer would

choose to operate at a level of in(cid:12)nity or zero, depending on whether its activity were pro(cid:12)table or

unpro(cid:12)table at the going prices (at break-even, the producer is indi(cid:11)erent among all

levels). This

would lead to perpetual oscillation, a problem noticed (and solved) by Paul Samuelson in 1949 when he

considered the use of market mechanisms to solve linear programming problems (Samuelson, 1966).

15

Wellman

A

2,3,4

C 2,4

G

2,4

A

1,2,4

G

3,4

C

3,4

G1,4

G 1,2

C 1,2

G 2,3

C2,3

G

0

A2,3,1

C

2,1

G

2,1

C

3,1

G

3,1

A

4,2,1

C

4,2

G

4,2

G

4,1

S

1,4

S

4,1

Figure 5: The revised walras market con(cid:12)guration with arbitrageurs.

Despite the simpli(cid:12)ed shipper behavior, walras still converges to the SE, or optimal

solution, in this con(cid:12)guration. Although the resulting allocation of resources is identical,

a qualitative change in market structure here corresponds to a qualitative change in the

degree of decentralization.

In fact, the behavior of walras on the market con(cid:12)guration with arbitrageurs is vir-

tually identical to a standard distributed algorithm (Gallager, 1977) for multicommodity

(cid:13)ow (minimum delay on communication networks). In Gallager's algorithm, distributed

modules expressly di(cid:11)erentiate the cost function to derive the marginal cost of increasing

(cid:13)ow on a communication link. Flows are adjusted up or down so to equate the marginal

costs along competing subpaths. This procedure provably converges to the optimal solution

as long as the iterative adjustment parameter is su(cid:14)ciently small. Similarly, convergence

in walras for this model requires that the arbitrageurs do not adjust their activity levels

too quickly in response to pro(cid:12)t opportunities or loss situations.

4.4 Summary

The preceding sections have developed three progressively elaborate market con(cid:12)gurations

for the multicommodity (cid:13)ow problem. Table 2 summarizes the size and shape of the con-

(cid:12)guration for a transportation network with V locations and E links, and M movement

requirements. The basic shipper model results in the user equilibrium, while both of the

augmented models produce the globally optimal system equilibrium. The carrier model re-

quires E new producer agents to produce the superior result. The arbitrageur model adds

O(V E ) more producers and potentially some new goods as well, but reduces the number of

goods of interest to any individual agent from O(E ) to a small constant.

These market models represent three qualitatively distinct points on the spectrum of

potential con(cid:12)gurations. Hybrid models are also conceivable, for example, where a partial

set of arbitrageurs are included, perhaps arranged in a hierarchy or some other regular

16

Market-Oriented Programming

model

goods

shippers

carriers

arbitrageurs

Basic shipper

E + 1 M [O(E )]

|

|

: : : plus carriers

E + 1 M [O(E )]

E [2]

|

: : : plus arbitrageurs O(V

)

M [2]

E [2]

O(V E ) [3]

2

Table 2: Numbers of goods and agents for the three market con(cid:12)gurations. For each type of

agent, the (cid:12)gure in brackets indicates the number of goods on which each individual

bids.

structure.

I would expect such con(cid:12)gurations to exhibit behaviors intermediate to the

speci(cid:12)c models studied here, with respect to both equilibrium produced and degree of

decentralization.

5. Limitations

One serious limitation of walras is the assumption that agents act competitively. As

mentioned above, this behavior is rational when there are many agents, each small with

respect to the overall economy. However, when an individual agent is large enough to a(cid:11)ect

prices signi(cid:12)cantly (i.e., possesses market power), it forfeits utility or pro(cid:12)ts by failing to

take this into account. There are two approaches toward alleviating the restriction of perfect

competition in a computational economy. First, we could simply adopt models of imperfect

competition, perhaps based on speci(cid:12)c forms of imperfection (e.g., spatial monopolistic

competition) or on general game-theoretic models. Second, as architects we can con(cid:12)gure

the markets to promote competitive behavior. For example, decreasing the agent's grain size

and enabling free entry of agents should enhance the degree of competition. Perhaps most

interestingly, by controlling the agents' knowledge of the market structure (via standard

information-encapsulation techniques), we can degrade their ability to exploit whatever

market power they possess. Uncertainty has been shown to increase competitiveness among

risk-averse agents in some formal bidding models (McAfee & McMillan, 1987), and in a

computational environment we have substantial control over this uncertainty.

The existence of competitive equilibria and e(cid:14)cient market allocations also depends

critically on the assumption of nonincreasing returns to scale. Although congestion is a

real factor in transportation networks, for example, for many modes of transport there

are often other economies of scale and density that may lead to returns that are increasing

overall (Harker, 1987). Note that strategic interactions, increasing returns, and other factors

degrading the e(cid:11)ectiveness of market mechanisms also inhibit decentralization in general,

and so would need to be addressed directly in any approach.

Having cast walras as a general environment for distributed planning, it is natural to

ask how universal \market-oriented programming"" is as a computational paradigm. We can

characterize the computational power of this model easily enough, by correspondence to the

class of convex programming problems represented by economies satisfying the classical con-

ditions. However, the more interesting issue is how well the conceptual framework of market

17

Wellman

equilibrium corresponds to the salient features of distributed planning problems. Although

it is too early to make a de(cid:12)nitive assertion about this, it seems clear that many planning

tasks are fundamentally problems in resource allocation, and that the units of distribution

often correspond well with units of agency. Economics has been the most prominent (and

arguably the most successful) approach to modeling resource allocation with decentralized

decision making, and it is reasonable to suppose that the concepts economists (cid:12)nd useful

in the social context will prove similarly useful in our analogous computational context.

Of course, just as economics is not ideal for analyzing all aspects of social interaction, we

should expect that many issues in the organization of distributed planning will not be well

accounted-for in this framework.

Finally, the transportation network model presented here is a highly simpli(cid:12)ed ver-

sion of the actual planning problem for this domain. A more realistic treatment would

cover multiple commodity types, discrete movements, temporal extent, hierarchical net-

work structure, and other critical features of the problem. Some of these may be captured

by incremental extensions to the simple model, perhaps applying elaborations developed

by the transportation science community. For example, many transportation models (in-

cluding Harker's more elaborate formulation (Harker, 1987)) allow for variable supply and

demand of the commodities and more complex shipper-carrier relationships. Concepts of

spatial price equilibrium, based on markets for commodities in each location, seem to o(cid:11)er

the most direct approach toward extending the transportation model within walras.

6. Related Work

6.1 Distributed Optimization

The techniques and models described here obviously build on much work in economics,

transportation science, and operations research. The intended research contribution here is

not to these (cid:12)elds, but rather in their application to the construction of a computational

framework for decentralized decision making in general. Nevertheless, a few words are in

order regarding the relation of the approach described here to extant methods for distributed

optimization.

Although the most elaborate walras model is essentially equivalent to existing algo-

rithms for distributed multicommodity (cid:13)ow (Bertsekas & Tsitsiklis, 1989; Gallager, 1977),

the market framework o(cid:11)ers an approach toward extensions beyond the strict scope of this

particular optimization problem. For example, we could reduce the number of arbitrageurs,

and while this would eliminate the guarantees of optimality, we might still have a reasonable

expectation for graceful degradation. Similarly, we could realize conceptual extensions to

the structure of the problem, such as distributed production of goods in addition to trans-

portation, by adding new types of agents. For any given extension, there may very well be

a customized distributed optimization algorithm that would outperform the computational

market, but coming up with this algorithm would likely involve a completely new analysis.

Nevertheless, it must be stated that speculations regarding the methodological advantages

of the market-oriented framework are indeed just speculations at this point, and the relative

(cid:13)exibility of applications programming in this paradigm must ultimately be demonstrated

empirically.

18

Market-Oriented Programming

Finally, there is a large literature on decomposition methods for mathematical program-

ming problems, which is perhaps the most common approach to distributed optimization.

Many of these techniques can themselves be interpreted in economic terms, using the close

relationship between prices and Lagrange multipliers. Again, the main distinction of the

approach advocated here is conceptual. Rather than taking a global optimization prob-

lem and decentralizing it, our aim is to provide a framework for formulating a task in a

distributed manner in the (cid:12)rst place.

6.2 Market-Based Computation

The basic idea of applying economic mechanisms to coordinate distributed problem solving

is not new to the AI community. Starting with the contract net (Davis & Smith, 1983),

many have found the metaphor of markets appealing, and have built systems organized

around markets or market-like mechanisms (Malone, Fikes, Grant, & Howard, 1988). The

original contract net actually did not include any economic notions at all in its bidding

mechanism, however, recent work by Sandholm (1993) has shown how cost and price can

be incorporated in the contract net protocol to make it more like a true market mecha-

nism. Miller and Drexler (Drexler & Miller, 1988; Miller & Drexler, 1988) have examined

the market-based approach in depth, presenting some underlying rationale and addressing

speci(cid:12)c issues salient in a computational environment. Waldspurger, Hogg, Huberman,

Kephart, and Stornetta (1992) investigated the concepts further by actually implementing

market mechanisms to allocate computational resources in a distributed operating system.

Researchers in distributed computing (Kurose & Simha, 1989) have also applied specialized

algorithms based on economic analyses to speci(cid:12)c resource-allocation problems arising in

distributed systems. For further remarks on this line of work, see (Wellman, 1991).

Recently, Kuwabara and Ishida (1992) have experimented with demand adjustment

methods for a task very similar to the multicommodity (cid:13)ow problem considered here. One

signi(cid:12)cant di(cid:11)erence is that their method would consider each path in the network as a

separate resource, whereas the market structures here manipulate only links or location

pairs. Although they do not cast their system in a competitive-equilibrium framework, the

results are congruent with those obtained by walras.

Walras is distinct from these prior e(cid:11)orts in two primary respects. First, it is con-

structed expressly in terms of concepts from general equilibrium theory, to promote math-

ematical analysis of the system and facilitate the application of economic principles to

architectural design. Second, walras is designed to serve as a general programming envi-

ronment for implementing computational economies. Although not developed speci(cid:12)cally

to allocate computational resources, there is no reason these could not be included in mar-

ket structures con(cid:12)gured for particular application domains. Indeed, the idea of grounding

measures of the value of computation in real-world values (e.g., cargo movements) follows

naturally from the general-equilibrium view of interconnected markets, and is one of the

more exciting prospects for future applications of walras to distributed problem-solving.

Organizational theorists have studied markets as mechanisms for coordinating activities

and allocating resources within (cid:12)rms. For example, Malone (1987) models information

requirements, (cid:13)exibility and other performance characteristics of a variety of market and

non-market structures. In his terminology, walras implements a centralized market, where

19

Wellman

the allocation of each good is mediated by an auction. Using such models, we can determine

whether this gross form of organization is advantageous, given information about the cost

of communication, the (cid:13)exibility of individual modules, and other related features. In this

paper, we examine in greater detail the coordination process in computational markets,

elaborating on the criteria for designing decentralized allocation mechanisms. We take the

distributivity constraint as exogenously imposed; when the constraint is relaxable, both

organizational and economic analysis illuminate the tradeo(cid:11)s underlying the mechanism

design problem.

Finally, market-oriented programming shares with Shoham's agent-oriented program-

ming (Shoham, 1993) the view that distributed problem-solving modules are best designed

and understood as rational agents. The two approaches support di(cid:11)erent agent operations

(transactions versus speech acts), adopt di(cid:11)erent rationality criteria, and emphasize dif-

ferent agent descriptors, but are ultimately aimed at achieving the same goal of specifying

complex behavior in terms of agent concepts (e.g., belief, desire, capability) and social orga-

nizations. Combining individual rationality with laws of social interaction provides perhaps

the most natural approach to generalizing Newell's \knowledge level analysis"" idea (Newell,

1982) to distributed computation.

7. Conclusion

In summary, walras represents a general approach to the construction and analysis of

distributed planning systems, based on general equilibrium theory and competitive mech-

anisms. The approach works by deriving the competitive equilibrium corresponding to a

particular con(cid:12)guration of agents and commodities, speci(cid:12)ed using walras's basic con-

structs for de(cid:12)ning computational market structures.

In a particular realization of this

approach for a simpli(cid:12)ed form of distributed transportation planning, we see that qualita-

tive di(cid:11)erences in economic structure (e.g., cost-sharing among shippers versus ownership

of shared resources by pro(cid:12)t-maximizing carriers) correspond to qualitatively distinct be-

haviors (user versus system equilibrium). This exercise demonstrates that careful design of

the distributed decision structure according to economic principles can sometimes lead to

e(cid:11)ective decentralization, and that the behaviors of alternative systems can be meaningfully

analyzed in economic terms.

The contribution of the work reported here lies in the idea of market-oriented program-

ming, an algorithm for distributed computation of competitive equilibria of computational

economies, and an initial illustration of the approach on a simple problem in distributed

resource allocation. A great deal of additional work will be required to understand the pre-

cise capabilities and limitations of the approach, and to establish a broader methodology

for con(cid:12)guration of computational economies.

Acknowledgements

This paper is a revised and extended version of (Wellman, 1992). I have bene(cid:12)ted from

discussions of computational economies with many colleagues, and would like to thank in

particular Jon Doyle, Ed Durfee, Eli Gafni, Daphne Koller, Tracy Mullen, Anna Nagurney,

20

Market-Oriented Programming

Scott Shenker, Yoav Shoham, Hal Varian, Carl Waldspurger, Martin Weitzman, and the

anonymous reviewers for helpful comments and suggestions.

References

Arrow, K. J., & Hurwicz, L. (Eds.). (1977). Studies in Resource Al location Processes.

Cambridge University Press, Cambridge.

Bertsekas, D. P., & Tsitsiklis, J. N. (1989). Paral lel and Distributed Computation. Prentice-

Hall, Englewood Cli(cid:11)s, NJ.

Dafermos, S., & Nagurney, A. (1989). Supply and demand equilibration algorithms for a

class of market equilibrium problems. Transportation Science, 23, 118{124.

Davis, R., & Smith, R. G. (1983). Negotiation as a metaphor for distributed problem

solving. Arti(cid:12)cial Intel ligence, 20, 63{109.

Drexler, K. E., & Miller, M. S. (1988). Incentive engineering for computational resource

management. In Huberman (1988), pp. 231{266.

Eydeland, A., & Nagurney, A. (1989). Progressive equilibration algorithms: The case of

linear transaction costs. Computer Science in Economics and Management, 2, 197{

219.

Gallager, R. G. (1977). A minimum delay routing algorithm using distributed computation.

IEEE Transactions on Communications, 25, 73{85.

Harker, P. T. (1986). Alternative models of spatial competition. Operations Research, 34,

410{425.

Harker, P. T. (1987). Predicting Intercity Freight Flows. VNU Science Press, Utrecht, The

Netherlands.

Harker, P. T. (1988). Multiple equilibrium behaviors on networks. Transportation Science,

22, 39{46.

Haurie, A., & Marcotte, P. (1985). On the relationship between Nash-Cournot and Wardrop

equilibria. Networks, 15, 295{308.

Hicks, J. R. (1948). Value and Capital (second edition). Oxford University Press, London.

Hildenbrand, W., & Kirman, A. P. (1976). Introduction to Equilibrium Analysis: Vari-

ations on Themes by Edgeworth and Walras. North-Holland Publishing Company,

Amsterdam.

Huberman, B. A. (Ed.). (1988). The Ecology of Computation. North-Holland.

Hurwicz, L. (1977). The design of resource allocation mechanisms. In Arrow and Hurwicz

(1977), pp. 3{37. Reprinted from American Economic Review Papers and Proceedings,

1973.

21

Wellman

Kephart, J. O., Hogg, T., & Huberman, B. A. (1989). Dynamics of computational ecosys-

tems. Physical Review A, 40, 404{421.

Koopmans, T. C. (1970). Uses of prices. In Scienti(cid:12)c Papers of Tjal ling C. Koopmans, pp.

243{257. Springer-Verlag. Originally published in the Proceedings of the Conference

on Operations Research in Production and Inventory Control, 1954.

Kurose, J. F., & Simha, R. (1989). A microeconomic approach to optimal resource allocation

in distributed computer systems. IEEE Transactions on Computers, 38, 705{717.

Kuwabara, K., & Ishida, T. (1992). Symbiotic approach to distributed resource allocation:

Toward coordinated balancing. In Pre-Proceedings of the 4th European Workshop on

Modeling Autonomous Agents in a Multi-Agent World.

Lavoie, D., Baetjer, H., & Tulloh, W. (1991). Coping with complexity: OOPS and the

economists' critique of central planning. Hotline on Object-Oriented Technology, 3 (1),

6{8.

Malone, T. W., Fikes, R. E., Grant, K. R., & Howard, M. T. (1988). Enterprise: A market-

like task scheduler for distributed computing environments. In Huberman (1988), pp.

177{205.

Malone, T. W. (1987). Modeling coordination in organizations and markets. Management

Science, 33, 1317{1332.

McAfee, R. P., & McMillan, J. (1987). Auctions and bidding. Journal of Economic Litera-

ture, 25, 699{738.

Milgrom, P., & Roberts, J. (1991). Adaptive and sophisticated learning in normal form

games. Games and Economic Behavior, 3, 82{100.

Miller, M. S., & Drexler, K. E. (1988). Markets and computation: Agoric open systems. In

Huberman (1988), pp. 133{176.

Nagurney, A. (1993). Network Economics: A Variational Inequality Approach. Kluwer

Academic Publishers.

Newell, A. (1982). The knowledge level. Arti(cid:12)cial Intel ligence, 18, 87{127.

Reiter, S. (1986). Information incentive and performance in the (new)

welfare economics. In

2

Reiter, S. (Ed.), Studies in Mathematical Economics. MAA Studies in Mathematics.

Samuelson, P. A. (1966). Market mechanisms and maximization. In Stiglitz, J. E. (Ed.),

The Col lected Scienti(cid:12)c Papers of Paul A. Samuelson, Vol. 1, pp. 415{492. MIT Press,

Cambridge, MA. Originally appeared in RAND research memoranda, 1949.

Samuelson, P. A. (1974). Complementarity: An essay on the 40th anniversary of the Hicks-

Allen revolution in demand theory. Journal of Economic Literature, 12, 1255{1289.

22

Market-Oriented Programming

Sandholm, T. (1993). An implementation of the contract net protocol based on marginal

cost calculations. In Proceedings of the National Conference on Arti(cid:12)cial Intel ligence,

pp. 256{262 Washington, DC. AAAI.

Scarf, H. E. (1984). The computation of equilibrium prices. In Scarf, H. E., & Shoven, J. B.

(Eds.), Applied General Equilibrium Analysis, pp. 1{49. Cambridge University Press,

Cambridge.

Shenker, S. (1991). Congestion control in computer networks: An exercise in cost-sharing.

Prepared for delivery at Annual Meeting of the American Political Science Association.

Shoham, Y. (1993). Agent-oriented programming. Arti(cid:12)cial Intel ligence, 60, 51{92.

Shoven, J. B., & Whalley, J. (1984). Applied general-equilibrium models of taxation and

international trade: An introduction and survey. Journal of Economic Literature, 22,

1007{1051.

Shoven, J. B., & Whalley, J. (1992). Applying General Equilibrium. Cambridge University

Press.

Varian, H. R. (1984). Microeconomic Analysis (second edition). W. W. Norton & Company,

New York.

Waldspurger, C. A., Hogg, T., Huberman, B. A., Kephart, J. O., & Stornetta, S. (1992).

Spawn: A distributed computational economy. IEEE Transactions on Software En-

gineering, 18, 103{117.

Wang, Z., & Slagle, J. (1993). An ob ject-oriented knowledge-based approach for formulating

applied general equilibrium models.

In Third International Workshop on Arti(cid:12)cial

Intel ligence in Economics and Management Portland, OR.

Wellman, M. P. (1991). Review of Huberman (1988). Arti(cid:12)cial Intel ligence, 52, 205{218.

Wellman, M. P. (1992). A general-equilibrium approach to distributed transportation plan-

ning. In Proceedings of the National Conference on Arti(cid:12)cial Intel ligence, pp. 282{289

San Jose, CA. AAAI.

23

","Market price systems constitute a well-understood class of mechanisms that
under certain conditions provide effective decentralization of decision making
with minimal communication overhead. In a market-oriented programming approach
to distributed problem solving, we derive the activities and resource
allocations for a set of computational agents by computing the competitive
equilibrium of an artificial economy. WALRAS provides basic constructs for
defining computational market structures, and protocols for deriving their
corresponding price equilibria. In a particular realization of this approach
for a form of multicommodity flow problem, we see that careful construction of
the decision process according to economic principles can lead to efficient
distributed resource allocation, and that the behavior of the system can be
meaningfully analyzed in economic terms."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1993) 47-59

Submitted 6/93; published 9/93

An Empirical Analysis of Search in GSAT

Ian P. Gent

I.P.Gent@edinburgh.ac.uk

Department of Arti(cid:12)cial Intel ligence, University of Edinburgh

80 South Bridge, Edinburgh EH1 1HN, United Kingdom

Toby Walsh

walsh@loria.fr

INRIA-Lorraine, 615, rue du Jardin Botanique,

54602 Vil lers-les-Nancy, France

Abstract

We describe an extensive study of search in GSAT, an approximation procedure for

propositional satis(cid:12)ability. GSAT performs greedy hill-climbing on the number of satis(cid:12)ed

clauses in a truth assignment. Our experiments provide a more complete picture of GSAT's

search than previous accounts. We describe in detail the two phases of search: rapid hill-

climbing followed by a long plateau search. We demonstrate that when applied to randomly

generated 3-SAT problems, there is a very simple scaling with problem size for both the

mean number of satis(cid:12)ed clauses and the mean branching rate. Our results allow us to

make detailed numerical conjectures about the length of the hill-climbing phase, the average

gradient of this phase, and to conjecture that both the average score and average branching

rate decay exponentially during plateau search. We end by showing how these results can

be used to direct future theoretical analysis. This work provides a case study of how

computer experiments can be used to improve understanding of the theoretical properties

of algorithms.

1. Introduction

Mathematicians are increasingly recognizing the usefulness of experiments with computers

to help advance mathematical theory. It is surprising therefore that one area of mathematics

which has bene(cid:12)tted little from empirical results is the theory of algorithms, especially those

used in AI. Since the ob jects of this theory are abstract descriptions of computer programs,

we should in principle be able to reason about programs entirely deductively. However,

such theoretical analysis is often too complex for our current mathematical tools. Where

theoretical analysis is practical, it is often limited to (unrealistically) simple cases. For

example, results presented in (Koutsoupias & Papadimitriou, 1992) for the greedy algorithm

for satis(cid:12)ability do not apply to interesting and hard region of problems as described in x3.

In addition, actual behaviour on real problems is sometimes quite di(cid:11)erent to worst and

average case analyses. We therefore support the calls of McGeoch (McGeoch, 1986), Hooker

(Hooker, 1993) and others for the development of an empirical science of algorithms. In

such a science, experiments as well as theory are used to advance our understanding of

the properties of algorithms. One of the aims of this paper is to demonstrate the bene(cid:12)ts

of such an empirical approach. We will present some surprising experimental results and

demonstrate how such results can direct future e(cid:11)orts for a theoretical analysis.

The algorithm studied in this paper is GSAT, a randomized hill-climbing procedure for

propositional satis(cid:12)ability (or SAT) (Selman, Levesque, & Mitchell, 1992; Selman & Kautz,

1993a). Propositional satis(cid:12)ability is the problem of deciding if there is an assignment for

(cid:13)1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Gent & Walsh

the variables in a propositional formula that makes the formula true. Recently, there has

been considerable interest in GSAT as it appears to be able to solve large and di(cid:14)cult satis-

(cid:12)ability problems beyond the range of conventional procedures like Davis-Putnam (Selman

et al., 1992). We believe that the results we give here will actually apply to a larger family

of procedures for satis(cid:12)ability called GenSAT (Gent & Walsh, 1993). Understanding such

procedures more fully is of considerable practical interest since SAT is, in many ways, the

archetypical (and intractable) NP-hard problem. In addition, many AI problems can be

encoded quite naturally in SAT (eg. constraint satisfaction, diagnosis and vision interpret-

ation, refutational theorem proving, planning).

This paper is structured as follows. In x2 we introduce GSAT, the algorithm studied

in the rest of the paper. In x3 we de(cid:12)ne and motivate the choice of problems used in our

experiments. The experiments themselves are described in x4. These experiments provide

a more complete picture of GSAT's search than previous informal accounts. The results

of these experiments are analysed more closely in x5 using some powerful statistical tools.

This analysis allow us to make various experimentally veri(cid:12)able conjectures about GSAT's

search. For example, we are able to conjecture: the length of GSAT's initial hill-climbing

phase; the average gradient of this phase; the simple scaling of various important features

like the score (on which hill-climbing is performed) and the branching rate. In x6 we suggest

how such results can be used to direct future theoretical analysis. Finally, in x7 we describe

related work and end with some brief conclusions in x8.

2. GSAT

GSAT is a random greedy hill-climbing procedure. GSAT deals with formulae in conjunct-

ive normal form (CNF); a formula, (cid:6) is in CNF i(cid:11) it is a conjunction of clauses, where a

clause is a disjunction of literals. GSAT starts with a randomly generated truth assignment,

then hill-climbs by \(cid:13)ipping"" the variable assignment which gives the largest increase in

the number of clauses satis(cid:12)ed (called the \score"" from now on). Given the choice between

several equally good (cid:13)ips, GSAT picks one at random. If no (cid:13)ip can increase the score,

then a variable is (cid:13)ipped which does not change the score or (failing that) which decreases

the score the least. Thus GSAT starts in a random part of the search space and searches

for a global solution using only local information. Despite its simplicity, this procedure has

been shown to give good performance on hard satis(cid:12)ability problems (Selman et al., 1992).

procedure GSAT((cid:6))

for i := 1 to Max-tries

T := random truth assignment

for j := 1 to Max-(cid:13)ips

if T satis(cid:12)es (cid:6) then return T

else Poss-(cid:13)ips := set of vars which increase satis(cid:12)ability most

V := a random element of Poss-(cid:13)ips

T := T with V's truth assignment (cid:13)ipped

end

end

return \no satisfying assignment found""

48

An Empirical Analysis of Search in GSAT

In (Gent & Walsh, 1993) we describe a large number of experiments which suggest

that neither greediness not randomness is important for the performance of this procedure.

These experiments also suggest various other conjectures. For instance, for random 3-SAT

problems (see x3) the log of the runtime appears to scale with a less than linear dependency

on the problem size. Conjectures such as these could, as we noted in the introduction,

be very pro(cid:12)tably used to direct future e(cid:11)orts to analyse GSAT theoretically.

Indeed,

we believe that the experiments reported here suggest various conjectures which would be

useful in a proof of the relationship between runtime and problem size (see x6 for more

details)

3. Problem Space

To be able to perform experiments on an algorithm, you need a source of problems on which

to run the algorithm.

Ideally the problems should come from a probability distribution

with some well-de(cid:12)ned properties, contain a few simple parameters and be representative of

problems which occur in real situations. Unfortunately, it is often di(cid:14)cult to meet all these

criteria.

In practice, one is usually forced to accept either problems from a well-de(cid:12)ned

distribution with a few simple parameters or a benchmark set of real problems, necessarily

from some unknown distribution. In these experiments we adopt the former approach and

use CNF formulae randomly generated according to the random k-SAT model.

Problems in random k-SAT with N variables and L clauses are generated as follows:

a random subset of size k of the N variables is selected for each clause, and each vari-

able is made positive or negative with probability

. For random 3-SAT, there is a phase

2

1

transition from satis(cid:12)able to unsatis(cid:12)able when L is approximately 4.3N (Mitchell, Selman,

& Levesque, 1992; Larrabee & Tsuji, 1992; Crawford & Auton, 1993). At lower L, most

problems generated are under-constrained and are thus satis(cid:12)able; at higher L, most prob-

lems generated are over-constrained and are thus unsatis(cid:12)able. As with many NP-complete

problems, problems in the phase transition are typically much more di(cid:14)cult to solve than

problems away from the transition (Cheeseman, Kanefsky, & Taylor, 1991). The region

L=4.3N is thus generally considered to be a good source of hard SAT problems and has

been the focus of much recent experimental e(cid:11)ort.

4. GSAT's search

When GSAT was (cid:12)rst introduced, it was noted that search in each try is divided into two

phases.

In the (cid:12)rst phase of a try, each (cid:13)ip increases the score. However, this phase is

relatively short and is followed by a second phase in which most (cid:13)ips do not increase the

score, but are instead sideways moves which leave the same number of clauses satis(cid:12)ed.

This phase is a search of a \plateau"" for the occasional (cid:13)ip that can increase the score.

1

One of the aims of this paper is to improve upon such informal observations by making

quantitative measurements of GSAT's search, and by using these measurements to make

several experimentally testable predictions.

1. Informal observations to this e(cid:11)ect were made by Bart Selman during the presentation of (Selman et al.,

1992) at AAAI-92. These observations were enlarged upon in (Gent & Walsh, 1992).

49

Gent & Walsh

In our experiments, we followed three methodological principles from (McGeoch, 1986).

First, we performed experiments with large problem sizes and many repetitions, to reduce

variance and allow for emergent properties. Second, we sought good views of the data. That

is, we looked for features of performance which are meaningful and which are as predictable

as possible. Third, we analysed our results closely. Suitable analysis of data may show

features which are not clear from a simple presentation. In the rest of this paper we show

how these principles enabled us to make very detailed conjectures about GSAT's search.

Many features of GSAT's search space can be graphically illustrated by plotting how

they vary during a try. The most obvious feature to plot is the score, the number of satis(cid:12)ed

clauses. In our quest for a good view of GSAT's search space, we also decided to plot \poss-

(cid:13)ips"" at each (cid:13)ip: that is, the number of equally good (cid:13)ips between which GSAT randomly

picks. This is an interesting measure since it indicates the branching rate of GSAT's search

space.

We begin with one try of GSAT on a 500 variable random 3-SAT problem in the

di(cid:14)cult region of L = 4.3N (Figure 1a). Although there is considerable variation between

tries, this graph illustrates features common to all tries. Both score (in Figure 1a) and

poss-(cid:13)ips (in Figure 1b) are plotted as percentages of their maximal values, that is L and N

respectively. The percentage score starts just above 87.5%, which might seem surprisingly

high. Theoretically, however, we expect a random truth assignment in k-SAT to satisfy

k

2

(cid:0)1

7

k

2

8

of all clauses (in this instance,

). As expected from the earlier informal description,

the score climbs rapidly at (cid:12)rst, and then (cid:13)attens o(cid:11) as we mount the plateau. The graph

is discrete since positive moves increase the score by a (cid:12)xed amount, but some of this

discreteness is lost due to the small scale. To illustrate the discreteness, in Figure 1b we

plot the change in the number of satis(cid:12)ed clauses made by each (cid:13)ip (as its exact value,

unscaled). Note that the x-axis for both plots in Figure 1b is the same.

Percentage score

100

 97.5

 95

 92.5

 90

 87.5

Change in score

6

5

4

3

2

1

0

-1

% poss-flips

20

10

0

0

40

80

120

160

200

240
flips

0
0

40
40

80
80

120
120

160
160

200
200

240
240

flips

(a) Score

(b) Change in score, and poss-(cid:13)ips

Figure 1: GSAT's behaviour during one try, N = 500, L = 2150, (cid:12)rst 250 (cid:13)ips

The behaviour of poss-(cid:13)ips is considerably more complicated than that of the score. It

is easiest (cid:12)rst to consider poss-(cid:13)ips once on the plateau. The start of plateau search, after

115 (cid:13)ips, coincides with a very large increase in poss-(cid:13)ips, corresponding to a change from

50

An Empirical Analysis of Search in GSAT

the region where a small number of (cid:13)ips can increase the score by 1 to a region where a

large number of (cid:13)ips can be made which leave the score unchanged. Once on the plateau,

there are several sharp dips in poss-(cid:13)ips. These correspond to (cid:13)ips where an increase by 1

in the score was e(cid:11)ected, as can be seen from Figure 1b. It seems that if you can increase

the score on the plateau, you only have a very small number of ways to do it. Also, the

dominance of (cid:13)ips which make no change in score graphically illustrates the need for such

\sideways"" (cid:13)ips, a need that has been noted before (Selman et al., 1992; Gent & Walsh,

1993).

Perhaps the most fascinating feature is the initial behaviour of poss-(cid:13)ips. There are

four well de(cid:12)ned wedges starting at 5, 16, 26, and 57 (cid:13)ips, with occasional sharp dips.

These wedges demonstrate behaviour analogous to the that of poss-(cid:13)ips on the plateau.

The plateau spans the region where (cid:13)ips typically do not change the score: we call this

region H

since hill-climbing typically makes zero change to the score. The last wedge

0

spans the region H

where hill-climbing typically increases the score by 1, as can be seen

1

very clearly from Figure 1b. Again Figure 1b shows that the next three wedges (reading

right to left) span regions H

, H

, and H

. As with the transition onto the plateau, the

2

3

4

transition between each region is marked by a sharp increase in poss-(cid:13)ips. Dips in the

wedges represent unusual (cid:13)ips which increase the score by more than the characteristic

value for that region, just as the dips in poss-(cid:13)ips on the plateau represent (cid:13)ips where an

increase in score was possible. This exact correlation can be seen clearly in Figure 1b. Note

that in this experiment, in no region H

did a change in score of j + 2 occur, and that there

j

was no change in score of (cid:0)1 at all. In addition, each wedge in poss-(cid:13)ips appears to decay

close to linearly. This is explained by the facts that once a variable is (cid:13)ipped it no longer

appears in poss-(cid:13)ips ((cid:13)ipping it back would decrease score), that most of the variables in

poss-(cid:13)ips can be (cid:13)ipped independently of each other, and that new variables are rarely

added to poss-(cid:13)ips as a consequence of an earlier (cid:13)ip. On the plateau, however, when a

variable is (cid:13)ipped which does not change the score, it remains in poss-(cid:13)ips since (cid:13)ipping it

back also does not change the score.

To determine if this behaviour is typical, we generated 500 random 3-SAT problems

with N=500 and L=4.3N, and ran 10 tries of GSAT on each problem. Figure 2a shows the

mean percentage score

while Figure 2b presents the mean percentage poss-(cid:13)ips together

2

with the mean change in score at each (cid:13)ip. (The small discreteness in this (cid:12)gure is due to

the discreteness of Postscript's plotting.)

The average percentage score is very similar to the behaviour on the individual run of

Figure 1, naturally being somewhat smoother. The graph of average poss-(cid:13)ips seems quite

di(cid:11)erent, but it is to be expected that you will neither observe the sharply de(cid:12)ned dips

in poss-(cid:13)ips from Figure 1b, nor the very sharply de(cid:12)ned start to the wedges, since these

happen at varying times.

It is remarkable that the wedges are consistent enough to be

visible when averaged over 5,000 tries; the smoothing in the wedges and the start of the

plateau is caused by the regions not starting at exactly the same time in each try.

Figure 2 does not distinguish between satis(cid:12)able and unsatis(cid:12)able problems. There

is no current technique for determining the satis(cid:12)ability of 500 variable 3-SAT problems

in feasible time. From instances we have been able to test, we do not believe that large

2. In this paper we assign a score of 100% to (cid:13)ips which were not performed because a satisfying truth

assignment had already been found.

51

Gent & Walsh

Mean percentage score

Mean change in score

100

 97.5

 95

 92.5

 90

 87.5

6

5

4

3

2

1

0

-1

Mean percent poss-flips

20

10

0

0

40

80

120

160

200

240
flips

0

40

80

120

160

200

240

flips

(a) Mean score

(b) Mean change in score, poss-(cid:13)ips

Figure 2: Mean GSAT behaviour, N = 500, L = 4.3N, (cid:12)rst 250 (cid:13)ips

di(cid:11)erences from Figure 2 will be seen when it is possible to plot satis(cid:12)able and unsatis(cid:12)able

problems separately, but this remains an interesting topic to investigate in the future.

Experiments with other values of N with the same ratio of clauses to variables demon-

strated qualitatively similar behaviour. More careful analysis shows the remarkable fact that

not only is the behaviour qualitatively similar, but quantitatively similar, with a simple lin-

ear dependency on N. If graphs similar to Figure 2 are plotted for each N with the x-axis

scaled by N, behaviour is almost identical. To illustrate this, Figure 3 shows the mean

percentage score, percentage poss-(cid:13)ips, and change in score, for N = 500, 750, and 1000, for

L = 4.3N and for the (cid:12)rst 0.5N (cid:13)ips (250 (cid:13)ips at N = 500). Both Figure 3a and Figure 3b

demonstrate the closeness of the scaling, to the extent that they may appear to contain just

one thick line. In Figure 3b there is a slight tendency for the di(cid:11)erent regions of hill-climbing

to become better de(cid:12)ned with increasing N.

The (cid:12)gures we have presented only reach a very early stage of plateau search. To

investigate further along the plateau, we performed experiments with 100, 200, 300, 400,

and 500 variables from 0 to 2.5N (cid:13)ips.

In Figure 4a shows the mean percentage score in

3

each case, while Figure 4b shows the mean percentage poss-(cid:13)ips, magni(cid:12)ed on the y -axis

for clarity. Both these (cid:12)gures demonstrate the closeness of the scaling on the plateau. In

Figure 4b the graphs are not quite so close together as in Figure 4a. The phases of hill-

climbing become much better de(cid:12)ned with increasing N. During plateau search, although

separate lines are distinguishable, the di(cid:11)erence is always considerably less than 1% of the

total number of variables.

The problems used in these experiments (random 3-SAT with L=4.3N) are believed to

be unusually hard and are satis(cid:12)able with probability approximately

. Neither of these

2

1

facts appears to be relevant to the scaling of GSAT's search. To check this we performed

a similar range of experiments with a ratio of clauses to variables of 6. Although almost all

such problems are unsatis(cid:12)able, we observed exactly the same scaling behaviour. The score

3. At 100 variables, 2.5N (cid:13)ips is close to the optimal value for Max-(cid:13)ips. However, experiments have

suggested that Max-(cid:13)ips may need to vary quadratically for larger N (Gent & Walsh, 1993).

52

An Empirical Analysis of Search in GSAT

Mean percentage score

Mean change in score

100

97.5

95

92.5

90

87.5

6

5

4

3

2

1

0

-1

Mean percent poss-flips

20

10

0

000

0.08N

0.16N

0.24N

0.32N

0.40N

0.48N
flips

0

0.08N

0.16N

0.24N

0.32N

0.40N

0.48N
flips

(a) Mean score

(b) Mean change in score, poss-(cid:13)ips

Figure 3: Scaling of mean GSAT behaviour, N = 500, 750, 1000, (cid:12)rst 0.5N (cid:13)ips

Mean percentage

score

Mean percentage poss-flips

100

 97.5

 95

 92.5

 90

 87.5

0

0.4N      0.8N     1.2N      1.6N      2.0N     2.4N
flips

12.5

10

7.5

5

2.5

0

0

0.4N      0.8N     1.2N      1.6N      2.0N     2.4N
flips

(a) mean score, L = 4.3N

(b) mean poss-(cid:13)ips, L = 4.3N

Figure 4: Scaling of mean GSAT behaviour, N = 100, 200, 300, 400, 500

does not reach such a high value as in Figure 4a, as is to be expected, but nevertheless shows

the same linear scaling. On the plateau, the mean value of poss-(cid:13)ips is lower than before.

We again observed this behaviour for L = 3N, where almost all problems are satis(cid:12)able.

The score approaches 100% faster than before, and a higher value of poss-(cid:13)ips is reached

on the plateau, but the decay in the value of poss-(cid:13)ips seen in Figure 4b does not seem to

be present.

To summarise, we have shown that GSAT's hill-climbing goes through several distinct

phases, and that the average behaviour of certain important features scale in linear fashion

with N. These results provide a considerable advance on previous informal descriptions of

GSAT's search.

53

   
   
Gent & Walsh

5. Numerical Conjectures

In this section, we will show that detailed numerical conjectures can be made if the data

presented graphically in x4 is analysed numerically. We divide our analysis into two parts:

(cid:12)rst we deal with the plateau search, where behaviour is relatively simple, then we analyse

the hill-climbing search.

On the plateau, both average score and poss-(cid:13)ips seem to decay exponentially with a

simple linear dependency on problem size. To test this, we performed regression analysis

on our experimental data, using the models

S (x) = N (cid:1) (B (cid:0) C (cid:1) e

)

(1)

x

(cid:0)

A(cid:1)N

P (x) = N (cid:1) (E + F (cid:1) e

)

(2)

x

(cid:0)

D (cid:1)N

where x represents the number of (cid:13)ips, S (x) the average score at (cid:13)ip x and P (x) the average

number of possible (cid:13)ips. To determine GSAT's behaviour just on the plateau, we analysed

data on mean score, starting from 0.4N (cid:13)ips, a time when plateau search always appears to

have started (see x5). Our experimental data (cid:12)tted the model very well. Detailed results for

N = 500 are given in Table 1 to three signi(cid:12)cant (cid:12)gures. The values of A, B, and C change

only slightly with N, providing further evidence for the scaling of GSAT's behaviour. For L

= 3N the asymptotic mean percentage score is very close to 100% of clauses being satis(cid:12)ed,

while for L = 4.3N it is approximately 99.3% of clauses and for L = 6N it is approximately

98.2% of clauses. A good (cid:12)t was also found for mean poss-(cid:13)ips behaviour (see Table 2 for

N = 500), except for L = 3N, where the mean value of poss-(cid:13)ips on the plateau may be

constant. It seems that for L = 4.3N the asymptotic value of poss-(cid:13)ips is about 10% of N

and that for 6 it is about 5% of N.

It is important to note that the behaviour we analysed was for mean behaviour over

both satis(cid:12)able and unsatis(cid:12)able problems. It is likely that individual problems will exhibit

similar behaviour with di(cid:11)erent asymptotes, but we do not expect even satis(cid:12)able problems

to yield a mean score of 100% asymptotically. Note that as N increases a small error in

percentage terms may correspond to a large error in the actual score. As a result, our

predictions of asymptotic score may be inaccurate for large N, or for very large numbers of

(cid:13)ips. Further experimentation is necessary to examine these issues in detail.

L/N N

A

B

C

R

2

3

500

0.511

2.997

0.0428

0.995

4.3

500

0.566

4.27

0.0772

0.995

6

500

0.492

5.89

0.112

0.993

Table 1: Regression results for average score of GSAT.

4

4. The value of R

is a number in the interval [0; 1] indicating how well the variance in data is explained by

2

the regression formula. 1 (cid:0) R

is the ratio between variance of the data from its predicted value, and the

2

variance of the data from the mean of all the data. A value of R

close to 1 indicates that the regression

2

formula (cid:12)ts the data very well.

54

An Empirical Analysis of Search in GSAT

L/N N

D

E

F

R

2

4.3

500

0.838

0.100

0.0348

0.996

6

500

0.789

0.0502

0.0373

0.999

Table 2: Regression results on average poss-(cid:13)ips of GSAT.

We have also analysed GSAT's behaviour during its hill-climbing phase. Figure 1b

shows regions where most (cid:13)ips increase the score by 4, then by 3, then by 2, then by 1.

Analysis of our data suggested that each phase lasts roughly twice the length of the previous

one. This motivates the following conjectures: GSAT moves through a sequence of regions

H

for j = :::; 3; 2; 1 in which the ma jority of (cid:13)ips increase the score by j , and where the

j

length of each region H

is proportional to 2

(except for the region H

which represents

j

0

(cid:0)j

plateau search).

To investigate this conjecture, we analysed 50 tries each on 20 di(cid:11)erent problems for

random 3-SAT problems at N=500 and L=4.3N. We very rarely observe (cid:13)ips in H

that

j

increase the score by less than j , and so de(cid:12)ne H

as the region between the (cid:12)rst (cid:13)ip which

j

increases the score by exactly j and the (cid:12)rst (cid:13)ip which increases the score by less than

j (unless the latter actually appears before the former, in which case H

is empty). One

j

simple test of our conjecture is to compare the total time spent in H

with the total time up

1

j

to the end of H

; we predict that this ratio will be

. For j = 1 to 4 the mean and standard

j

2

deviations of this ratio, and the length of each region are shown in Table 3.

This data

5

supports our conjecture although as j increases each region is slightly longer than predicted.

The total length of hill-climbing at N=500 is 0.22N (cid:13)ips, while at N=100 it is 0.23N. This

is consistent with the scaling behaviour observed in x4.

Region

mean ratio

s.d.

mean length

s.d.

All climbing

|

|

112

7.59

H

0.486

0.0510

54.7

7.69

1

H

0.513

0.0672

29.5

5.12

2

H

0.564

0.0959

15.7

3.61

3

H

0.574

0.0161

7.00

2.48

4

Table 3: Comparative and Absolute Lengths of hill-climbing phases

Our conjecture has an appealing corollary. Namely, that if there are i non-empty hill-

climbing regions, the average change in score per (cid:13)ip during hill-climbing is:

1

1

1

1

1

(cid:1) 1 +

(cid:1) 2 +

(cid:1) 3 +

(cid:1) 4 + (cid:1) (cid:1) (cid:1) +

(cid:1) i (cid:25) 2:

(3)

2

4

8

16

2

i

It follows from this that mean gradient of the entire hill-climbing phase is approximately 2.

At N=500, we observed a mean ratio of change in score per (cid:13)ip during hill-climbing of 1.94

5. The data for \All climbing"" is the length to the start of H

.

0

55

Gent & Walsh

with a standard deviation of 0.1. At N=100, the ratio is 1.95 with a standard deviation of

0.2.

The model presented above ignores (cid:13)ips in H

which increase the score by more than

j

j . Such (cid:13)ips were seen in Figure 1b in regions H

to H

. In our experiment 9.8% of (cid:13)ips

3

1

in H

were of size 2 and 6.3% of (cid:13)ips in H

were of size 3. However, (cid:13)ips of size j + 2

1

2

were very rare, forming only about 0.02% of all (cid:13)ips in H

and H

. We conjectured that

1

2

an exponential decay similar to that in H

occurs in each H

. That is, we conjecture that

0

j

the average change in number of satis(cid:12)ed clauses from (cid:13)ip x to (cid:13)ip x + 1 in H

is given by:

j

x

(cid:0)

D

(cid:1)N

j

j + E

(cid:1) e

(4)

j

This might correspond to a model of GSAT's search in which there are a certain number

of (cid:13)ips of size j + 1 in each region H

, and the probability of making a j + 1 (cid:13)ip is merely

j

dependent on the number of such (cid:13)ips left; the rest of the time, GSAT is obliged to make

a (cid:13)ip of size j . Our data from 1000 tries (cid:12)tted this model well, giving values of R

of 96.8%

2

for H

and 97.5% for H

. The regression gave estimates for the parameters of: D

= 0:045,

1

2

1

E

= 0:25, D

= 0:025, E

= 0:15. Not surprisingly, since the region H

is very short,

1

2

2

3

data was too noisy to obtain a better (cid:12)t with the model (4) than with one of linear decay.

These results support our conjecture, but more experiments on larger problems are needed

to lengthen the region H

for j (cid:21) 3.

j

6. Theoretical Conjectures

Empirical results like those given in x5 can be used to direct e(cid:11)orts to analyse algorithms

theoretically. For example, consider the plateau region of GSAT's search. If the model (1)

applies also to successful tries, the asymptotic score is L, giving

S (x) = L (cid:0) C (cid:1) N (cid:1) e

x

(cid:0)

A(cid:1)N

Di(cid:11)erentiating with respect to x we get,

dS (x)

C

L (cid:0) S (x)

x

=

(cid:1) e

=

(cid:0)

a(cid:1)N

dx

A

A (cid:1) N

The gradient is a good approximation for D

, the average size of a (cid:13)ip at x. Hence,

x

L (cid:0) S (x)

D

=

x

A (cid:1) N

Our experiments suggest that downward (cid:13)ips and those of more than +1 are very rare on the

plateau. Thus, a good ((cid:12)rst order) approximation for D

is as follows, where prob(D

= j )

x

x

is the probability that a (cid:13)ip at x is of size j .

L

X

D

=

j (cid:1) prob(D

= j ) = prob(D

= 1)

x

x

x

j=(cid:0)L

Hence,

prob(D

= 1) =

x

L (cid:0) S (x)

A (cid:1) N

56

An Empirical Analysis of Search in GSAT

That is, on the plateau the probability of making a (cid:13)ip of size +1 may be directly pro-

portional to L (cid:0) S (x), the average number of clauses remaining unsatis(cid:12)ed and inversely

proportional N, to the number of variables. A similar analysis and result can be given for

prob(D

= j + 1) in the hill-climbing region H

, which would explain the model (4) proposed

x

j

in x5.

If our theoretical conjecture is correct, it can be used to show that the mean number

of (cid:13)ips on successful tries will be proportional to N ln N. Further investigation, both ex-

perimental and theoretical, will be needed to determine the accuracy of this prediction.

Our conjectures in this section should be seen as conjectures as to what a formal theory

of GSAT's search might look like, and should be useful in determining results such as av-

erage runtime and the optimal setting for a parameter like Max-(cid:13)ips.

In addition, if we

can develop a model of GSAT's search in which prob(D

= j ) is related to the number

x

of unsatis(cid:12)ed clauses and N as in the above equation, then the experimentally observed

exponential behaviour and linear scaling of the score will follow immediately.

7. Related Work

Prior to the introduction of GSAT in (Selman et al., 1992), a closely related set of proced-

ures were studied by Gu (Gu, 1992). These procedures have a di(cid:11)erent control structure

to GSAT which allows them, for instance, to make sideways moves when upwards moves

are possible. This makes it di(cid:14)cult to compare our results directly. Nevertheless, we are

con(cid:12)dent that the approach taken here would apply equally well to these procedures, and

that similar results could be expected. Another \greedy algorithm for satis(cid:12)ability"" has

been analysed in (Koutsoupias & Papadimitriou, 1992), but our results are not directly

applicable to it because, unlike GSAT, it disallows sideways (cid:13)ips.

In (Gent & Walsh, 1993) we describe an empirical study of GenSAT, a family of pro-

cedures related to GSAT. This study focuses on the importance of randomness, greediness

and hill-climbing for the e(cid:11)ectiveness of these procedures. In addition, we determine how

performance depends on parameters like Max-tries and Max-(cid:13)ips. We showed also that

certain variants of GenSAT could outperform GSAT on random problems. It would be

very interesting to perform a similar analysis to that given here of these closely related

procedures.

GSAT is closely related to simulated annealing (van Laarhoven & Aarts, 1987) and

the Metropolis algorithm, which both use greedy local search with a randomised method of

allowing non-optimal (cid:13)ips. Theoretical work on these algorithms has not applied to SAT

problems, for example (Jerrum, 1992; Jerrum & Sorkin, 1993), while experimental studies of

the relationship between GSAT and simulated annealing have as yet only reached tentative

conclusions (Selman & Kautz, 1993b; Spears, 1993).

Procedures like GSAT have also been successfully applied to constraint satisfaction

problems other than satis(cid:12)ability. For example, (Minton, Johnston, Philips, & Laird, 1990)

proposed a greedy local search procedure which performed well scheduling observations on

the Hubble Space Telescope, and other constraint problems like the million-queens, and

3-colourability. It would be very interesting to see how the results given here map across

to these new problem domains.

57

Gent & Walsh

8. Conclusions

We have described an empirical study of search in GSAT, an approximation procedure for

satis(cid:12)ability. We performed detailed analysis of the two basic phases of GSAT's search,

an initial period of fast hill-climbing followed by a longer period of plateau search. We

have shown that the hill-climbing phases can be broken down further into a number of

distinct phases each corresponding to progressively slower climbing, and each phase lasting

twice as long as the last. We have also shown that, in certain well de(cid:12)ned problem classes,

the average behaviour of certain important features of GSAT's search (the average score

and the average branching rate at a given point) scale in a remarkably simple way with

the problem size We have also demonstrated that the behaviour of these features can be

modelled very well by simple exponential decay, both in the plateau and in the hill-climbing

phase. Finally, we used our experiments to conjecture various properties (eg. the probability

of making a (cid:13)ip of a certain size) that will be useful in a theoretical analysis of GSAT. These

results illustrate how carefully performed experiments can be used to guide theory, and how

computers have an increasingly important r^ole to play in the analysis of algorithms.

Acknowledgements

This research is supported by a SERC Postdoctoral Fellowship to the (cid:12)rst author and

a HCM Postdoctoral fellowship to the second. We thank Alan Bundy, Ian Green, and

the members of the Mathematical Reasoning Group for their constructive comments and

for the quadrillion CPU cycles donated to these and other experiments from SERC grant

GR/H/23610. We also thank Andrew Bremner, Judith Underwood, and the reviewers of

this journal for other help.

References

Cheeseman, P., Kanefsky, B., & Taylor, W. (1991). Where the really hard problems are.

In Proceedings of the 12th IJCAI, pp. 163{169. International Joint Conference on

Arti(cid:12)cial Intelligence.

Crawford, J., & Auton, L. (1993). Experimental results on the crossover point in satis-

(cid:12)ability problems. In Proceedings of the Eleventh National Conference on Arti(cid:12)cial

Intel ligence, pp. 21{27. AAAI Press/The MIT Press.

Gent, I. P., & Walsh, T. (1993). Towards an Understanding of Hill-climbing Procedures for

SAT. In Proceedings of the Eleventh National Conference on Arti(cid:12)cial Intel ligence,

pp. 28{33. AAAI Press/The MIT Press.

Gent, I. P., & Walsh, T. (1992). The enigma of SAT hill-climbing procedures. Research

paper 605, Dept. of Arti(cid:12)cial Intelligence, University of Edinburgh.

Gu, J. (1992). E(cid:14)cient local search for very large-scale satis(cid:12)ability problems. SIGART

Bul letin, 3 (1).

58

An Empirical Analysis of Search in GSAT

Hooker, J. N. (1993). Needed: An empirical science of algorithms. Tech. rep., Graduate

School of Industrial Administration, Carnegie Mellon University, Pittsburgh PA.

Jerrum, M. (1992). Large cliques elude the Metropolis process. Random Structures and

Algorithms, 3 (4), 347{359.

Jerrum, M., & Sorkin, G. (1993). Simulated annealing for graph bisection. Tech. rep.

ECS-LFCS-93-260, Department of Computer Science, University of Edinburgh.

Koutsoupias, E., & Papadimitriou, C. H. (1992). On the greedy algorithm for satis(cid:12)ability.

Information Processing Letters, 43, 53{55.

Larrabee, T., & Tsuji, Y. (1992). Evidence for a Satis(cid:12)ability Threshold for Random 3CNF

Formulas. Tech. rep. UCSC-CRL-92-42, Baskin Center for Computer Engineering and

Information Sciences, University of California, Santa Cruz.

McGeoch, C. (1986). Experimental Analysis of Algorithms. Ph.D. thesis, Carnegie Mellon

University. Also available as CMU-CS-87-124.

Minton, S., Johnston, M. D., Philips, A. B., & Laird, P. (1990). Solving large-scale con-

straint satisfaction and scheduling problems using a heuristic repair method.

In

AAAI-90, Proceedings Eighth National Conference on Arti(cid:12)cial Intel ligence, pp. 17{

24. AAAI Press/MIT Press.

Mitchell, D., Selman, B., & Levesque, H. (1992). Hard and easy distributions of SAT

problems. In Proceedings, 10th National Conference on Arti(cid:12)cial Intel ligence. AAAI

Press/The MIT Press.

Selman, B., & Kautz, H. (1993a). Domain-independent extensions to GSAT: Solving large

structured satis(cid:12)ability problems. In Proceedings, IJCAI-93. International Joint Con-

ference on Arti(cid:12)cial Intelligence.

Selman, B., & Kautz, H. (1993b). An empirical study of greedy local search for satis(cid:12)ability

testing. In Proceedings of the Eleventh National Conference on Arti(cid:12)cial Intel ligence,

pp. 46{51. AAAI Press/The MIT Press.

Selman, B., Levesque, H., & Mitchell, D. (1992). A new method for solving hard satis(cid:12)ability

problems. In Proceedings, 10th National Conference on Arti(cid:12)cial Intel ligence. AAAI

Press/The MIT Press.

Spears, W. M. (1993). Simulated annealing for hard satis(cid:12)ability problems. Tech. rep.

AIC-93-015, AI Center, Naval Research Laboratory.

van Laarhoven, P., & Aarts, E. (1987). Simulated Annealing: Theory and Applications. D.

Reidel Publishing Company, Dordrecht, Holland.

59

","We describe an extensive study of search in GSAT, an approximation procedure
for propositional satisfiability. GSAT performs greedy hill-climbing on the
number of satisfied clauses in a truth assignment. Our experiments provide a
more complete picture of GSAT's search than previous accounts. We describe in
detail the two phases of search: rapid hill-climbing followed by a long plateau
search. We demonstrate that when applied to randomly generated 3SAT problems,
there is a very simple scaling with problem size for both the mean number of
satisfied clauses and the mean branching rate. Our results allow us to make
detailed numerical conjectures about the length of the hill-climbing phase, the
average gradient of this phase, and to conjecture that both the average score
and average branching rate decay exponentially during plateau search. We end by
showing how these results can be used to direct future theoretical analysis.
This work provides a case study of how computer experiments can be used to
improve understanding of the theoretical properties of algorithms."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1993) 91-107

Submitted 8/93; published 11/93

The Di(cid:14)culties of Learning Logic Programs with Cut

Francesco Bergadano

bergadan@di.unito.it

Universit(cid:18)a di Catania, Dipartimento di Matematica,

via Andrea Doria 6, 95100 Catania, Italy

Daniele Gunetti

gunetti@di.unito.it

Umberto Trinchero

trincher@di.unito.it

Universit(cid:18)a di Torino, Dipartimento di Informatica,

corso Svizzera 185, 10149 Torino, Italy

Abstract

As real logic programmers normally use cut (!), an e(cid:11)ective learning procedure for logic

programs should be able to deal with it. Because the cut predicate has only a procedural

meaning, clauses containing cut cannot be learned using an extensional evaluation method,

as is done in most learning systems. On the other hand, searching a space of possible

programs (instead of a space of independent clauses) is unfeasible. An alternative solution

is to generate (cid:12)rst a candidate base program which covers the positive examples, and then

make it consistent by inserting cut where appropriate. The problem of learning programs

with cut has not been investigated before and this seems to be a natural and reasonable

approach. We generalize this scheme and investigate the di(cid:14)culties that arise. Some of the

ma jor shortcomings are actually caused, in general, by the need for intensional evaluation.

As a conclusion, the analysis of this paper suggests, on precise and technical grounds, that

learning cut is di(cid:14)cult, and current induction techniques should probably be restricted to

purely declarative logic languages.

1. Introduction

Much recent research in AI and Machine Learning is addressing the problem of learning

relations from examples, especially under the title of Inductive Logic Programming (Mug-

gleton, 1991). One goal of this line of research, although certainly not the only one, is the

inductive synthesis of logic programs. More generally, we are interested in the construction

of program development tools based on Machine Learning techniques. Such techniques now

include e(cid:14)cient algorithms for the induction of logical descriptions of recursive relations.

However, real logic programs contain features that are not purely logical, most notably the

cut (!) predicate. The problem of learning programs with cut has not been studied before

in Inductive Logic Programming, and this paper analyzes the di(cid:14)culties involved.

1.1 Why Learn Programs with Cut?

There are two main motivations for learning logic programs with cut:

1. ILP should provide practical tools for developing logic programs, in the context of

some general program development methodology (e.g., (Bergadano, 1993b)); as real

size logic programs normally contain cut, learning cut will be important for creating

an integrated Software Engineering framework.

(cid:13)1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Bergadano, Gunetti, & Trinchero

2. Extensive use of cut can make programs sensibly shorter, and the di(cid:14)culty of learning

a given logic program is very much related to its length.

For both of these ob jectives, we need not only cuts that make the programs more

e(cid:14)cient without changing their input-output behavior (\green cuts""), but also cuts that

eliminate some possible computed results (\red cuts""). Red cuts are sometimes considered

bad programming style, but are often useful. Moreover, only the red cuts are e(cid:11)ective in

making programs shorter. Green cuts are also important, and less controversial. Once a

correct program has been inferred via inductive methods, it could be made more e(cid:14)cient

through the insertion of green cuts, either manually or by means of automated program

transformation techniques (Lau & Clement, 1993).

1.2 Why Standard Approaches Cannot be Used?

Most Machine Learning algorithms generate rules or clauses one at a time and independently

of each other: if a rule is useful (it covers some positive example) and correct (it does not

cover any negative example), then it is added to the description or program which is being

generated, until all positive examples have been covered. This means that we are searching

a space of possible clauses, without backtracking. This is obviously a great advantage, as

programs are sets of clauses, and therefore the space of possible programs is exponentially

larger.

The one principle which allows this simpli(cid:12)cation of the problem is the extensional

evaluation of possible clauses, used to determine whether a clause C covers an example

e. The fact that a clause C covers an example e is then used as an approximation of the

fact that a logic program containing C derives e. Consider, for instance, the clause C =

\p(X,Y)   (cid:11)"", and suppose the example e is p(a,b). In order to see whether C covers e, the

extensionality principle makes us evaluate any literal in (cid:11) as true if and only if it matches

some given positive example. For instance, if (cid:11) = q(X,Z) ^ p(Z,Y), then the example p(a,b)

is extensionally covered i(cid:11) there is a ground term c such that q(a,c) and p(c,b) are given

as positive examples.

In particular, in order to obtain the truth value of p(c,b), we will

not need to call other clauses that were learned previously. For this reason, determining

whether C covers e only depends on C and on the positive examples. Therefore, the learning

system will decide whether to accept C as part of the (cid:12)nal program P independently of the

other clauses P will contain.

The extensionality principle is found in Foil (Quinlan, 1990) and its derivatives, but is

also used in bottom-up methods such as Golem (Muggleton & Feng, 1990). Shapiro's MIS

system (Shapiro, 1983) uses it when re(cid:12)ning clauses, although it does not when backtracing

inconsistencies. We have also used an extensional evaluation of clauses in the FILP system

(Bergadano & Gunetti, 1993).

When learning programs with cut, clauses are no longer independent and their stand-

alone extensional evaluation is meaningless. When a cut predicate is evaluated, other pos-

sible clauses for proving the same goal will be ignored. This changes the meaning of these

other clauses. Even if a clause extensionally covers some example e, it may be the case that

the (cid:12)nal program does not derive e, because some derivation paths have been eliminated

by the evaluation of a cut predicate.

92

The Difficulties of Learning Logic Programs with Cut

However, an exhaustive search in a space of programs is prohibitive. Learning methods,

even if based on extensionality, are often considered ine(cid:14)cient if su(cid:14)cient prior information

is not available; searching for sets of clauses will be exponentially worse. This would amount

to a brute-force enumeration of all possible logic programs containing cut, until a program

that is consistent with the given examples is found.

1.3 Is there an Alternative Method?

Cut will only eliminate some computed results, i.e., after adding cut to some program, it

may be the case that some example is no longer derived. This observation suggests a general

learning strategy: a base program P is induced with standard techniques, given the positive

and maybe some of the negative examples, then the remaining negative examples are ruled

out by inserting cut in some clause of P. Obviously, after inserting cut, we must make sure

that the positive examples may still be derived.

Given the present technology and the discussion above, this seems to be the only viable

path to a possible solution. Using standard techniques, the base program P would be gener-

ated one clause at a time, so that the positive examples are extensionally covered. However,

we think this view is too restrictive, as there are programs which derive all given positive

examples, although they do not cover them extensionally (Bergadano, 1993a; DeRaedt,

Lavrac, & Dzeroski, 1993). More generally, we consider traces of the positive examples:

De(cid:12)nition 1 Given a hypothesis space S of possible clauses, and an example e such that S

` e, the set of clauses T(cid:18)S which is used during the derivation of e is cal led a trace for e.

We will use as a candidate base program P any subset of S which is the union of some

traces for the positive examples. If P(cid:18)S extensionally covers the positive examples, then it

will also be the union of such traces, but the converse is not always true. After a candidate

program has been generated, an attempt is made to insert cuts so that the negative examples

are not derived. If this is successful, we have a solution, otherwise, we backtrack to another

candidate base program. We will analyze the many problems inherent in learning cut with

this class of trace-based learning methods, but, as we discuss later (Section 4), the same

problems need to be faced in the more restrictive framework of extensional evaluation. In

other words, even if we choose to learn the base program P extensionally, and then we

try to make it consistent by using cut, the same computational problems would still arise.

The main di(cid:11)erence is that standard approaches based on extensionality do not allow for

backtracking and do not guarantee that a correct solution is found (Bergadano, 1993a).

As far as computational complexity is concerned, trace-based methods have a complexity

standing between the search in a space of independent clauses (for the extensional methods)

and the exhaustive search in a space of possible programs. We need the following:

De(cid:12)nition 2 Given a hypothesis space S, the depth of an example e is the maximum

number of clauses in S successful ly used in the derivation of e.

For example, if we are in a list processing domain, and S only contains recursive calls of

the type \P([HjT]) :- ..., P(T), ..."" then the depth of an example P(L) is the length of L.

For practical program induction tasks, it is often the case that the depth of an example is

93

Bergadano, Gunetti, & Trinchero

related to its complexity, and not to the hypothesis space S. If d is the maximum depth for

the given m positive examples, then the complexity of trace-based methods is of the order

of jS j

, while extensional methods will just enumerate possible clauses with a complexity

md

which is linear in jS j, and enumerating all possible programs is exponential in jS j.

2. A Simple Induction Procedure

The trace-based induction procedure we analyze here takes as input a (cid:12)nite set of clauses

S and a set of positive and negative examples E+ and E- and tries to (cid:12)nd a subset T of S

such that T derives all the positive examples and none of the negative examples. For every

positive example e+ 2 E+, we assume that S is large enough to derive it. Moreover, we

assume that all clauses in S are (cid:13)attened

. If this is not the case, clauses are (cid:13)attened as a

1

preprocessing step.

We consider one possible proof for S ` e+, and we build an intermediate program T (cid:18) S

containing a trace of the derivation. The same is done for the other positive examples, and

the corresponding traces T are merged. Every time T is updated, it is checked against the

negative examples. If some of them are derived from T, cut (!) is inserted in the antecedents

of the clauses in T, so that a consistent program is found, if it exists. If this is not the case,

the procedure backtracks to a di(cid:11)erent proof for S ` e+. The algorithm can be informally

described as follows:

input: a set of clauses S

a set of positive examples E+

a set of negative examples E-

S := (cid:13)atten(S)

T   ;

For each positive example e+ 2 E+

(cid:12)nd T1 (cid:18) S such that T1 `

e+ (backtracking point 1)

SLD

T   T [ T1

if T derives some negative example e- then trycut(T,e-)

if trycut(T,e-) fails then backtrack

output the clauses listed in T

trycut(T,e-):

insert ! somewhere in T (backtracking point 2) so that

1. all previously covered positive examples are still derived from T, and

2. T 6`

e-

SLD

The complexity of adding cut somewhere in the trace T, so that the negative example e-

is no longer derived, obviously only depends on the size of T. But this size depends on the

depth of the positive examples, not on the size of the hypothesis space S. Although more

1. A clause is f lattened if it does not contain any functional symbol. Given an un(cid:13)attened clause, it is alway

possible to (cid:13)atten it (by turning functions into new predicates with an additional argument representing

the result of the function) and vice versa (Rouveirol, in press).

94

The Difficulties of Learning Logic Programs with Cut

clever ways of doing this can be devised, based on the particular example e-, we propose a

simple enumerative technique in the implementation described in the Appendix.

3. Example: Simplifying a List

In this section we show an example of the use of the induction procedure to learn the logic

program \simplif y"". S implif y takes as input a list whose members may be lists, and

transforms it into a \(cid:13)attened"" list of single members, containing no repetitions and no

lists as members. This program appears as exercise number 25 in (Coelho & Cotta, 1988),

is composed of nine clauses (plus the clauses for append and member); six of them are

recursive, one is doubly-recursive and cut is extensively used. Even if simplif y is a not a

very complex logic program, it is more complex than usual ILP test cases. For instance,

the quicksort and partition program, which is very often used, is composed of only (cid:12)ve

clauses (plus those for append), and three of them are recursive. Moreover, note that the

conciseness of simplif y is essentially due to the extensive use of cut. Without cut, this

program would be much longer. In general, the longer a logic program, the more di(cid:14)cult

to learn it.

As a consequence, we start with a relatively strong bias; suppose that the following

hypothesis space of N=8449 possible clauses is de(cid:12)ned by the user:

(cid:15) The clause \simplify(L,NL) :- (cid:13)atten(L,L1), remove(L1,NL).""

(cid:15) All clauses whose head is \(cid:13)atten(X,L)"" and whose body is composed of a conjunction

of any of the following literals:

head(X,H), tail(X,L1), equal(X,[L1,T]), null(T), null(H), null(L1), equal(X,[L1]),

(cid:13)atten(H,X1), (cid:13)atten(L1,X2),

append(X1,X2,L), assign(X1,L), assign(X2,L), list(X,L).

(cid:15) All clauses whose head is \remove(IL,OL)"" and whose body is composed of a con-

junction of any of the following literals:

cons(X,N,OL), null(IL), assign([],OL),

head(IL,X), tail(IL,L), member(X,L), remove(L,OL), remove(L,N).

(cid:15) The correct clauses for null, head, tail, equal, assign, member, append are given:

null([]).

head([Hj ],H).

tail([ jT],T).

equal(X,X).

assign(X,X).

member(X,[Xj ]).

member(X,[ jT]) :- member(X,T).

95

Bergadano, Gunetti, & Trinchero

append([],Z,Z).

append([HjX],Y,[HjZ]) :- append(X,Y,Z).

By using various kinds of constraints, the initial number of clauses can be strongly reduced.

Possible constraints are the following:

(cid:15) Once an output is produced it must not be instantiated again. This means that any

variable cannot occur as output in the antecedent more than once.

(cid:15) Inputs must be used: all input variables in the head of a clause must also occur in its

antecedent.

(cid:15) Some conjunctions of literals are ruled out because they can never be true, e.g.

null(IL)^head(IL,X).

By applying various combination of these constraints it is possible to strongly restrict the

initial hypothesis space, which is then given in input to the learning procedure. The set of

positive and negative examples used in the learning task is:

simplify pos([[[],[b,a,a]],[]],[b,a]). remove pos([a,a],[a]).

(simplify neg([[[],[b,a,a]],[]],X),not equal(X,[b,a])).

simplify neg([[a,b,a],[]],[a,[b,a]]). remove neg([a,a],[a,a]).

Note that we de(cid:12)ne some negative examples of simplif y to be all the examples with

the same input of a given positive example and a di(cid:11)erent output, for instance sim-

plify neg([[[],[b,a,a]],[]],[a,b]). Obviously, it is also possible to give negative examples as

normal ground literals. The learning procedure outputs the program for simplif y reported

below, which turns out to be substantially equivalent to the one described in (Coelho &

Cotta, 1988) (we have kept clauses un(cid:13)attened).

simplify(L,NL) :- (cid:13)atten(L,L1), remove(L1,NL).

(cid:13)atten(X,L) :- equal(X,[L1,T]), null(T), !, (cid:13)atten(L1,X2), assign(X2,L).

(cid:13)atten(X,L) :- head(X,H), tail(X,L1), null(H), !, (cid:13)atten(L1,X2), assign(X2,L).

(cid:13)atten(X,L) :- equal(X,[L1]), !, (cid:13)atten(L1,X2), assign(X2,L).

(cid:13)atten(X,L) :- head(X,H), tail(X,L1), !,

(cid:13)atten(H,X1), !, (cid:13)atten(L1,X2), append(X1,X2,L).

(cid:13)atten(X,L) :- list(X,L).

remove(IL,OL) :- head(IL,X), tail(IL,L), member(X,L), !, remove(L,OL).

remove(IL,OL) :- head(IL,X), tail(IL,L), remove(L,N), cons(X,N,OL).

remove(IL,OL) :- null(IL), assign([],OL).

The learning task takes about 44 seconds on our implementation. However, This is obtained

at some special conditions, which are thoroughly discussed in the next sections:

(cid:15) All the constraints listed above are applied, so that the (cid:12)nal hypothesis space is

reduced to less than one hundred clauses.

96

The Difficulties of Learning Logic Programs with Cut

(cid:15) Clauses in the hypothesis space are generated in the correct order, as they must appear

in the (cid:12)nal program. Moreover, literals in each clause are in the correct position. This

is important, since in a logic program with cut the relative position of clauses and

literals is signi(cid:12)cant. As a consequence, we can learn simplif y without having to test

for di(cid:11)erent clause and literal orderings (see subsections 4.2 and 4.5).

(cid:15) We tell the learning procedure to use at most two cuts per clause. This seems to be

quite an intuitive constraint since, in fact, many classical logic programs have no more

than one cut per clause (see subsections 4.1 and 5.4).

4. Problems

Experiments with the above induction procedure have shown that many problems arise when

learning logic programs containing cut. In the following, we analyze these problems, and

this is a ma jor contribution of the present paper. As cut cannot be evaluated extensionally,

this analysis is general, and does not depend on the speci(cid:12)c induction method adopted.

Some possible partial solutions will be discussed in Section 5.

4.1 Problem 1: Intensional Evaluation, Backtracking and Cut

The learning procedure of Section 2 is very simple, but it can be ine(cid:14)cient. However,

we believe this is common to every intensional method, because clauses cannot be learned

independently of one another. As a consequence, backtracking cannot be avoided and this

can have some impact on the complexity of the learning process. Moreover, cut must be

added to every trace covering negative examples.

If no constraints are in force, we can

range from only one cut in the whole trace to a cut between each two literals of each clause

in the trace. Clearly, the number of possibilities is exponential in the number of literals in

the trace. Fortunately, this number is usually much smaller than the size of the hypothesis

space, as it depends on the depth of the positive examples.

However, backtracking also has some advantages; in particular, it can be useful to search

for alternative solutions. These alternative programs can then be confronted on the basis of

any required characteristic, such as simplicity or e(cid:14)ciency. For example, using backtracking

we discovered a version of simplif y equivalent to the one given but without the cut predicate

between the two recursive calls of the fourth clause of f latten.

4.2 Problem 2: Ordering of Clauses in the Trace

In a logic program containing cut, the mutual position of clauses is signi(cid:12)cant, and a di(cid:11)er-

ent ordering can lead to a di(cid:11)erent (perhaps wrong) behavior of the program. For example,

the following program for intersection:

c

) int(X,S2,Y) :- null(X), null(Y).

1

c

) int(X,S2,Y) :- head(X,H), tail(X,Tail), member(H,S2), !, int(Tail,S2,S), cons(H,S,Y).

2

c

) int(X,S2,Y) :- head(X,H), tail(X,Tail), int(Tail,S2,Y).

3

behaves correctly only if c

comes before c

. Suppose the hypothesis space given in input

2

3

to the induction procedure consists of the same three clauses as above, but with c

before

3

97

Bergadano, Gunetti, & Trinchero

c

. If :int([a],[a],[]) is given as a negative example, then the learning task fails, because

2

clauses c

and c

derive that example.

1

3

In other words, learning a program containing cut means not only to learn a set of

clauses, but also a speci(cid:12)c ordering for those clauses. In terms of our induction procedure

this means that for every trace T covering some negative example, we must check not only

every position for inserting cuts, but also every possible clause ordering in the trace. This

\generate and test"" behavior is not di(cid:14)cult to implement, but it can dramatically decrease

the performance of the learning task. In the worst case all possible permutations must be

generated and checked, and this requires a time proportional to (md)!

for a trace of md

clauses

.

2

The necessity to test for di(cid:11)erent permutations of clauses in a trace is a primary source

of ine(cid:14)ciency when learning programs with cut, and probably the most di(cid:14)cult problem

to solve.

4.3 Problem 3: Kinds of Given Examples

Our induction procedure is only able to learn programs which are traces, i.e. where every

clause in the program is used to derive at least one positive example. When learning de(cid:12)nite

clauses, this is not a problem, because derivation is monotone, and for every program P,

complete and consistent w.r.t. the given examples, there is a program P

(cid:18)P which is also

0

complete and consistent and is a trace

. On the other hand, when learning clauses contain-

3

ing cut, it may happen that the only complete and consistent program(s) in the hypothesis

space is neither a trace, nor contains it as a subset. This is because derivation is no longer

monotone and it can be the case that a negative example is derived by a set of clauses, but

not by a superset of them, as in the following simple example:

S = fsum(A,B,C) :- A>0, !, M is A-1, sum(M,B,N), C is N+1.

sum(A,B,C) :- C is B.g

sum pos(0,2,2), sum neg(2,2,2).

The two clauses in the hypothesis space represent a complete and consistent program for

the given examples, but our procedure is unable to learn it. Observe that the negative

example is derived by the second clause, which is a trace for the positive example, but not

by the (cid:12)rst and the second together.

This problem can be avoided if we require that, for every negative example, a corre-

sponding positive example with the same input be given (in the above case, the example

required is sum pos(2,2,4)). In this way, if a complete program exists in the hypothesis

space, then it is also a trace, and can be learned. Then it can be made consistent using

cut, in order to rule out the derivation of negative examples. The constraint on positive

and negative examples seems to be quite intuitive.

In fact, when writing a program, a

2. it must be noted that if we are learning programs for two di(cid:11)erent predicates, of j and k clauses

respectively (that is, md = j+k), then we have to consider not (j+k)! di(cid:11)erent programs, but only

j !+k!. We can do better if, inside a program, it is known that non-recursive clauses have a (cid:12)xed

position, and can be put before or after of all the recursive clauses.

3. a learned program P is complete if it derives all the given positive examples, and it is consistent if it

does not derive any of the given negative examples

98

The Difficulties of Learning Logic Programs with Cut

programmer usually thinks in terms of what a program should compute on given inputs,

and then tries to avoid wrong computations for those inputs.

4.4 Problem 4: Ordering of Given Examples

When learning clauses with cut, even the order of the positive examples may be signi(cid:12)cant.

In the example above, if sum pos(2,2,4) comes after sum pos(0,2,2) then the learning task

fails to learn a correct program for sum, because it cannot (cid:12)nd a program consistent w.r.t.

the (cid:12)rst positive example and the negative one(s).

In general, for a given set of m positive examples this problem can be remedied by

testing di(cid:11)erent example orderings. Again, in the worst case k! di(cid:11)erent orderings of a set

of k positive examples must be checked. Moreover, in some situations a favorable ordering

does not exist. Consider the following hypothesis space:

c

) int(X,Y,W) :- head(X,A), tail(X,B), notmember(A,Y), int(B,Y,W).

1

c

) int(X,Y,W) :- head(X,A), tail(X,B), notmember(A,Y), !, int(B,Y,W).

2

c

) int(X,Y,Z) :- head(X,A), tail(X,B), int(B,Y,W), cons(A,W,Z).

3

c

) int(X,Y,Z) :- head(X,A), tail(X,B), !, int(B,Y,W), cons(A,W,Z).

4

c

) int(X,Y,Z) :- null(Z).

5

together with the set of examples:

e

) int pos([a],[b],[ ]).

1

e

) int pos([a],[a],[a]).

2

e

) int neg([a],[b],[a]).

3

e

) int neg([a],[a],[ ]).

4

Our induction procedure will not be able to (cid:12)nd a correct program for any ordering of the

two positive examples, even if such a program does exist ([c

,c

,c

]). This program is the

2

4

5

union of two traces: [c

,c

], which covers e

, and [c

,c

], which covers e

. Both of these traces

2

5

1

4

5

2

are inconsistent, because the (cid:12)rst covers e

, and the second covers e

. This problem can

4

3

be remedied only if all the positive examples are derived before the check against negative

examples is done.

However, in that case we have a further loss of e(cid:14)ciency, because some inconsistent

traces are discarded only in the end. In other words, we would need to learn a program

covering all the positive examples, and then make it consistent by using cut and by reorder-

ing clauses. Moreover, there can be no way to make a program consistent by using cut and

reorderings. As a consequence, all the time used to build that program is wasted. As an

example, suppose we are given the following hypothesis space:

0

c

) int(X,Y,Z) :- head(X,A), tail(X,B), int(B,Y,W), cons(A,W,Z).

1

0

c

) int(X,Y,Z) :- null(X), null(Z).

2

0

c

) int(X,Y,Z) :- null(Z).

3

99

Bergadano, Gunetti, & Trinchero

with the examples:

0

e

) int pos([a],[a],[a]).

1

0

e

) int pos([a,b],[c],[]).

2

0

e

) int neg([a],[b],[a]).

3

Then we can learn the trace [c

,c

] from e

and the trace [c

] from e

. But [c

,c

,c

] covers

1

2

1

3

2

1

2

3

0

0

0

0

0

0

0

0

0

e

, and there is no way to make it consistent using cut or by reordering its clauses. In fact,

3

the (cid:12)rst partial trace is responsible for this inconsistency, and hence the time used to learn

0

[c

] is totally wasted.

3

Here it is also possible to understand why we need (cid:13)attened clauses. Consider the fol-

lowing program for intersection, which is equivalent to [c

,c

,c

], but with the three clauses

2

4

5

un(cid:13)attened:

u

) int([AjB],Y,W) :- notmember(A,Y), !, int(B,Y,W).

2

u

) int([AjB],Y,[AjW]) :- !, int(B,Y,W).

4

u

) int( , ,[]).

5

Now, this program covers int neg([a],[a],[]), i.e.

[u

,u

,u

] ` int([a],[a],[]). In fact, clause

2

4

5

u

fails on this example because a is a member of [a]. Clause u

fails because the empty

2

4

list cannot be matched with [AjW]. But clause u

succeeds because its arguments match

5

those of the negative example. As a consequence, this program would be rejected by the

induction procedure.

The problem is that, if we use un(cid:13)attened clauses, it may happen that a clause body is

not evaluated because an example does not match the head of the clause. As a consequence,

possible cuts in that clause are not evaluated and cannot in(cid:13)uence the behavior of the entire

program. In our example, the cut in clause u

has no e(cid:11)ect because the output argument of

4

int([a],[a],[]) does not match [AjW], and the body of u

is not evaluated at all. Then u

is

4

5

(cid:12)red and the negative example is covered. In the (cid:13)attened version, clause c

fails only when

4

cons(a,[],[]) is reached, but at that point a cut is in force and clause c

cannot be activated.

5

Note that program [u

,u

,u

] behaves correctly on the query int([a],[a],X), and gives X=[a]

2

4

5

as the only output.

4.5 Problem 5: Ordering of Literals

Even the relative position of literals and cut in a clause is signi(cid:12)cant. Consider again the

correct program for intersection as above ([c

,c

,c

]), but with c

modi(cid:12)ed by putting the

2

4

5

4

cons literal in front of the antecedent:

0

c

) int(X,Y,Z) :- cons(A,W,Z), head(X,A), tail(X,B), int(B,Y,W).

4

Then, there is no way to get a correct program for intersection using this clause. To rule

out the negative example int neg([a],[a],[]) we must put a cut before the cons predicate,

in order to prevent the activation of c

. But, then, some positive examples are no longer

5

covered, such as int pos([a],[],[]). In fact, we have a wrong behavior every time clause c

is

4

0

100

The Difficulties of Learning Logic Programs with Cut

called and fails, since it prevents the activation on c

. In general, this problem cannot be

5

0

avoided even by reordering clauses: if we put c

after c

and c

, then int neg([a],[a],[]) will

4

2

5

be covered. As a consequence, we should also test for every possible permutation of literals

in every clause of a candidate program.

5. Situations where Learning Cut is still Practical

From the above analysis, learning cut appears to be di(cid:14)cult since, in general, a learning

procedure should be able to backtrack on the candidate base programs (e.g., traces), on

the position of cut(s) in the program, on the order of the clauses in the program, on the

order of literals in the clauses and on the order of given positive examples. However, we

have spotted some general conditions at which learning cut could still be practical. Clearly,

these conditions cannot be a (cid:12)nal solution to learning cut, but, if applicable, can alleviate

the computational problems of the task.

5.1 Small Hypothesis Space

First of all, a restricted hypothesis space is necessary. If clauses cannot be learned inde-

pendently of one another, a small hypothesis space would help to limit the backtracking

required on candidate traces (problem 1). Moreover, even the number of clauses in a trace

would be probably smaller, and hence also the number of di(cid:11)erent permutations and the

number of di(cid:11)erent positions for inserted cuts (problems 2 and 1). A small trace would also

have a slight positive impact on the need to test for di(cid:11)erent literal orderings in clauses

(problem 5).

In general, many kinds of constraints can be applied to keep a hypothesis space small,

such as ij-determinism (Muggleton & Feng, 1990), rule sets and schemata (Kietz & Wrobel,

1991; Bergadano & Gunetti, 1993), determinations (Russell, 1988), locality (Cohen, 1993),

etc (in fact, some of these restrictions and others, such as those listed in Section 3, are

available in the actual implementation of our procedure - see the Appendix

). Moreover,

4

candidate recursive clauses must be designed so that no in(cid:12)nite chains of recursive calls

can take place (Bergadano & Gunetti, 1993) (otherwise the learning task itself could be

non-terminating). In general, the number of possible recursive calls must be kept small, in

order to avoid too much backtracking when searching for possible traces. However, general

constraints may not be su(cid:14)cient. The hypothesis space must be designed carefully from

the very beginning, and this can be di(cid:14)cult. In the example of learning simplif y an initial

hypothesis space of \only"" 8449 clauses was obtained specifying not only the set of required

predicates, but even the variables occurring in every literal.

If clauses cannot be learned independently, experiments have shown to us that a dra-

matic improvement of the learning task can be obtained by generating the clauses in the

hypothesis space so that recursive clauses, and in general more complex clauses, are taken

into consideration after the simpler and non-recursive ones. Since simpler and non recursive

clauses require less time to be evaluated, they will have a small impact on the learning time.

Moreover, learning simpler clauses (i.e. shorter) also alleviates problem 5.

4. We found these constraints particularly useful. By using them we were often able to restrict a hypothesis

space of one order of magnitude without ruling out any possible solution.

101

Bergadano, Gunetti, & Trinchero

Finally, it must be noted that our induction procedure does not necessarily require that

the hypothesis space S of possible clauses be represented explicitly. The learning task could

start with an empty set S and an implicit description of the hypothesis space, for example

the one given in Section 3. When a positive example cannot be derived from S, a new clause

is asked for to a clause generator and added to S. This step is repeated until the example

is derivable from the updated S, and then the learning task can proceed normally.

5.2 Simple Examples

Another improvement can be achieved by using examples that are as simple as possible.

In fact, each example which may involve a recursive call is potentially responsible for the

activation of all the corresponding clauses in the hypothesis space. The more complex the

example, the larger the number of consecutive recursive activations of clauses and the larger

the number of traces to be considered for backtracking (problem 1). For instance, to learn

the append relation, it may be su(cid:14)cient to use an example like append([a],[b],[a,b]) instead

of one like append([a,b,c,d],[b],[a,b,c,d,b]). Since simple examples would probably require

a smaller number of di(cid:11)erent clauses to be derived, this would result in smaller traces,

alleviating the problem of permutation of clauses and literals in a trace (problems 2 and 5)

and decreasing the number of positions for cuts (problem 1).

5.3 Small Number of Examples

Since a candidate program is formed by taking the union of partial traces learned for single

examples, if we want a small trace (problems 2 and 5) we must use as few examples as

possible, while still completely describing the required concept. In other words, we should

avoid redundant information. For example, if we want to learn the program for append, it

will be normally su(cid:14)cient to use only one of the two positive examples append([a],[b],[a,b])

and append([c],[d],[c,d]). Obviously it may happen that di(cid:11)erent examples are derived by

the same set of clauses, and in this case the (cid:12)nal program does not change.

Having to check for all possible orderings of a set of positive examples, a small number of

examples is also a solution to problem 4. Fortunately, experiments have shown that normally

very few positive examples are needed to learn a program, and hence the corresponding

number of di(cid:11)erent orderings is, in any case, a small number. Moreover, since in our

method a positive example is su(cid:14)cient to learn all the clauses necessary to derive it, most

of the time a complete program can be learned using only one well chosen example. If such

an example can be found (as in the case of the learning task of section 3, where only one

example of simplif y and one of remove are given), the computational problem of testing

di(cid:11)erent example orderings is automatically solved.

However, it must be noted that, in general, a small number of examples may not be

su(cid:14)cient, except for very simple programs.

In fact, if we want to learn logic programs

such as member, append, reverse and so on, then any example involving recursion will be

su(cid:14)cient. But for more complex programs the choice may not be trivial. For example, our

procedure is able to learn the quicksort (plus partition) program with only one \good""

example. But if one does not know how quicksort and partition work, it is likely that

she or he will provide an example allowing to learn only a partial description of partition.

This is particularly clear in the example of simplif y . Had we used the positive example

102

The Difficulties of Learning Logic Programs with Cut

simplify pos([[[],[b,a,a]]],[b,a]) (which is very close to the one e(cid:11)ectively used), the (cid:12)rst clause

of f latten would not have been learned. In other words, to give few examples we must give

good examples, and often this is possible only by having in mind (at least partially and in

an informal way) the target program. Moreover, for complex programs, good examples can

mean complex examples, and this is in contrast with the previous requirement. For further

studies of learning from good examples we refer the reader to the work of Ling (1991) and

Aha, Ling, Matwin and Lapointe (1993).

5.4 Constrained Positions for Cut and Literals

Experiments have shown that it is not practical to allow the learning procedure to test all

possible positions of cut in a trace, even if we are able to keep the number of clauses in

a trace small. The user must be able to indicate the positions where a cut is allowed to

occur, e.g., at the beginning of a clause body, or before a recursive call. In this case, many

alternative programs with cut are automatically ruled out and thus do not have to be tested

against the negative examples. It may also be useful to limit the maximum number of cuts

per clause or per trace. For example, most of the time one cut per clause can be su(cid:14)cient

to learn a correct program. In the actual implementation of our procedure, it is in fact

possible to specify the exact position of cut w.r.t. a literal or a group of literals within each

clause of the hypothesis space, when this information is known.

To eliminate the need to test for di(cid:11)erent ordering of literals (problem 5), we may also

impose a particular global order, which must be maintained in every clause of the hypothesis

space. However this requires a deep knowledge of the program we want, otherwise some

(or even all) solutions will be lost. Moreover, this solution can be in contrast with a use of

constrained positions for cut, since a solution program for a particular literal ordering and

for particular positions for cuts may not exist.

6. Conclusion

Our induction procedure is based on an intensional evaluation of clauses. Since the cut

predicate has no declarative meaning, we believe that intensional evaluation of clauses

cannot be abandoned, independently of the kind of learning method adopted. This can

decrease the performance of the learning task, compared with extensional methods, which

examine clauses one at a time without backtracking. However, the computational problems

outlined in Section 4 remain even if we choose to learn a complete program extensionally,

and then we try to make it consistent by inserting cut. The only di(cid:11)erence is that we do

not have backtracking (problem 1), but the situation is probably worse, since extensional

methods can fail to learn a complete program even if it exists in the hypothesis space.

(Bergadano, 1993a).

Even if the ability to learn clauses containing procedural predicates like cut seems to be

fundamental to learning \real"" logic programs, in particular short and e(cid:14)cient programs,

many problems in(cid:13)uencing the complexity of the learning task must be faced. These include

the number and the relative ordering of clauses and literals in the hypothesis space, the kind

and the relative ordering of given examples. Such problems seem to be related to the need

for an intensional evaluation of clauses in general, and not to the particular learning method

adopted. Even just to alleviate these problems, it seems necessary to know a lot about the

103

Bergadano, Gunetti, & Trinchero

target program. An alternative solution is simply to ignore some of the problems. That is,

avoid testing for di(cid:11)erent clause and/or literal and/or example orderings. Clearly, in this

way the learning process can become feasible, but it can fail to (cid:12)nd a solution even when

it exists. However, many ILP systems (such as Foil) adopt such an \incomplete-but-fast""

approach, which is guided by heuristic information.

As a consequence, we view results presented in this paper as, at least partially, nega-

tive. The problems we raised appear computationally di(cid:14)cult, and suggest that attention

should be restricted to purely declarative logic languages, which are, in any case, su(cid:14)ciently

expressive.

Acknowledgements

This work was in part supported by BRA ESPRIT pro ject 6020 on Inductive Logic Pro-

gramming.

Appendix A

The induction procedure of Section 2 is written in C-prolog (interpreted) and runs on a

SUNsparcstation 1. We are planning to translate it in QUINTUS prolog. This Appendix

contains a simpli(cid:12)ed description of its implementation. As a preliminary step, in order to

record a trace of the clauses deriving a positive example e+, every clause in the hypothesis

space

S must be numbered and modi(cid:12)ed by adding to its body two literals. The (cid:12)rst

5

one, allowed(n,m) is used to activate only the clauses which must be checked against the

negative examples. The second one, marker(n), is used to remember that clause number n

has been successfully used while deriving e+. Hence, in general, a clause in the hypothesis

space S takes the following form:

P (X

,: : : ,X

) :- allowed(n,m),(cid:13) ,marker(n).

1

m

where (cid:13) is the actual body of the clause, n is the number of the clause in the set and m is a

number used to deal with cuts. For every clause n, the one without cut is augmented with

allowed(n,0), while those containing a cut somewhere in their body are augmented with

allowed(n,1), allowed(n,2), ..., and so on. Moreover, for every augmented clause as above,

a fact \alt(n,m)."" is inserted in S, in order to implement an enumeration mechanism.

A simpli(cid:12)ed (but running) version of the learning algorithm is reported below. In the

algorithm, the output, if any, is the variable Trace containing the list of the (numbers of the)

clauses representing the learned program P. By using the backtracking mechanism of Prolog,

more than one solution (trace) can be found. We assume the two predicates listpositive

and listnegative build a list of the given positive and negative examples, respectively.

consult((cid:12)le containing the set of clauses S).

5. We assume clauses in the hypothesis space to be (cid:13)attened

104

The Difficulties of Learning Logic Programs with Cut

allowed(X,0).

marker(X) :- assert(trace(X)).

marker(X) :- retract(trace(X)), !, fail.

main :- listpositive(Posexamplelist), tracer([],Posexamplelist,Trace).

tracer(Covered,[ExamplejCdr],Trace) :- Example, /? backtracking point 1 ?/

setof(L,trace(L),Trace1),

notneg(Trace1,[ExamplejCovered],Cdr),

tracer([ExamplejCovered],Cdr,Trace).

tracer( ,[],Trace) :- setof((I,J),allowed(I,J),Trace), asserta((marker(X) :- true, !)).

assertem([]).

assertem([IjCdr]) :- alt(I,J), backassert(allowed(I,J)), assertem(Cdr).

prep(T) :- retract(allowed(X,0)), assertem(T).

backassert(X) :- assert(X).

backassert(X) :- retract(X), !, fail.

resetallowed([]) :- !.

resetallowed( ) :- abolish(allowed,2), assert(allowed(X,0)), !.

notneg(T,Covered,Remaining) :- listnegative([]).

notneg(T,Covered,Remaining) :- listnegative(Negexamplelist),

asserta((marker(X) :- true,!)),

prep(T), /? backtracking point 2 ?/

trypos(Covered), trynegs(Negexamplelist),

resetallowed(Remaining),

retract((marker(X) :- true,!)).

notneg(T,Covered,Remaining) :- resetallowed(Remaining),

retract((marker(X) :- true,!)), !, fail.

trypos([ExamplejCdr]) :- Example, !, trypos(Cdr).

trypos([]) :- !.

trynegs([ExamplejCdr]) :- Example,!,fail.

trynegs([ExamplejCdr]) :- trynegs(Cdr).

trynegs([]) :- !.

Actually, our complete implementation is more complex, also in order to achieve greater

e(cid:14)ciency. The behavior of the learning task is quite simple. Initially, the set S of clauses is

read into the Prolog interpreter, together with the learning algorithm. Then the learning

task can be started by calling the predicate main. A list of the positive examples is formed

105

Bergadano, Gunetti, & Trinchero

and the tracer procedure is called on that list. For every positive example, tracer calls

the example itself, (cid:12)ring all the clauses in S that may be resolved against that example.

Observe that, initially, an allowed(X,0) predicate is asserted in the database: in this way

only clauses not containing a cut are allowed to be used (this is because clauses with cut are

employed only if some negative example is derived). Then, a trace, if any, of (the numbers

associated to) the clauses successfully used in the derivation of that example is built, using

the setof predicate.

The trace is added to the traces found for the previous examples, and the result is

checked against the set of the negative examples calling the notneg procedure. If notneg

does not fail (i.e. no negative examples are covered by this trace) then a new positive

example is taken into consideration. Otherwise notneg modi(cid:12)es the trace with cut and

tests it again. If also this fails, backtracking occurs and a new trace for the current example

(and possibly for the previous ones) is searched for.

The notneg procedure works as follows. First, only the clauses in the trace are allowed

to be checked against the negative examples, by retracting the allowed(X,0) clause and

asserting an allowed(n,0) if the n-th clause (without cut) is in the trace. This is done with

the prep and assertem predicates. Then a list of the negative examples is formed and we

check if they can be derived from the clauses in the trace. If at least one negative example is

covered, (i.e., if trynegs fails) then we backtrack to the prep procedure (backtracking point

2) where a clause of the trace is substituted with an equivalent one but with cut inserted

somewhere (or in a di(cid:11)erent position). If no correct program can be found in such a way

by trying all possible alternatives (i.e. by using cut in all possible ways), notneg fails, and

backtracking to backtracking point 1 occurs, where another trace is searched for. Otherwise,

all clauses in S without cut are reactivated by asserting again allowed(X,0), and the next

positive example is considered. Note that trypos is used in notneg to verify if a modi(cid:12)ed

trace still derives the set of positive examples derived initially. The possibility to substitute

clauses in the current trace with others having cut inserted somewhere is achieved through

the alt predicate in the assertem procedure. Finally, note that this simpli(cid:12)ed version of

the learning procedure is not able to generate and test for di(cid:11)erent orderings of clauses in

a trace or for di(cid:11)erent ordering of literals in each clause, nor to use di(cid:11)erent orderings for

the set of positive examples.

In order to derive all the positive examples before the check against the negative ones

(see subsection 4.4), we must change the (cid:12)rst clause of the tracer procedure into:

tracer([Pos1, ... ,Posn]):-Pos1, ... ,Posn, setof(L,trace(L),T), notneg(T).

The actual implementation of the above induction procedure is available through ftp. For

further information contact gunetti@di.unito.it.

References

Aha, D., Ling, C., Matwin, S., & Lapointe, S. (1993). Learning Singly Recursive Relations

from Small Datasets. In Proceedings of the IJCAI-93 workshop on ILP.

Bergadano, F. (1993a).

Inductive database relations.

IEEE Transactions on Data and

Know ledge Engineering, 5 (6).

106

The Difficulties of Learning Logic Programs with Cut

Bergadano, F. (1993b). Test Case Generation by Means of Learning Techniques. In Pro-

ceedings of ACM SIGSOFT-93.

Bergadano, F., & Gunetti, D. (1993). An interactive system to learn functional logic pro-

grams. In Proceedings of IJCAI-93.

Coelho, H., & Cotta, J. C. (1988). Prolog by Example: how to learn teach and use it. Berlin:

Springer-Verlag.

Cohen, W. (1993). Rapid Prototyping of ILP Systems Using Explicit Bias. In Proceedings

of the IJCAI-93 workshop on ILP.

DeRaedt, L., Lavrac, N., & Dzeroski, S. (1993). Multiple predicate learning. In Proceedings

of IJCAI-93.

Kietz, J. U., & Wrobel, S. (1991). Controlling the Complexity of Learning in Logic through

Syntactic and Task-Oriented Models. In Muggleton, S. (Ed.), Inductive Logic Pro-

gramming. London: Academic Press.

Lau, K. K., & Clement, T. (Eds.). (1993). Logic Program Synthesis and Transformation.

Berlin: Springer-Verlag.

Ling, X. C. (1991). Learning from Good Examples. In Proceedings of IJCAI-91.

Muggleton, S. (Ed.). (1991). Inductive Logic Programming. London: Academic Press.

Muggleton, S., & Feng, C. (1990). E(cid:14)cient Induction of Logic Programs. In Proceedings of

the (cid:12)rst conference on Algorithmic Learning Theory.

Quinlan, R. (1990). Learning Logical De(cid:12)nitions from Relations. Machine Learning, 5,

239{266.

Rouveirol, C. (in press). Flattening: a representation change for generalization. Machine

Learning.

Russell, S. (1988). Tree-structured bias. In Proceedings of AAAI-88.

Shapiro, E. Y. (1983). Algorithmic Program Debugging. Cambridge, CA: MIT Press.

107

","As real logic programmers normally use cut (!), an effective learning
procedure for logic programs should be able to deal with it. Because the cut
predicate has only a procedural meaning, clauses containing cut cannot be
learned using an extensional evaluation method, as is done in most learning
systems. On the other hand, searching a space of possible programs (instead of
a space of independent clauses) is unfeasible. An alternative solution is to
generate first a candidate base program which covers the positive examples, and
then make it consistent by inserting cut where appropriate. The problem of
learning programs with cut has not been investigated before and this seems to
be a natural and reasonable approach. We generalize this scheme and investigate
the difficulties that arise. Some of the major shortcomings are actually
caused, in general, by the need for intensional evaluation. As a conclusion,
the analysis of this paper suggests, on precise and technical grounds, that
learning cut is difficult, and current induction techniques should probably be
restricted to purely declarative logic languages."
"Journal of Artificial Intelligence Research 1 (1993) 61–89

Submitted 8/93; published 11/93

Software Agents: Completing Patterns and
Constructing User Interfaces

Jeffrey C. Schlimmer
Leonard A. Hermens
School of Electrical Engineering & Computer Science,
Washington State University, Pullman, WA 99164-2752, U.S.A.

SCHLIMMER@EECS.WSU.EDU

LHERMENS@EECS.WSU.EDU

Abstract

To  support  the  goal  of  allowing  users  to  record  and  retrieve  information,  this  paper
describes an interactive note-taking system for pen-based computers with two distinctive
features. First, it actively predicts what the user is going to write. Second, it automatically
constructs a custom, button-box user interface on request. The system is an example of a
learning-apprentice  software-agent.  A  machine  learning  component  characterizes  the
syntax  and  semantics  of  the  user’s  information.  A  performance  system  uses  this  learned
information to generate completion strings and construct a user interface.

1. Introduction and Motivation

People  like  to  record  information  for  later  consultation.  For  many,  the  media  of  choice  is
paper. It is easy to use, inexpensive, and durable. To its disadvantage, paper records do not
scale well. As the amount of information grows, retrieval becomes inefﬁcient, physical stor-
age  becomes  excessive,  and  duplication  and  distribution  become  expensive.  Digital  media
offers  better  scaling  capabilities. With  indexing  and  sub-linear  algorithms,  retrieval  is  efﬁ-
cient; using high density devices, storage space is minimal; and with electronic storage and
high-speed networks, duplication and distribution is fast and inexpensive. It is clear that our
computing environments are evolving as several vendors are beginning to market inexpen-
sive, hand-held, highly portable computers that can convert handwriting into text. We view
this as the start of a new paradigm shift in how traditional digital information will be gath-
ered  and  used.  One  obvious  change  is  that  these  computers  embrace  the  paper  metaphor,
eliminating the 
 for typing. It is in this paradigm that our research is inspired, and one of
our primary goals is to combine the best of both worlds by making digital media as conve-
nient as paper.

need

This document describes an interactive note-taking software system for computers with
pen-based input devices. Our software has two distinctive features: ﬁrst, it actively predicts
what the user is going to write and provides a default that the user may select; second, the
software automatically constructs a graphical interface at the user’s request. The purpose of
these  features  is  to  speed  up  information  entry  and  reduce  user  errors.  Viewed  in  a  larger
context, the interactive note-taking system is a type of self-customizing software.

To  clarify  this  notion,  consider  a  pair  of  dimensions  for  characterizing  software.  As
Figure 1  depicts,  one  dimension  is  task  speciﬁcity.  Software  that  addresses  a  generic  task
(e.g.,  a  spreadsheet)  lies  between  task  independent  software  (e.g.,  a  compiler)  and  task
speciﬁc  software  (e.g.,  a  particular  company’s  accounting  software). Another  dimension  is
the  amount  of  user  customization  required  to  make  the  software  useful.  Task  generic  soft-
ware  lies  between  the  two  extremes,  requiring  modest  programming  in  a  specialized

 1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

 
 
(cid:211)
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

High

Visual BASIC

User 
Customization 
Required

Spreadsheets

Low

Self-Customizing

Custom Software

Generic

Task Specificity of Product

Specific

Low

Development Cost / User

High

Figure 1:  Continuum  of  software  development  depicting  the  traditional  trade-off
between the development cost per user and the amount of user customization required.
Self-customizing software eliminates the need for user customization by starting with
partially-specified software and applying machine learning methods to complete any
remaining customization.

language. Self-customizing software uses machine learning techniques to automatically cus-
tomize task generic software to a speciﬁc user. Because the software learns to assist the user
by  watching  them  complete  tasks,  the  software  is  also  a  learning  apprentice.  Similarly,
because  the  user  does  not  explicitly  program  the  defaults  or  the  user  interface  for  the  note
taking system, it is a type of software agent. Agents are a new user interface paradigm that
free the user from having to explicitly command the computer. The user can record informa-
tion directly and in a free-form manner. Behind the interface, the software is acting on behalf
of the user, helping to capture and organize the information.

Next we will introduce the performance component of the note-taking software in more
detail,  then  describe  the  representations  and  algorithms  used  by  the  learning  methods.  We
also  present  empirical  results,  comparing  the  performance  of  seven  alternate  methods  on
nine  realistic  note-taking  domains,  and  ﬁnally,  we  describe  related  research  and  identify
some of the system’s limitations.

62

 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

Figure 2: Screen snapshot of the note-taking software in contextual prompting mode
for a PowerBook note. The two triangles in the lower left are scroller buttons.

2. Performance Task

The primary function of the note-taking software is to improve the user’s speed and accuracy
as they enter notes about various domains of interest. A 
 is a short sequence of descrip-
tive terms that describe a single object of interest. Example 1 shows a note describing a par-
ticular  personal  computer  (recorded  by  the  ﬁrst  author  from  a  Usenet  newsgroup  during
1992):

note

4096K PowerBook 170, 1.4MB and 40MB Int. Drives, 2400/9600 Baud FAX Modem
(Example 1)

Example 2 is a note describing a fabric pattern (recorded by the ﬁrst author’s wife):

Butterick 3611 Size 10 dress, top

(Example 2)

Tables 5 through 11 later in the paper list sample notes drawn from seven other domains.
The user may enter notes from different domains at their convenience and may use whatever
syntactic style comes naturally.

From the user’s point of view, the software operates in one of two modes: a 

contextual
prompting mode, and an interactive graphical interface mode. In the ﬁrst mode, the software
continuously  predicts  a  likely  completion  as  the  user  writes  out  a  note.  It  offers  this  as  a
default  for  the  user.  The  location  and  presentation  of  this  default  must  balance  conﬂicting
requirements  to  be  convenient  yet  unobtrusive.  For  example,  the  hand  should  not  hide  the
indicated default while the user is writing. Our solution is to have a small, colored comple-
tion button follow to the left and below where the user is writing. In this location, it is visible
to either right- or left-handed people as they write out notes. The user can reposition the but-
ton to another location if they prefer. The default text is displayed to the immediate right of
this button in a smaller font. The completion button is green; the text is black. The comple-
tion button saturation ranges from 1 (appearing green), when the software is highly conﬁdent
of the predicted value, to 0 (appearing white), when the software lacks conﬁdence. The but-
ton has a light gray frame, so it is visible even when the software has no prediction. Figure 2

63

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

Figure 3:  Screen  snapshot  of  the  note-taking  software  in  button-box  mode  for  a
PowerBook note.

portrays a screen snapshot of the software operating in the contextual prompting mode for a
PowerBook note.

descriptive terms

button-box  interface

The  software’s  second  mode  presents  an  interactive  graphical  interface.  Instead  of
requiring  the  user  to  write  out  the  text  of  a  note,  the  software  presents  a  radio-button  and
).  With  this,  the  user  may  select
check-box  interface  (what  we  call  a 
from text fragments, portions of notes called 
, by tapping on radio-buttons
or check-boxes with the pen interface device. Each selection from the button-box interface is
added to the current note. Intuitively, check boxes are generated to depict optional descrip-
tive terms, whereas radio-button panels are generated to depict alternate, exclusive descrip-
tive terms. For user convenience, the radio-buttons are clustered into panels and are sorted
alphabetically in ascending order from top to bottom. To allow the user to add new descrip-
tive terms to a button-box panel, an additional blank button is included at the bottom of each.
When the user selects a radio button item, the graphical interface is expanded to depict addi-
tional  choices  corresponding  to  descriptive  terms  that  follow  syntactically.  The  software
indicates its predictions by preselecting the corresponding buttons and highlighting them in
green.  The  user  may  easily  override  the  default  selection  by  tapping  the  desired  button.
Figure 3  portrays  a  screen  snapshot  of  the  software  operating  in  the  interactive  graphical
interface mode for a PowerBook note.

The software is in prompting mode when a user begins to write a note. If the learned syn-
tax for the domain of the note is sufﬁciently mature (see Section 6, Constructing a Button-
Box Interface), then the software can switch into the button-box mode. To indicate this to the
user,  a  mode  switch  depicted  as  a  radio  button  is  presented  for  the  user’s  notice. A  conve-
nient and unobtrusive location for this switch is just below the completion button. In keeping
with the color theme, the mode switch also has a green hue. If the user taps this switch, the
written text is removed, and the appropriate radio buttons and check boxes are inserted. The
system  automatically  selects  buttons  that  match  the  user-written  text.  As  the  user  makes
additional  selections,  the  interface  expands  to  include  additional  buttons.  When  the  user
ﬁnishes  a  note,  in  either  mode,  the  software  returns  to  prompting  mode  in  anticipation  of
  Because  the  interface  is  constructed  from  a  learned  syntax,  as  the  software
another  note.

1

64

 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

reﬁnes its representation of the domains of the notes, the button-box interface also improves.
On-line Appendix 1 is a demonstration of the system’s operation in each of its two modes.

3. Learning a Syntax

To  implement  the  two  modes  of  the  note  taking  software,  the  system  internally  learns  two
structures. To characterize the syntax of user’s notes, it learns ﬁnite-state machines (FSMs).
To generate predictions, it learns decision tree classiﬁers situated at states within the FSMs.
In  order  to  construct  a  graphical  user  interface,  the  system  converts  a  FSM  into  a  set  of
buttons. This section describes the representation and method for learning FSMs. The next
section discusses learning of the embedded classiﬁers.

3.1 Tokenization

Prior  to  learning  a  ﬁnite-state  machine,  the  user’s  note  must  ﬁrst  be  converted  into  a
sequence  of  tokens.  Useful  tokenizers  can  be  domain  independent.  However,  handcrafted
domain-speciﬁc  tokenizers  lead  to  more  useful  representations. The  generic  tokenizer  used
for the results reported here uses normal punctuation, whitespace, and alpha-numeric charac-
ter  boundaries  as  token  delimiters.  For  example,  our  generic  tokenizer  splits  the  sample
PowerBook note in Example 1 into the following 16 tokens:

""

""

""

:NULL
""
""
4096
""
""
 K
""
 PowerBook
""
""
 170
""
, 1.4
""
""
MB
""
 and
""
""
 40
""
""
MB
""
 Int.
""
 Drives
""
, 2400/9600
""
""
 Baud
""
""
 FAX
""
 Modem

"" .

""

""

""

The  token 
constructing a FSM.

:NULL

  is  prepended  by  the  tokenizer.  This  convention  simpliﬁes  the  code  for

3.2 Learning a Finite-State Machine

Deterministic  ﬁnite-state  machines  (FSMs)  are  one  candidate  approach  for  describing  the
syntax of a user’s notes because they are well understood and relatively expressive. More-
over, Angluin (1982) and Berwick and Pilato (1987) present a straightforward algorithm for
learning a speciﬁc subclass of FSMs called k-reversible FSMs. The algorithm is incremental

1. Of the functionality described here, our prototype implements all but the transition from button-box to contex-
tual prompting. The mechanism for such a transition is machine dependent and is not germane to this research.

65

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

start

:NULL

Butterick

start

:NULL

:NULL

Butterick

Butterick

3035

Size

11/12

dress

(a)

3035

Size

11/12

dress

terminal

terminal

3611

Size

10

dress

top

terminal

(b)

Figure 4: (a) Degenerate finite-state machine after processing a single fabric pattern
note, and (b) prefix tree finite-state machine after adding a second fabric pattern note
(cf. Example 2).

and does not suffer from presentation order effects. Berwick and Pilato deﬁne a k-reversible
FSM as:

k-reversible

, where 
whose last k words [tokens] match

 is a non-negative integer, if whenever two
k
“A regular language is 
preﬁxes 
 have a tail in common, then the two preﬁxes
have  all  tails  in  common.  In  other  words,  a  deterministic  ﬁnite-state  automaton  (DFA)
[FSM]  is 
  when  its  sets  of  initial  and
-reversible  if  it  is  deterministic  with  lookahead 
ﬁnal states are swapped and all of its arcs [transitions] are reversed.”

k

k

Given a list of tokens, the k-reversible FSM algorithm ﬁrst constructs a preﬁx tree, where
all  token  sequences  with  common  k-leaders  share  a  k-length  path  through  the  FSM.  For
example,  Figure 4a  depicts  a  simple  FSM  constructed  for  a  single  fabric  pattern  note. The
text of the user’s note was converted into a sequence of tokens. Then a transition was created
for each token and a sequence of states was created to link them together. One state serves as
the initial state, and another indicates the completion of the sequence. For convenience, this
latter, terminal state is depicted with a double circle. If the FSM is able to ﬁnd a transition
for each token in the sequence, and it arrives at the terminal state, then the FSM accepts the
token  sequence  as  an  instance  of  the  language  it  deﬁnes.  Figure 4b  depicts  the  same  FSM
after another path has been added corresponding to a second fabric pattern note (Example 2).
Now  the  FSM  will  accept  either  note  if  expressed  as  a  sequence  of  tokens. This  FSM  is  a
trivial preﬁx tree because only the ﬁrst state is shared between the two paths.

66

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

A k-leader is deﬁned as a path of length k that accepts in the given state.
Merge any two states if either of the following is true:
1. Another state transitions to both states on the same token; or

(This enforces determinism.)

2. Both states have a common k-leader and
a. Both states are accepting states, or
b. Both states transition to a common state via the same token.

Table 1: FSM state merging rules from (Angluin, 1982).

 A preﬁx tree is minimal for observed token sequences, but it may not be general enough
for  use  in  prediction.  (The  preﬁx  tree  is,  in  essence,  an  expensive  method  for  memorizing
token sequences—which is not the desired result.) For the sake of prediction, it is desirable
to  have  a  FSM  that  can  accept  new,  previously  unseen  combinations  of  tokens. The  preﬁx
tree automaton can be converted into a more general FSM by merging some of its states. A
particular method for doing this converts a preﬁx tree into a k-reversible FSM via Angluin’s
(1982) algorithm. The algorithm merges states that have similar transitions, and it creates a
FSM that accepts all token sequences in the preﬁx tree, as well as other candidate sequences.
Table 1 lists the three rules for deciding when to merge a pair of states in a preﬁx tree to form
a  k-reversible  FSM.  In  the  special  case  where  k  equals  zero,  all  states  have  a  common  k-
leader, and Rule 2a ensures that there will be only one accepting state.

Because  the  rules  in  Table 1  must  be  applied  to  each  pair  of  states  in  the  FSM,  and
because  each  time  a  pair  of  states  is  merged  the  process  must  be  repeated,  the  asymptotic
complexity of the process is O(

 is the number of states in the FSM.

), where 

n3

n

Applying these rules to the preﬁx tree in Figure 4b with k equal to zero results in a FSM
depicted  in  Figure 5a.  Notice  that  the  ﬁrst  two  states  have  been  merged  to  make  the  FSM
deterministic  (Rule 1).  The  accepting  states  have  also  been  merged  in  compliance  with
Rule 2a. The resulting FSM has fewer states but is not more general. It only accepts the two
token sequences originally seen. Extending this example, Figure 5b illustrates the addition of
a third fabric pattern note as a preﬁx tree path to the FSM. Reapplying the rules results in the
FSM shown in Figure 6. The ﬁrst two states have been merged as before through the action
of  the  determinism  Rule 1.  Note  that  a  pair  of  latter  states  have  also  been  merged  because
they share a common zero-leader (true of all pairs of states) and because they transition to
"".
the common terminal state on the token ""

dress

Figure 7 depicts a more sophisticated result; it shows a learned zero-reversible FSM for
notes  about  PowerBook  computers.  This  example  shows  that  the  model  number  ""
""  is
never followed by a speciﬁcation for an internal ﬂoppy drive, but that other model numbers
are. Any model may have an external ﬂoppy drive. Note that there is a single terminal state.
Whitespace and punctuation have been eliminated for clarity in the ﬁgure.

100

The  rules  listed  in  Table 1  are  generalization  operators  that  allow  the  FSM  to  accept
previously  unobserved  sequences.  Whenever  two  or  more  states  are  merged  into  one,  the
FSM will accept more sequences than before if the new state is at the tail end of more transi-
tions  than  one  of  the  previous  states  and  if  the  new  state  is  at  the  head  end  of  at  least  one
transition.  For  example,  the  state  just  after  State 1  in  Figure 7  was  merged  from  several
previous states and generalizes memory sizes for PowerBook models. These rules comprise a
heuristic bias and may be too conservative. For example, Figure 8 depicts a FSM for notes

67

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

start

:NULL

start

:NULL

:NULL

Butterick

Butterick

Butterick

3035

Size

11/12

dress

3611

3035

Size

10

Size

11/12

dress

dress

top

terminal

(a)

3674

Size

10

dress

terminal

terminal

3611

Size

10

dress

top

(b)

Figure 5:  (a)  Finite-state  machine  after  processing  two  fabric  pattern  notes  and
applying state merging rules in Table 1, and (b) prefix tree finite-state machine after
adding a third fabric pattern note.

about  fabric  patterns.  Many  of  the  states  prior  to  the  accepting  state  could  be  usefully
merged, but using only the rules listed in Table 1, many more notes will have to be processed
before this happens. If the FSM in Figure 8 were rendered as a button-box interface, it would
reﬂect little of the true structure of the domain of fabric patterns. Table 2 lists specializations
of Rules 2a and 2b and an additional pair of rules we developed to make the FSM generalize
more readily. Note that the parameter k has been set to zero in Rule 2 and to one in Rule 3.
Effectively, two states are merged by Rules 3a or 2b' if they share an incoming or outgoing
transition.  Rule 3b  is  a  Kleene  rule  that  encourages  the  FSM  to  generalize  the  number  of
times a token may appear in a sequence. If one state has a transition to another, then merging
them  will  result  in  a  transition  that  loops  from  and  to  the  newly  merged  state.  Figure 9
depicts a FSM for notes about fabric patterns learned using all three generalization rules in
Table 2. The resulting FSM accurately captures the syntax of the user’s fabric pattern notes
and correctly indicates the syntactically optional tokens that may appear at the end of note.
When rendered as a button-box interface, it clearly depicts the user’s syntax (as illustrated
later  by  Figure 12). The  added  generalization  rules  may  have  only  marginal  effects  on  the
system’s ability to accurately predict a completion as the user writes out a note (as Table 14
below indicates). Their purpose is to improve the quality of the custom interface.

Cohen (1988) uses an interesting alternative representation for learning a syntactic form.
The goal in his work is to guide the generation of proof structures. Intuitively, the represen-
tation is a ﬁnite-state machine that accepts a tree rather than a sequence, and for this reason it
is termed a tree automaton. Like the rules in Tables 1 and 2, tree automatons are generalized

68

 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

start

:NULL

Butterick

3674

Size

10

3611

Size

10

dress

top

3035

Size

11/12

dress

Figure 6: Sample finite-state machine after processing three fabric pattern notes.

terminal

Merge any two states if any of the following are true:
1. Another state transitions to both states on the same token; or

(This enforces determinism.)

2'. Both states have a common 0-leader and
a. Both states are accepting states, or
b. Both states transition to a common state via the same token; or

3. Both states have a common 1-leader and

a. Both states transition to a common state via any token, or
b. One transitions to the other via any token.

Table 2: Extended FSM state merging rules.

by merging states that share similar transitions. Oddly enough, one motivation for using tree
automatons  is  that  they  are  less  likely  to  introduce  extraneous  loops,  the  opposite  of  the
problem  with  the  original  FSM  merging  rules  in  Table 1.  It  is  not  clear  how  to  map  the
sequence of tokens in the user’s notes into a tree structure, but the less sequential nature of
the  tree  automaton  may  help  alleviate  sequencing  problems  in  rendering  the  custom  user
interface (see Section 9, Observations/Limitations).

3.3 Parsing

To use the ﬁnite-state machine for prediction, the software needs a strategy for dealing with
novel tokens. For example, when the user takes a note about a PowerBook computer with a

69

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

start

:NULL

1

2048

4096

6144

8192

160

1.4

MB

and

4

40

170

80

120

K

PowerBook

2

145

20

MB

Int

5

Drive

6

80

MB

Int

and

1.4

MB

Ext

Drives

100

3

140

20

40

2xBattery,
Battery,
Case,
Charger,
FPU,
Video Output

Drives

terminal

14.4

K

9.6

K

and

1.4

MB

Ext

v

32

bis

9600

2400/4800

2400/9600

4800/9600

Baud

7

FAX

Modem

Figure 7: Zero-reversible FSM characterizing PowerBook notes (cf. Example 1).

70

 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

start

:NULL

Butterick

McCall's

Simplicity

4198

3722

4352

6171

3674

3035

3611

4864

5057

5377

5906

5424

5465

Size

Sizes

Size

Size

Size

Size

Suze

Size

Size

Size

Size

12

8-10-12

12

10

10

11/12

10

12

12

11/12

11/12

Dress

Jumper

Dress

Dress

Dress

Jumper

Dress

Top

Skirt

Skirt

Jumper

Top

terminal

Figure 8:  Zero-reversible  finite-state  machine  characterizing  fabric  pattern  notes
learned using merging rules listed in Table 1.

new memory conﬁguration, the FSM will not have a transition for the ﬁrst token. If the soft-
ware is to prompt the user, then it must have a means for deciding where novel tokens lie in
a  note’s  syntax—which  state  to  predict  from.  Without  such  a  mechanism,  no  meaningful
prediction can be generated after novel tokens.

A state may not have a transition for the next token. In general, this is a single symptom
with three possible causes: (1) a novel token has been inserted, (2) a suitable token has been
omitted and the next token would be accepted by a subsequent state, or (3) a token has been
simply  replaced  by  another  in  the  syntax.  For  example,  in  the  sequence  of  tokens  {
,
:NULL
""  is  a  novel  token,  a  familiar  memory  size  has  been  omitted,  and
""
K
12288
""
PowerBook

"",  ""
"" has been replaced by ""

12288

""},  ""

"",  ""

PB

PB

"".

An optimal solution would identify the state requiring a minimum number of insertions,
omissions,  and  replacements  necessary  to  parse  the  new  sequence.  An  efﬁcient,  heuristic
approximation does a greedy search using a special marker. Each time the marked state in the
FSM has a transition for the next token written by the user, the marker is moved forward, and
a  prediction  is  generated  from  that  state.  When  there  is  no  transition  for  the  next  token,  a
greedy  search  is  conducted  for  some  state  (including  the  marked  one  and  those  reachable
from it) that has a transition for some token (including the next one and those following). If
such a state is found, the marker is moved forward to that state, tokens for the transitions of
skipped states are assumed omitted, and novel tokens are assumed inserted. If no state past
the  marker  has  a  transition  for  any  of  the  remaining  tokens,  the  remaining  tokens  are
assumed to be replacements for the same number of the most likely transitions; the marker is
not moved. If the user writes a subsequent token for which some state has a transition, the

71

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

start

:NULL

Butterick

McCall's

Simplicity

3722

4198

4352

6171

3674

3035

3611

4864

5057

5377

5906

5424

5465

Sizes

8-10-12

Size

10

11/12

12

Dress

Jumper

terminal

Jumper

Skirt

Top

Figure 9:  Finite-state  machine  characterizing  fabric  pattern  notes  learned  using
extended  rules  in  Table 2.  Compare  to  zero-reversible  finite-state  machine  for  the
same domain in Figure 8.

.  Because  State 1  doesn’t  have  a  transition  for  the  next  token  ""
K

marker is moved as described above, and the syntax of the user’s note is realigned with the
learned  syntax.  Continuing  with  the  simple  PowerBook  example,  the  marker  is  moved  to
State 1  of  the  FSM  in  Figure 7  because  the  initial  state  had  a  transition  for  the  ﬁrst  token
"",  a  greedy  search
:NULL
"".  The  state  just
is  conducted  to  ﬁnd  a  nearby  state  that  accepts  either  ""
PB
12288
"",  so  the  marker  is  moved  to  that  state.  Another  greedy  search  is
before  State 2  accepts  ""
"".  Because  one  cannot  be  found,  the  heuristic  parsing
started  to  ﬁnd  a  state  that  accepts  ""
assumes  that  it  should  skip  to  the  next  transition.  In  this  case  the  one  labeled  ""
"".
Consequently, the system generates a prediction from State 2 to prompt the user.

12288
"",  or  ""

PowerBook

"",  ""

PB

K

3.4 Multiple Finite-State Machines

If the user decides to take notes about multiple domains, it may be necessary to learn a sepa-
rate syntax for each domain. For example, a single syntax generalized over both the Power-
Book and fabric pattern notes is likely to yield confusing predictions and an unnatural user
interface. Maintenance of multiple ﬁnite-state machines is an instance of the clustering prob-
lem—deciding  which  notes  should  be  clustered  together  to  share  a  FSM. As  Fisher  (1987)
discusses, this involves a trade-off between maximizing similarity within a cluster and mini-
mizing  similarity  between  clusters.  Without  the  ﬁrst  criteria,  all  notes  would  be  put  into  a
single cluster. Without the second criteria, each note would be put into its own cluster.

One obvious approach would be to require the user to prepend each note with a unique
token  to  identify  each  note’s  domain. This  simpliﬁes  the  clustering  computation. All  notes
sharing the ﬁrst token would share a FSM. However, with this scheme, the user would have

72

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

start
terminal

X

Figure 10: Simple finite-state machine with one state.

to  remember  the  identifying  token  or  name  for  each  domain. An  interface  could  provide  a
pop-up  list  of  all  previously  used  domain  identiﬁers.  This  is  not  satisfactory  because  it
requires overhead not needed when taking notes on paper.

An  alternative  approach  doesn’t  require  any  extra  effort  on  the  part  of  the  user. A  new
note is grouped with the FSM that skips the fewest of its tokens. This heuristic encourages
within cluster similarity because a FSM will accept new token sequences similar to those it
summarizes. To inhibit the formation of single-note FSMs, a new FSM is constructed only if
all other FSMs skip more than half of the new note’s tokens. This is a parametrized solution
to encourage between-cluster dissimilarity.

4. Learning Embedded Classiﬁers

Finite-state  machines  are  useful  representations  for  capturing  the  syntax  of  a  user’s  notes,
and they are easy to learn. When predicting a note’s completion, it is essential that a predic-
tion be made from the correct state in the FSM (as discussed above). It is also necessary to
decide whether to terminate (indicating acceptance of the note) or continue prediction, and,
in the later case, which transition to predict. To facilitate these decisions, the FSM can main-
tain a count of how many times parsing terminated and how many times each transition was
taken. Prediction can then return the option with the maximum frequency.

Figure 10 depicts a FSM for which this method will prove insufﬁcient. There is only one
state,  an  accepting  state,  and  the  transition  corresponding  to  the  token  ""
""  is  optional.  (This
corresponds to a check box interface item.) There are two problems with a frequency-based
prediction. First, the FSM does not indicate that the transition is to be taken at most once, yet
this is quite clear from the user interface. Second, simple frequency-based prediction would
always recommend termination and never the transition. The FSM accepts whether the box is
checked or not, thus the frequency of termination is greater than or equal to the frequency of
the transition. This problem arises whenever there is a loop.

X

Embedding general classiﬁers in a FSM can alleviate some of the FSM’s representational
shortcomings. For example, in the FSM depicted in Figure 10, a decision tree embedded in
this  state  easily  tests  whether  the  transition  has  already  been  taken  and  can  advise  against
repeating it. Moreover, a classiﬁer can predict based on previous transitions rather than just
the  frequency  of  the  current  state’s  transitions. Therefore,  a  decision  tree  embedded  in  the
state  of  Figure 10  can  predict  when  the  transition  should  be  taken  as  a  function  of  other,
earlier tokens in the sequence. Table 3 lists sample decision trees embedded in states of the
FSM depicted in Figure 7. The ﬁrst tree tests which token was parsed by a distant state, in
effect  augmenting  the  FSM  representation.  It  relates  memory  size  to  hard  disk  capacity
(small  amounts  of  memory  correlate  with  a  small  hard  disk).  The  second  tree  prevents  an
optional  loop  from  being  taken  a  second  time  by  testing  to  see  if  the  state  has  yet  been
visited during a parse of the note. After processing additional notes, this second decision tree

73

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

Decision tree embedded in State 3:
If State 1 exited with ""2048""
Then predict "" 20""

Else if with ""4096""

Then predict "" 40""

Else if with ""6144""

Then predict "" 40""

Else if with ""8192""

Then predict "" 40"" .

Decision tree embedded in State 7:
If State 7 has not been visited
Then predict "" FAX""

Else if State 7 exited with "" FAX""
Then predict "" Modem"" .

Table 3:  Sample  decision  trees  embedded  in  the  finite-state  machine  depicted  in
Figure 7.

becomes more complex as the system tries to predict which PowerBooks have FAX modems
and which do not.

A classiﬁer is trained for each state in the FSM which: (a) has more than one transition,
or (b) is marked as a terminal state but also has a transition. The classiﬁers are updated incre-
mentally after the user ﬁnishes each note. The classiﬁer’s training data are token sequences
parsed at this state. The class value of the data is the transition taken from, or termination at,
this state by the token sequences. Only those classiﬁers whose states are used in a parse are
updated. The attributes of the data are the names of states prior to this one, and the values of
the attributes are the transitions taken from those states. A distinct attribute is deﬁned each
time  a  state  is  visited  during  a  given  parse,  so  when  a  loop  transition  is  taken  a  speciﬁc
attribute reﬂects this fact. For any of the attributes, if the corresponding state was not visited
while parsing the token sequence, the attribute has a special, empty value.

Consider  the  PowerBook  FSM  shown  in  Figure 7.  A  classiﬁer  would  be  embedded  at
States 1, 2, 3, 4, 5, 6, 7. A training example corresponding to the note in Example 1 for the
classiﬁer at State 6 would be:

Attributes:
S1
S2
S3
S4
S5
S6
S7
S7-1
Class:

Values:
""
4096
""
 170

= ""
= ""
= NIL
= ""
""
 40
= ""
 Drives
= ""
, 2400/9600
= ""
 FAX
= ""
 Modem
=
:TERMINATE

 .

""

""

""

""

Note  that  there  is  no  value  for  State 3,  denoting  that  it  wasn’t  visited  during  the  parse  of
Example 1. Also there are two attributes for State 7 denoting that it has been visited twice. 

The classiﬁer gives informed advice about which transition to take or whether to termi-
nate. The  FSM  in  turn  gives  the  classiﬁer  a  speciﬁc  context  for  operation.  If  only  a  single
classiﬁer were used to predict the next token, it would be hard pressed to represent the differ-
ent  predictions  required.  The  domain  is  naturally  narrowed  by  the  FSM  and  therefore
reduces  the  representational  demands  on  the  classiﬁer.  Later,  we  present  empirical  results

74

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

comparing  a  single  classiﬁer  to  a  set  of  classiﬁers  embedded  in  a  FSM. The  ﬁndings  there
show  that  the  latter  outperforms  the  former,  conﬁrming  the  intuition  that  learning  is  more
effective if situated within a narrow context.

From the classiﬁer’s point of view, the learning task is non-stationary. The concept to be
learned is changing over time because the structure of the FSM is changing. When two states
are merged, one of the two classiﬁers is discarded. The other is now embedded in a different
position  in  the  FSM,  and  it  sees  different  training  data.  Similarly,  when  other  states  are
merged, the attributes of the training data also change. To help mitigate this effect, the new
state  takes  the  oldest  identiﬁer  assigned  to  the  two  merged  states.  Empirical  results  in
Table 14  illustrate  that  the  FSM  does  not  have  to  be  ﬁxed  before  the  classiﬁer  can  learn
useful information.

5. Contextual Prompting

In  the  prompting  mode,  the  software  continuously  predicts  a  likely  completion  as  the  user
writes  out  a  note.  It  presents  this  as  a  default  next  to  the  completion  button.  The  button’s
saturation ranges from white to green in proportion to the conﬁdence of the prediction. If the
user taps the completion button, the prompt text is inserted at the end of the current note.

A completion is generated by parsing the tokens already written by the user, ﬁnding the
last state visited in the FSM, and predicting the next most likely transition (or termination).
This  process  is  repeated  until  a  stopping  criterion  is  satisﬁed,  which  is  discussed  below.  If
the last token written by the user is incomplete, matching only a preﬁx of a state’s transition,
then  the  remainder  of  that  transition  is  predicted.  If  the  last  token  matches  more  than  one
transition, a generalized string is predicted using special characters to indicate the type and
number  of  characters  expected.  If  a  digit  is  expected,  a  ""
""  is
""  is  included;  and  if  some  transition’s  tokens  are  longer
included;  if  either  are  possible,  a  ""
?
the  user  has  written
to 
is  appended 
than  others,  a  ""
""
"",
"",  the  possible  values  for  PowerBook  models  of  ""
4096K PowerBook 1
100
""
160C

"" are generalized, and the prompt is ""

""  is  included;  if  a  letter,  an  ""

the  end.  For  example, 

"", and ""

"",  ""

140

170

#0…

if 

"".

"" 

…

#

a

A  simple  calculation  is  used  to  compute  the  conﬁdence  of  the  prediction  and  set  the

button’s color saturation. It is the simple ratio

(
f prediction
+(
)
1

)

skipped

)

(

f

total

)

(
f prediction

  is  the  frequency  of  the  predicted  arc  (or  terminate)  [i.e.,  the  number  of
where 
  is  the  total
times  this  choice  was  taken  while  parsing  previously  observed  notes], 
  is  the  number  of  tokens  skipped  during
frequency  of  all  arcs  (and  terminate),  and 
heuristic parsing (cf. Section 3.3, Parsing). Conﬁdence is directly proportional to the simple
likelihood of the prediction and is degraded in proportion to the number of tokens the FSM
had to skip to get to this point. This information is used in a simple way, so it is unclear if
more sophisticated measures are needed.

skipped

total

(

)

f

The stopping criterion is used to determine how much of a prompt to offer the user. At
one extreme, only a single token can be predicted. This gives the user little context and may
not provide much assistance. At the other extreme, a sequence of tokens that completes the
note can be predicted. This may be too lengthy, and the user would have to edit the prompt if
selected. The stopping criterion in Table 4 balances these two extremes and attempts to limit
prompts to a consistent set of tokens. In particular, Condition 3 stops expanding the prompt

75

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
·
S

CHLIMMER 

& H

ERMENS

Stop expanding the prompt if any of the following are true:
1. The next prediction is to terminate; or
2. The next prediction is a generalized string; or
3. At least one token has already been predicted and
a. The prediction starts with punctuation, or
b. The conﬁdence of the prediction is lower; or

4. The next prediction is the same as the last prediction; or
5. More than 10 tokens have already been predicted.

Table 4: Stopping criterion for contextual prompting.

upon  reaching  a  syntactic  boundary  (leading  punctuation)  or  upon  reaching  a  semantic
boundary (falling conﬁdence).

6. Constructing a Button-Box Interface

In the button-box mode, the software presents an interactive graphical interface. Instead of
writing out the note, the user may select note fragments by tapping buttons. To switch from
contextual mode to button-box mode, a green radio button indicator is displayed below the
completion button when the software is conﬁdent about the user’s syntax. If the user taps this
indicator, the existing text is removed, and the corresponding buttons in the button-box inter-
face are selected. As the user selects additional buttons, the interface dynamically expands to
reveal  additional  choices.  Because  the  interface  reﬂects  an  improving  syntactic  representa-
tion, it also improves with successive notes.

The button-box interface is a direct presentation of a ﬁnite-state machine. After the user
has written out a token or so of the note, the software ﬁnds the FSM that best parses these
tokens.  The  mode  switch  is  presented  if  the  syntax  is  sufﬁciently  mature—if  the  average
number of times each state has been used to parse earlier notes is greater than 2. If the user
selects this indicator, the FSM is incrementally rendered as a set of radio buttons and check
boxes.

The  two  user  interface  item  types  correspond  to  optional  choices  (check  boxes)  and
exclusive  choices  (radio  buttons).  Mapping  a  FSM  into  these  two  item  types  proceeds  one
state at a time. Given a particular state to be rendered, any transition that starts a path that
does not branch and eventually returns back to the state is rendered as a check box (a loop).
The loop corresponds to syntactically optional information. The label for the check box con-
sists of each of the transition labels along the looping path. Other non-looping transitions are
rendered as buttons in a single radio button panel along with an extra, unlabeled button. They
correspond to syntactically exclusive information. The label for each radio button consists of
each  transition  label  up  to  the  point  of  a  subsequent  branch  or  termination.  For  example,
compare  the  FSM  depicted  in  Figure 7  and  the  corresponding  button-box  interface  in
Figure 3.

Because the transitions for different radio buttons lead to different parts of the FSM, it
may confuse the user to render the entire FSM at once. So, each branching state is rendered
as it is visited. Initially, the ﬁrst state in the FSM is rendered. Then, when a radio button is
selected,  the  branching  state  at  the  end  of  its  transition  path  is  rendered.  Note  that  check
boxes do not trigger additional rendering because the branching state at the end of their loop

76

 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

has  already  been  rendered.  This  interactive  process  is  repeated  as  long  as  the  user  selects
radio buttons that lead to branching states.

7. Empirical Results

We  tested  the  interactive  note  taking  software  on  notes  drawn  from  a  variety  of  domains.
Tables 5 through 11 list sample notes from seven domains (in addition to the PowerBook and
fabric pattern sample notes listed above).

CVA-62 8/6/63 to 3/4/64 Mediterranean A-5A AG 60X

CVA-61 8/5/64 to 5/6/65 Vietnam RA-5C NG 10X

Table 5: Sample notes from the airwing domain. Listed above are 2 of the 78 notes
about  airwing  assignments  aboard  aircraft  carriers  collected  from  (Grove  &  Miller,
1989).

B, 81, 5, 151 (2.5), Cyl. 4, 2-bbl., Pontiac

C, 82, X, 173 (2.8), Cyl. 6, 2-bbl., Chevrolet

Table 6: Sample notes from the engine code domain. Listed above are 2 of the 20 notes
about  the  meaning  of  engine  codes  stamped  on  automobile  identification  plates
collected from Chilton’s Repair & Tune-Up Guide (1985).

90, Mazda MPV, 40K MI, 7 Pass, V6, Auto
ABS, PL/PW, Cruise, Dual Air

87, Grand Caravan, 35K MI, 7 Pass, V6, Auto
Cruise, Air, Tilt, Tinting

Table 7: Sample notes from the minivan domain. Listed above are 2 of the 22 notes
about minivan automobiles collected by the first author.

Lorus Disney Oversize Mickey Mouse Watch.
Genuine leather strap.

Seiko Disney Ladies' Minnie Mouse Watch.
Leather strap.

Table 8:  Sample  notes  from  the  watch  domain.  Listed  above  are  2  of  the  89  notes
about personal watches collected from the Best catalog (a department store).

77

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

azatadine maleate
Blood: thrombocytopenia.
CNS: disturbed coordination, dizziness, drowsiness, sedation, 
vertigo.
CV: palpitations, hypotension.
GI: anorexia, dry mouth and throat, nausea, vomiting.
GU: Urinary retention.
Skin: rash, urticaria.
Other: chills, thickening of bronchial secretions.

brompheniramine maleate
Blood: aganulocytosis, thrombocytopenia.
CNS: dizziness, insomnia, irritability, tremors.
CV: hypotension, palpitations.
GI: anorexia, dry mouth and throat, nausea, vomiting.
GU: urinary retention.
Skin: rash, urticaria.
After parenteral administration:
 local reaction, sweating, syncope may occur.

Table 9: Sample notes from the antihistamine domain. Listed above are 2 of the 17
notes on the side effects of antihistamines collected from the Nurses Guide to Drugs
(1979).

Canon FD f/1.8, 6oz., f/22, 13in.,
good sharpness, poor freedom from flare,
better freedom from distortion,
focal length marked on sides as well as
on front of lens

Chinon f/1.7, 6oz., f/22, 9in.,
poor sharpness, good freedom from flare,
good freedom from distortion,
cannot be locked in program mode, which
is only a problem, of course, when lens is
used on program-mode cameras

Table 10: Sample notes from the lens domain. Listed above are 2 of the 31 notes about
35mm SLR camera normal lenses collected from the Consumer Reports (1988).

78

 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

22in. W. 48in.
A very large falcon. Three color phases occur:
blackish, white, and gray-brown. All
are more uniformly colored than the
Peregrine Falcon, which has dark
mustaches and hood.

16-24in. W. 42in.
Long-winged, long-tailed hawk with a
white rump, usually seen soaring
unsteadily over marshes with its wings
held in a shallow 'V'. Male has a pale
gray back, head, and breast. Female
and young are brown above, streaked
below, young birds with a rusty tone.

Table 11: Sample notes from the raptor domain. Listed above are 2 of the 21 notes
about North American birds of prey collected from (Bull & Farrand, 1977).

Summary  characteristics  of  the  nine  domains  are  listed  in  Table 12  together  with  some
simple measures to indicate prediction difﬁculty. For instance, Column 1 shows the number
of notes in the domain. With a larger number of notes, the easier it should be to accurately
train a predictive method. Column 4 shows the standard deviation (STD) of the length of all
notes  in  each  domain.  It  is  more  likely  that  a  well-behaved  FSM  can  be  discovered  when
STD  is  low.  In  this  and  successive  tables,  the  domains  are  ranked  by  STD.  Column 5
presents the percentage of unique tokens in the notes. The fewer novel tokens a note has, the
more likely that successive tokens can be predicted. This measure places an upper bound on
predictive  accuracy.  Column 6  shows  the  percentage  of  constant  tokens,  ones  that  always
appear  in  a  ﬁxed  position.
It  is  easier  to  predict  these  constant  tokens.  Finally,  Column 7
indicates the percentage of repeated tokens. When fewer tokens are repeated verbatim within
a note, the more likely that the predictive method will not become confused about its locale
within a note during prediction.

The ﬁrst six domains are natural for the interactive note taking task because they exhibit
a  regular  syntax.  The  last  three  domains  are  included  to  test  the  software’s  ability  on  less
suitable domains. Notes from the Antihistamine, Lens, and Raptor domains contain highly-
variable  lists  of  terms  or  natural  language  sentences.  Learned  FSMs  for  notes  in  these
domains are unlikely to converge, and, in the experiments reported here, only the FSM for
the Lens data exceeded the maturity threshold (average state usage greater than 2).

7.1 Contextual Prediction Accuracy

Column 7  of Table 13  lists  the  accuracy  of  next-token  predictions  made  by  the  software  in
prompting  mode. The  ﬁrst  nine  rows  list  predictive  accuracy  over  all  tokens  as  notes  from
each of the nine domains are independently processed in the order they were collected. The
last row lists predictive accuracy over all tokens as notes from all nine domains are collec-
tively processed. This simulates a user taking notes about several domains simultaneously. 

To put these results in context, the table also lists predictive accuracies for several other
methods. Column 1 lists the accuracy for a lower bound method. It assumes that each note
, this method initializes its structure to the
common
shares a ﬁxed sequence of tokens. Termed 

79

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

1

2

3

4

5

6

7

Domain

Airwing
Pattern
Engine Code
Minivan
PowerBook
Watch
Antihistamine
Lens
Raptor

N Notes N Tokens Tokens/Note STD % Unique % Constant % Repeated
0
0
0
0
15
1
1
19
22

0.3
12.0
0.7
5.8
0.8
11.1
1.7
15.2
2.6
13.0
5.1
9.3
9.4
24.8
34.4
9.6
41.8 11.5

936
75
222
335
1238
832
421
1066
878

78
13
20
22
95
89
17
31
21

18
21
0
9
1
13
17
1
33

8
0
0
17
31
0
8
26
7

Table 12: Quantitative properties of the nine domains used to test alternative methods.

ﬁrst  note.  It  then  removes  each  token  in  this  sequential  structure  that  cannot  be  found  in
order in other notes. At best, this method can only predict the constant, delimiter-like tokens
that may appear regularly in notes. Its performance is limited by the percentage of constant
tokens reported in Column 6 of Table 12. It performs best for the PowerBook notes where it
learns the following note syntax:

* :NULL * ""K"" * "" PowerBook"" * ""MB"" * ""MB"" * "" Int."" * .

(Example 3)

(The asterisks indicate Kleene star notation.) This reads as some sequence of zero or more
tokens  then  the  token 
"",  followed  by  zero  or
"",  and  so  on.  It  is  less  successful  for  the  minivan  notes  where  it
more  tokens  then ""
learns a simpler syntax:

,  followed  by  zero  or  more  tokens  then  ""

:NULL
PowerBook

K

* :NULL * ""K"" * "" MI"" * "" Pass"" * .

(Example 4)

Columns 2 and 3 of Table 13 list the accuracy of using a classiﬁer to directly predict the
next  token  without  explicitly  learning  a  syntax.  In  this  paradigm,  examples  are  preﬁxes  of
token sequences. Attributes are the last token in the sequence, the second to last token, the
third to last token, and so on. Class values are the next token in the sequence—the one to be
predicted.  Column 2  lists  the  performance  of  a  simple  Bayes  classiﬁer,  and  Column 3  lists
the  performance  of  an  incremental  variant  of  ID3  (Schlimmer  &  Fisher,  1986).  Perhaps
surprisingly, these methods perform considerably worse than the simple conjunctive method.
Without the beneﬁt of a narrow context provided by the FSM, these methods must implicitly
construct representations to detect differences between similar situations that arise within a
single note. For example, in the PowerBook notes, a classiﬁer-only approach must learn to
discriminate between the ﬁrst and second occurrence of the ""

"" token.

MB

Column 4 of Table 13 lists the accuracy of a more viable prediction mechanism. Based
on simple ideas of memorization and termed 
, the method maintains a list of tokens
that  have  immediately  followed  each  observed  token.  For  example,  in  the  fabric  pattern
domain,  this  method  retains  the  list  of  tokens  {""
""}  as  those
8-10-12
11/12
  tokens  are  kept  in  order  from  most  to  least
follow
that  follow  the  token  ""
Size
frequent. To predict the next token, the system looks for the last token written and predicts

"".  Each  list  of 

digram

"",  ""

"",  ""

"",  ""

10

12

80

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

Domain
Airwing
Pattern
Engine Code
Minivan
PowerBook
Watch
Antihistamine
Lens
Raptor
Combined

6

5

3

2

1

4
Common Bayes ID4 Digram FSM FSM+Bayes
44
43
63
44
76
33
22
60
9
45

8
8
16
15
8
8
7
6
8
7
14
10
6
4
3
3
3
2
— —

19
25
18
29
40
21
11
22
9
—

62
43
64
46
70
39
24
63
12
46

47
34
59
54
73
44
40
68
11
48

7

8

FSM+ID4 Upper
79
62
68
51
87
69
80
47
96
82
78
42
68
24
91
63
55
12
—
49

Table 13:  Percentage  of  tokens  correctly  predicted  as  a  function  of  the  learning
method.

the most frequent follow token. This method is nearly as effective as any other in Table 13,
especially on the combined task when notes from each domain are entered in random order.
Laird (1992) describes an efﬁcient algorithm for maintaining higher-dimensional n-grams, in
effect increasing the context of each prediction and effectively memorizing longer sequences
of tokens. Laird’s algorithm builds a Markov tree and incorporates heuristics that keep the
size of the tree from growing excessively large. Regrettably, these methods are unsuitable for
the  interactive  note-taking  software  because  of  the  difﬁculty  of  using  them  to  construct  a
custom user interface. It is plausible to construct a panel of exclusive choices based directly
on the set of follow tokens, but it is unclear how to identify optional choices corresponding
to loops in ﬁnite-state machines. Moreover, if notes are drawn from different domains, and
those  domains  share  even  a  single  token,  then  some  follow  set  will  include  tokens  from
different  domains.  Using  these  follow  sets  to  construct  a  user  interface  will  unnecessarily
confuse the user by introducing options from more than one domain at a time.

Column 5 of Table 13 lists the accuracy of prediction based solely on the learned FSMs.
Without  an  embedded  classiﬁer,  this  method  must  rely  on  prediction  of  the  most  common
transition (or termination) from each state. Because the prediction is based on simple counts
 predicts optional
(as noted in Section 4, Learning Embedded Classiﬁers), this method 
transitions.

never

Columns 6 and 7 of Table 13 list the accuracy of predicting using FSMs and embedded
classiﬁers. The classiﬁers used are simple Bayes and the incremental ID3, respectively. The
latter outperforms either the FSM alone or the FSM with embedded Bayes classiﬁers. If the
system only makes predictions when its conﬁdence measure is greater than 0.25, the accu-
racy  is  signiﬁcantly  different  for  the  Engine  Code,  Minivan,  Lens,  and  Raptor  domains,
ranging between 10 and 22 percentage points of improvement.

Column 8 of Table 13 lists an estimate of the upper-bound on predictive accuracy. This
was calculated by assuming that prediction errors were only made the ﬁrst time each distinct
token was written.

81

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

Domain

Airwing
Pattern
Engine Code
Minivan
PowerBook
Watch
Antihistamine
Lens
Raptor

1

Norm

62
51
69
47
82
42
24
63
12

2
Diff 
Tokens
62
51
71
48
80
42
25
66
11

3
Rules 
2a,b
63
53
72
48
83
43
24
64
12

4
Rules 
2ab,3a
62
52
69
47
83
43
24
63
12

5
No 
Restart
62
50
43
28
77
28
9
46
11

6
Accept
= 1/4
62
51
69
47
82
42
24
63
12

7
Accept
= 3/4
62
51
69
47
82
42
24
63
12

8
Repeat 
Atts
62
51
69
52
81
42
24
63
12

9
Drop 
Class’r
61
51
67
45
80
41
24
63
12

10

New IDs

63
53
72
48
82
43
24
64
12

Table 14: Percentage of tokens correctly predicted as a function of design variations.

7.2 Design Decisions

The note taking software embodies a number of design decisions. Table 14 lists the effects of
these decisions on predictive accuracy by comparing versions of the software with and with-
out  each  design  feature.  The  ﬁrst  column  lists  the  predictive  accuracy  for  the  software’s
nominal  conﬁguration.  Column 2  lists  the  accuracy  data  for  a  slightly  different  generic
tokenizer. Accuracy is higher for some domains, lower for others. A custom-built tokenizer
is one way to incorporate knowledge about the domain. Columns 3 and 4 show the accuracy
for  the  system  using  only  the  original  two  FSM  merging  rules  (cf.  Table 1)  and  all  but  the
last  merging  rule  (cf.  Table 2),  respectively.  The  decreased  structural  generality  tends  to
lower  predictive  accuracy,  but  the  embedded  classiﬁers  help  compensate  for  the  reduced
accuracy.  Column 5  lists  the  accuracy  for  when  the  FSM  does  not  heuristically  continue
parsing upon encountering a token for which there is no immediate transition. As expected,
accuracy  suffers  considerably  in  some  domains  because  a  novel  token  in  a  sequence
completely  foils  any  subsequent  prediction.  Columns 6  and 7  list  accuracy  for  different
values of the free parameter controlling the clustering of notes together into a FSM. There is
little  effect  on  predictive  accuracy  in  this  case.  Column 8  shows  the  accuracy  for  when
embedded classiﬁers do not use information about repeated states in the FSM.
Without this
information, the classiﬁers cannot predict that a loop transition should be taken exactly once.
Surprisingly, elimination of this feature has little effect on accuracy. Column 9 lists the accu-
racy  for  when  the  embedded  classiﬁers  associated  with  a  pair  of  FSM  states  are  discarded
when the states are merged. Finally, Column 10 lists the accuracy for when a new FSM state
is assigned a unique ID rather than the ID of the oldest of the two merged states.

7.3

Sample Button-Box Interfaces

In addition to Figure 3, Figures 11 through 15 depict button-box interfaces for the ﬁve other
well-behaved note taking domains listed at the top of Table 12. These interfaces are visual
and  offer  the  user  an  organized  view  of  their  notes,  presenting  options  in  a  natural  way.
However,  whenever  unique  tokens  are  involved,  the  current  software  makes  no  attempt  to
explicitly generalize tokens. This effect is reﬂected in the tour dates for the Airwing notes in
Figure 11. Note that the radio button panel consists of a long series of dates, none of which is
likely to be selected for a new note.

82

 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

Figure 11:  Screen  snapshot  of  the  note-taking  software  in  button-box  mode  for  an
airwing note.

Figure 12:  Screen  snapshot  of  the  note-taking  software  in  button-box  mode  for  a
fabric pattern note.

8. Related Work

•

—Does  the  system  present  alternatives  without  the  user  having  to

Self-customizing software agents have several subjective dimensions on which they can be
evaluated and compared:
Anticipation
request them?
User interface
User control
Modality
mode without explicitly selecting one of them?
Learning update

—Is the system graphical, or is it command-line oriented?
—Can the user override or choose to ignore predictive actions?

—If the system has a number of working modes, can the user work in any

—Is learning incremental, continuous and/or real-time?

•
•
•

•

83

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

Figure 13:  Screen  snapshot  of  the  note-taking  software  in  button-box  mode  for  an
engine code note.

Figure 14:  Screen  snapshot  of  the  note-taking  software  in  button-box  mode  for  a
minivan note.

•

User adjustable

—Can the user tune the system parameters manually?

Here we describe related systems that exhibit properties in each of these agent dimensions.

anticipation

Our note taking software utilizes the 

 user interface technique pioneered by
Eager (Cypher, 1991). Eager is a non-intrusive system that learns to perform iterative proce-
dures  by  watching  the  user. As  such,  it  is  a  learning  apprentice,  a  software  agent,  and  an
example of programming by example or demonstration. Situated within the HyperCard envi-
ronment,  it  continuously  watches  a  user’s  actions.  When  it  detects  the  second  cycle  of  an
iteration, it presents an execute icon for the user’s notice. It also visually indicates the antic-
ipated  next  action  by  highlighting  the  appropriate  button,  menu  item,  or  text  selection  in
green.  As  the  user  performs  their  task,  they  can  verify  that  Eager  has  learned  the  correct

84

 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

Figure 15:  Screen  snapshot  of  the  note-taking  software  in  button-box  mode  for  a
watch note.

procedure by comparing its anticipations to their actions. When the user is conﬁdent enough,
they can click on the execution icon, and Eager will run the iterative procedure to comple-
tion.  Eager  is  highly  anticipatory,  uses  a  graphical  interface,  is  non-obtrusive,  non-modal,
and learns in real-time, but is not user adjustable.

CAP is an apprenticeship system that learns to predict default values (Dent, et al., 1992).
Its domain of operation is calendar management, and it learns preferences as a knowledgable
secretary  might.  For  example,  a  professor  may  prefer  to  hold  a  regular  group  meeting  in  a
particular  room  at  a  particular  time  of  day  for  a  particular  duration—information  that  a
secretary would know from experience. CAP collects information as the user manages their
calendar,  learns  from  previous  meetings,  and  uses  the  regularities  it  learns  to  offer  default
values for meeting location, time, and duration. The learning system is re-run each night on
the most recent meeting data, and the learned rules are applied for prediction the following
day.  CAP  is  also  designed  to  utilize  an  extensible  knowledge  base  that  contains  calendar
information  and  a  database  of  personnel  information.  The  system  continues  to  be  used  to
manage  individual  faculty  calendars. Though  offering  some  intelligence,  CAP’s  user  inter-
face  is  line-oriented  and  is  based  on  the  Emacs  editor.  Questions  asked  of  the  user  about
meetings  are  presented  using  a  command-line  dialog,  and  the  default  predictions  are
displayed  one-at-a-time.  CAP  can  be  characterized  as  anticipatory,  command-line  oriented
and modal with user control (but not user adjustable), where learning is done in batch.

Another  related  system  addresses  the  task  of  learning  to  ﬁll  out  a  form  (Hermens  &
Schlimmer, 1993). The system recreates a paper form as an on-screen facsimile, allowing the
user to view all of the pertinent information at a glance. Input typed by the user into the elec-
tronic form is processed by a central form-ﬁlling module. When the user completes a form
copy, it is printed, and each ﬁeld value on the form is forwarded to a learning module (a deci-
sion tree learning method). The learned representations predict default values for each ﬁeld
on the form by referring to values observed on other ﬁelds and on the previous form copy.
From  the  user’s  point  of  view,  it  is  as  if  spreadsheet  functions  have  been  learned  for  each
ﬁeld  of  the  form.  Empirical  studies  indicate  that  this  system  reduced  the  number  of  key-
strokes  required  of  the  user  by  87%  on  269  forms  processed  over  the  8  month  period  in

85

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

which it was actually used by ofﬁce personnel. This system is unobtrusive, non-modal and
anticipatory, uses a graphical interface, and updates learning in real-time.

Maes and Kozierok (1993) are addressing the problem of self-customizing software at a
much more task-independent level. They identify three learning opportunities for a software
agent: observing the user’s actions and imitating them, receiving user feedback upon error,
and  incorporating  explicit  training  by  the  user.  To  illustrate  the  generality  of  their  frame-
work, they demonstrate simple learning apprentices that help sort the user’s electronic mail
and schedule meetings. Their initial systems use an instance-based (case- or memory-based)
approach  primarily  because  it  allows  efﬁcient  update  and  because  it  naturally  generates  a
conﬁdence in each of its predictions. User’s may set thresholds on these predictions, corre-
sponding to a minimum conﬁdence for when the agent should prompt the user (a “tell-me”
threshold)  and  a  higher  minimum  conﬁdence  for  the  agent  to  act  immediately  on  behalf  of
the user (a “do-it” threshold). The framework for learning in this case is anticipatory, utilizes
a graphical user interface, is devoted to user control, is non-modal, learns in real-time, and is
user adjustable.

CLEAR

A  system  developed  for  Macintosh  Common  Lisp  (MCL)  provides  a  word-completion
mechanism for word preﬁxes typed by the user in any window. J. Salem and A. Ruttenberg
(unpublished) have devised MCL methods to display a word completion in the status bar of
the each window. If the user desires to add the completion to the window, they simply press
  key.  This  word  completion  mechanism  is  similar  to  ﬁle-name  completion  in
the 
  systems,  except  that  the  word  is  displayed  for  the  user
EMACS
before it is added. This system is anticipatory (unlike the 
 ﬁle completion), is command
line  oriented  (but  displays  the  default  completion  in  a  graphical  window),  can  be  fully
controlled by the user, is non-modal, learns in real time, is not intended to be user adjustable
(though knowledgeable MCL programmers could easily make changes to the code).

  and  the  C-shell  in 

UNIX

UNIX

The interactive note taking software we have devised does not require any user program-
ming. It only receives implicit user feedback when the user chooses to complete a note in a
different way than prompted. It does not have any mechanisms for direct user instruction or
threshold tuning. In a system designed to be as easy to use as paper, such explicit adjustment
may be inappropriate. We characterize our system as anticipatory, graphically-oriented, and
modal  (due  to  the  switching  that  takes  place  when  a  user  wishes  to  display  the  button-box
interface).  It  allows  the  user  to  override  default  prompts  and  predictions,  and  it  learns  in
real-time. We have not included features that allow the user to conﬁgure the performance of
the agent.

9. Observations/Limitations

The interactive note-taking software is designed to help users capture information digitally,
both  to  speed  entry  and  improve  accuracy,  and  to  support  the  longer  term  goal  of  efﬁcient
retrieval. The software incorporates two distinctive features. First, it actively predicts what
the user is going to write. Second, it automatically constructs a custom radio-button, check-
box user interface.

a priori

This  research  explores  the  extremes  of  FSM  learning  and  prediction,  where  the  system
has no explicit 
 knowledge of the note domains. We have tried to design the system
so that it can learn quickly, yet adapt well to semantic and syntactic changes, all without a
knowledge store from which to draw. It is clear that knowledge in the form of a domain-spe-
ciﬁc tokenizer would aid FSM learning by chunking signiﬁcant phrases and relating similar
notations and abbreviations. Some preliminary work has shown that, after a few notes have

86

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

been written, users may create abbreviations instead of writing out whole words. A domain-
speciﬁc tokenizer would be able to relate an abbreviation and a whole word as being in the
same  class,  and  therefore  allow  for  more  ﬂexibility  during  note  taking.  For  example,  a
domain-speciﬁc  tokenizer  may  recognize  that  ""
""  all  repre-
sent the same token for memory sizes. One could imagine a framework that would allow for
domain-speciﬁc tokenizers to be simply plugged in.

Megabytes

"",  and  ""

"",  ""

Meg

  ""
,

MB

M

""

The  prototype  built  to  demonstrate  these  ideas  was  implemented  on  a  conventional,
micro computer with keyboard input. As a consequence, it was impossible to evaluate user
acceptance  of  the  new  interface  or  the  adaptive  agent.  With  newly  available  computing
devices  incorporating  pen  input  and  handwriting  recognition,  it  should  be  possible  to  re-
engineer the user interface and ﬁeld test these ideas with actual users.

One  aspect  of  note  learning,  related  to  tokenization  and  the  button-box  user  interface
display, is the difﬁculty of generalizing numeric strings or unique tokens. The cardinality of
the range of model numbers, telephone numbers, quantities, sizes, other numeric values, and
even  proper  names  is  very  large  in  some  note  domains.  The  ﬁnite-state  machine  learning
method  presented  here  is  incapable  of  generalizing  over  transitions  from  a  particular  state,
and,  as  a  consequence,  the  current  system  has  the  problem  of  displaying  a  very  lengthy
button-box interface list. (A button is displayed for each value encountered in the syntax of
notes, and there may be many choices.) For example, a large variety of pattern numbers may
be available in the fabric pattern note domain. An appropriate mechanism is desired to deter-
mine when the list of numeric choices is too large to be useful as a button-box interface. The
system can then generalize the expected number, indicating the number of digits to prompt
,  for  example.  This  may  be  helpful  to  remind  the  user  that  a  number  is
the  user: 
expected without presenting an overbearing list of possibilities.

####

Another  limitation  of  the  current  effort  lies  in  the  choice  of  ﬁnite-state  machines  to
represent  the  syntax  of  the  user’s  notes.  Notes  may  not  be  regular  expressions  with  the
consequence that the FSMs may become too large as the learning method attempts to acquire
a syntax. This may place an unreasonable demand on memory and lead to reduced prompting
effectiveness.

The choice of ﬁnite-state machines also apparently constraints the custom user interface.
Because FSMs branch in unpredicable ways, button-box interfaces must be rendered incre-
mentally. After the user indicates a particular transition (by selecting a button), the system
can render states reachable from that transition for the user. Ideally, the user should be able
to select buttons corresponding to note fragments in any order, allowing them to write down
the size before the pattern number, for example. To construct a non-modal user interface, a
more ﬂexible syntactic representation is needed.

Several of the low-level design decisions employed in this system are crude responses to
technical issues. For instance, the decision to render a syntax as a button-box interface only
if the average number of times each state has been used to parse notes is greater than 2. This
ignores the fact that some parts of the state machine have been used frequently for parsing
notes while other parts have rarely been used. Similarly, the particular measure for estimat-
ing prompting conﬁdence (and setting the saturation of the completion button) is simplistic
and would beneﬁt from a more sound statistical basis.

Acknowledgments

Anonymous  reviewers  suggested  an  additional  example  in  Section 3,  offered  some  reﬁne-
ments  to  the  user  interface,  graciously  identiﬁed  some  limitations  of  the  work  listed  in

87

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

CHLIMMER 

& H

ERMENS

Section 9,  and  pointed  out  some  additional  related  work.  Mike  Kibler,  Karl  Hakimian,  and
the EECS staff provided a consistent and reliable computing environment. Apple Cambridge
developed  and  supports  the  Macintosh  Common  Lisp  programming  environment.  Allen
Cypher  provided  the  tokenizer  code.  This  work  was  supported  in  part  by  the  National
Science  Foundation  under  grant  number  92-1290  and  by  a  grant  from  Digital  Equipment
Corporation.

References

Angluin,  D.  (1982).  Inference  of  reversible  languages. 
29

, 
Computing Machinery

, 741–765.

Journal  of  the  Association  for

Berwick, R. C., & Pilato, S. (1987). Learning syntax by automata induction. 

Machine Learn-

ing

, 

2

, 9–38.

Bull, J., & Farrand, J., Jr. (1977). The Audubon Society Field Guide to North American Birds

(Eastern Edition). NY: Alfred A. Knopf (pp. 401–682).

Chilton’s  Repair  &  Tune-Up  Guide:  GM  X-Body  1980-1985

  (1985).  Randor,  PA:  Chilton

Book (p. 7).

Cohen, W. W. (1988). Generalizing number and learning from multiple examples in explana-
Proceedings  of  the  Fifth  International  Conference  on  Machine

tion  based  learning. 
Learning

 (pp. 256–269). Ann Arbor, MI: Morgan Kaufmann.

Consumer Reports (1988), 

53

 (12), 302–303. Mount Vernon, NY: Consumers Union.

Cypher,  A.  (1991).  Eager:  Programming  repetitive  tasks  by  example. 

Proceedings  of  CHI

(pp. 33–39). New Orleans, LA: ACM.

Dent,  L.,  Boticario,  J.,  McDermott,  J.,  Mitchell,  T.,  &  Zabowski,  D.  (1992).  A  personal
Proceedings  of  the  Tenth  National  Conference  on Artiﬁcial  Intelli-

learning  apprentice. 
gence

 (pp. 96–103). San Jose, CA: AAAI Press.

Fisher, D. H. (1987). Knowledge acquisition via incremental conceptual clustering. 

Machine

Learning

, 

2

, 139–172.

Grove, M., & Miller, J. (1989). North American Rockwell A3J/A-5 Vigilante. Arlington, TX:

Aerofax (pp. 13–15).

Hermens, L. A., & Schlimmer, J. C. (1993). A machine-learning apprentice for the comple-
Proceedings of the Ninth IEEE Conference on Artiﬁcial Intelli-

tion of repetitive forms. 
gence for Applications

. Orlando, FL.

Laird, P. (1992). Discrete sequence prediction and its applications. 

Proceedings of the Tenth
 (pp. 135–140). San Jose, CA: AAAI Press.

National Conference on Artiﬁcial Intelligence

88

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
S

OFTWARE

 A

GENTS

: C

OMPLETING

 P

ATTERNS

 & C

ONSTRUCTING

 U

SER

 I

NTERFACES

Maes,  P.,  &  Kozierok,  R.  (1993).  Learning  interface  agents. 

Proceedings  of  the  Eleventh
 (pp. 459–465). Washington, D. C.: AAAI

National Conference on Artiﬁcial Intelligence
Press.

Nurse’s Guide to Drugs (1979). Horsham, PA: Intermed Communications (pp. 454–462).

Schlimmer,  J.  C.,  &  Fisher,  D.  H.  (1986). A  case  study  of  incremental  concept  induction.
  (pp.  496–501).

Proceedings  of  the  Fifth  National  Conference  on  Artiﬁcial  Intelligence
Philadelphia, PA: AAAI Press.

89

 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
","To support the goal of allowing users to record and retrieve information,
this paper describes an interactive note-taking system for pen-based computers
with two distinctive features. First, it actively predicts what the user is
going to write. Second, it automatically constructs a custom, button-box user
interface on request. The system is an example of a learning-apprentice
software- agent. A machine learning component characterizes the syntax and
semantics of the user's information. A performance system uses this learned
information to generate completion strings and construct a user interface.
Description of Online Appendix: People like to record information. Doing this
on paper is initially efficient, but lacks flexibility. Recording information
on a computer is less efficient but more powerful. In our new note taking
softwre, the user records information directly on a computer. Behind the
interface, an agent acts for the user. To help, it provides defaults and
constructs a custom user interface. The demonstration is a QuickTime movie of
the note taking agent in action. The file is a binhexed self-extracting
archive. Macintosh utilities for binhex are available from
mac.archive.umich.edu. QuickTime is available from ftp.apple.com in the
dts/mac/sys.soft/quicktime."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1993) 109-138

Submitted 7/93; published 12/93

Decidable Reasoning in Terminological Knowledge

Representation Systems

Martin Buchheit

buchheit@dfki.uni-sb.de

German Research Center for Arti(cid:12)cial Intel ligence (DFKI)

Stuhlsatzenhausweg 3, D-66123 Saarbr(cid:127)ucken, Germany

Francesco M. Donini

donini@assi.dis.uniroma1.it

Andrea Schaerf

aschaerf@assi.dis.uniroma1.it

Dipartimento di Informatica e Sistemistica

Universit(cid:18)a di Roma \La Sapienza"", Via Salaria 113, I-00198 Roma, Italy

Abstract

Terminological knowledge representation systems (TKRSs) are tools for designing and

using knowledge bases that make use of terminological languages (or concept languages).

We analyze from a theoretical point of view a TKRS whose capabilities go beyond the

ones of presently available TKRSs. The new features studied, often required in practical

applications, can be summarized in three main points. First, we consider a highly expres-

sive terminological language, called ALCN R, including general complements of concepts,

number restrictions and role conjunction. Second, we allow to express inclusion state-

ments between general concepts, and terminological cycles as a particular case. Third, we

prove the decidability of a number of desirable TKRS-deduction services (like satis(cid:12)ability,

subsumption and instance checking) through a sound, complete and terminating calculus

for reasoning in ALCN R-knowledge bases. Our calculus extends the general technique

of constraint systems. As a byproduct of the proof, we get also the result that inclusion

statements in ALCN R can be simulated by terminological cycles, if descriptive semantics

is adopted.

1. Introduction

A general characteristic of many proposed terminological knowledge representation systems

(TKRSs) such as krypton (Brachman, Pigman Gilbert, & Levesque, 1985), nikl (Kacz-

marek, Bates, & Robins, 1986), back (Quantz & Kindermann, 1990), loom (MacGregor &

Bates, 1987), classic (Borgida, Brachman, McGuinness, & Alperin Resnick, 1989), kris

(Baader & Hollunder, 1991), k-rep (Mays, Dionne, & Weida, 1991), and others (see Rich,

editor, 1991; Woods & Schmolze, 1992), is that they are made up of two di(cid:11)erent compo-

nents. Informally speaking, the (cid:12)rst is a general schema concerning the classes of individuals

to be represented, their general properties and mutual relationships, while the second is a

(partial) instantiation of this schema, containing assertions relating either individuals to

classes, or individuals to each other. This characteristic, which the mentioned proposals

inherit from the seminal TKRS kl-one (Brachman & Schmolze, 1985), is shared also by

several proposals of database models such as Abrial's (1974), candide (Beck, Gala, &

Navathe, 1989), and taxis (Mylopoulos, Bernstein, & Wong, 1980).

Retrieving information in actual knowledge bases (KBs) built up using one of these sys-

tems is a deductive process involving both the schema (TBox) and its instantiation (ABox).

(cid:13)1993 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Buchheit, Donini, & Schaerf

In fact, the TBox is not just a set of constraints on possible ABoxes, but contains intensional

information about classes. This information is taken into account when answering queries

to the KB.

During the realization and use of a KB, a TKRS should provide a mechanical solution

for at least the following problems (from this point on, we use the word concept to refer to

a class):

1. KB-satis(cid:12)ability : are an ABox and a TBox consistent with each other? That is, does

the KB admit a model? A positive answer is useful in the validation phase, while the

negative answer can be used to make inferences in refutation-style. The latter will be

precisely the approach taken in this paper.

2. Concept Satis(cid:12)ability : given a KB and a concept C , does there exist at least one

model of the KB assigning a non-empty extension to C ? This is important not only

to rule out meaningless concepts in the KB design phase, but also in processing the

user's queries, to eliminate parts of a query which cannot contribute to the answer.

3. Subsumption : given a KB and two concepts C and D, is C more general than D in

any model of the KB? Subsumption detects implicit dependencies among the concepts

in the KB.

4. Instance Checking : given a KB, an individual a and a concept C , is a an instance

of C in any model of the KB? Note that retrieving all individuals described by a

given concept (a query in the database lexicon) can be formulated as a set of parallel

instance checkings.

The above questions can be precisely characterized once the TKRS is given a semantics

(see next section), which de(cid:12)nes models of the KB and gives a meaning to expressions

in the KB. Once the problems are formalized, one can start both a theoretical analysis

of them, and|maybe independently|a search for reasoning procedures accomplishing the

tasks. Completeness and correctness of procedures can be judged with respect to the formal

statements of the problems.

Up to now, all the proposed systems give incomplete procedures for solving the above

problems 1{4, except for kris

. That is, some inferences are missed, in some cases without

1

a precise semantical characterization of which ones are. If the designer or the user needs

(more) complete reasoning, she/he must either write programs in a suitable programming

language (as in the database proposal of Abrial, and in taxis), or de(cid:12)ne appropriate in-

ference rules completing the inference capabilities of the system (as in back, loom, and

classic). From the theoretical point of view, for several systems (e.g., loom) it is not even

known if complete procedures can ever exist|i.e., the decidability of the corresponding

problems is not known.

Recent research on the computational complexity of subsumption had an in(cid:13)uence in

many TKRSs on the choice for incomplete procedures. Brachman and Levesque (1984)

1. Also the system classic is complete, but only w.r.t. a non-standard semantics for the treatment of

individuals. Complete reasoning w.r.t. standard semantics for individuals is not provided, and is coNP-

hard (Lenzerini & Schaerf, 1991).

110

Decidable Reasoning in Terminological KR Systems

started this research analyzing the complexity of subsumption between pure concept ex-

pressions, abstracting from KBs (we call this problem later in the paper as pure subsump-

tion). The motivation for focusing on such a small problem was that pure subsumption is

a fundamental inference in any TKRS. It turned out that pure subsumption is tractable

(i.e., worst-case polynomial-time solvable) for simple languages, and intractable for slight

extensions of such languages, as subsequent research de(cid:12)nitely con(cid:12)rmed (Nebel, 1988;

Donini, Lenzerini, Nardi, & Nutt, 1991a, 1991b; Schmidt-Schau(cid:25) & Smolka, 1991; Donini,

Hollunder, Lenzerini, Marchetti Spaccamela, Nardi, & Nutt, 1992). Also, beyond compu-

tational complexity, pure subsumption was proved undecidable in the TKRSs U (Schild,

1988), kl-one (Schmidt-Schau(cid:25), 1989) and nikl (Patel-Schneider, 1989).

Note that extending the language results in enhancing its expressiveness, therefore the

result of that research could be summarized as: The more a TKRS language is expressive,

the higher is the computational complexity of reasoning in that language|as Levesque

(1984) (cid:12)rst noted. This result has been interpreted in two di(cid:11)erent ways, leading to two

di(cid:11)erent TKRSs design philosophies:

1. `General-purpose languages for TKRSs are intractable, or even undecidable, and

tractable languages are not expressive enough to be of practical interest'. Follow-

ing this interpretation, in several TKRSs (such as nikl, loom and back) incomplete

procedures for pure subsumption are considered satisfactory (e.g., see (MacGregor &

Brill, 1992) for loom). Once completeness is abandoned for this basic subproblem,

completeness of overall reasoning procedures is not an issue anymore; but other issues

arise, such as how to compare incomplete procedures (Heinsohn, Kudenko, Nebel,

& Pro(cid:12)tlich, 1992), and how to judge a procedure \complete enough"" (MacGregor,

1991). As a practical tool, inference rules can be used in such systems to achieve the

expected behavior of the KB w.r.t. the information contained in it.

2. `A TKRS is (by de(cid:12)nition) general-purpose, hence it must provide tractable and

complete reasoning to a user'. Following this line, other TKRSs (such as krypton

and classic) provide limited tractable languages for expressing concepts, following

the \small-can-be-beautiful"" approach (see Patel-Schneider, 1984). The gap between

what is expressible in the TKRS language and what is needed to be expressed for the

application is then (cid:12)lled by the user, by a (sort of ) programming with inference rules.

Of course, the usual problems present in program development and debugging arise

(McGuinness, 1992).

What is common to both approaches is that a user must cope with incomplete reasoning.

The di(cid:11)erence is that in the former approach, the burden of regaining useful yet missed

inferences is mostly left to the developers of the TKRS (and the user is supposed to specify

what is \complete enough""), while in the latter this is mainly left to the user. These

are perfectly reasonable approaches in a practical context, where incomplete procedures

and specialized programs are often used to deal with intractable problems. In our opinion

incomplete procedures are just a provisional answer to the problem|the best possible up to

now. In order to improve on such an answer, a theoretical analysis of the general problems

1{4 is to be done.

Previous theoretical results do not deal with the problems 1{4 in their full generality.

For example, the problems are studied in (Nebel, 1990, Chapter 4), but only incomplete

111

Buchheit, Donini, & Schaerf

procedures are given, and cycles are not considered. In (Donini, Lenzerini, Nardi, & Schaerf,

1993; Schaerf, 1993a) the complexity of instance checking has been analyzed, but only KBs

without a TBox are treated. Instance checking has also been analyzed in (Vilain, 1991),

but addressing only that part of the problem which can be performed as parsing.

In addition, we think that the expressiveness of actual systems should be enhanced

making terminological cycles (see Nebel, 1990, Chapter 5) available in TKRSs. Such a

feature is of undoubtable practical interest (MacGregor, 1992), yet most present TKRSs

can only approximate cycles, by using forward inference rules (as in back, classic, loom).

In our opinion, in order to make terminological cycles fully available in complete TKRSs, a

theoretical investigation is still needed.

Previous theoretical work on cycles was done in (Baader, 1990a, 1990b; Baader, B(cid:127)urkert,

Hollunder, Nutt, & Siekmann, 1990; Dionne, Mays, & Oles, 1992, 1993; Nebel, 1990, 1991;

Schild, 1991), but considering KBs formed by the TBox alone. Moreover, these approaches

do not deal with number restrictions (except for Nebel, 1990, Section 5.3.5) |a basic feature

already provided by TKRSs| and the techniques used do not seem easily extensible to

reasoning with ABoxes. We compare in detail several of these works with ours in Section 4.

In this paper, we propose a TKRS equipped with a highly expressive language, includ-

ing constructors often required in practical applications, and prove decidability of problems

1{4. In particular, our system uses the language ALCN R, which supports general comple-

ments of concepts, number restrictions and role conjunction. Moreover, the system allows

one to express inclusion statements between general concepts and, as a particular case,

terminological cycles. We prove decidability by means of a suitable calculus, which is de-

veloped extending the well established framework of constraint systems (see Donini et al.,

1991a; Schmidt-Schau(cid:25) & Smolka, 1991), thus exploiting a uniform approach to reasoning

in TKRSs. Moreover, our calculus can easily be turned into a decision procedure.

The paper is organized as follows.

In Section 2 we introduce the language, and we

give it a Tarski-style extensional semantics, which is the most commonly used. Using this

semantics, we establish relationships between problems 1{4 which allow us to concentrate

on KB-satis(cid:12)ability only. In Section 3 we provide a calculus for KB-satis(cid:12)ability, and show

correctness and termination of the calculus. Hence, we conclude that KB-satis(cid:12)ability is

decidable in ALCN R, which is the main result of this paper. In Section 4 we compare our

approach with previous results on decidable TKRSs, and we establish the equivalence of

general (cyclic) inclusion statements and general concept de(cid:12)nitions using the descriptive

semantics. Finally, we discuss in detail several practical issues related to our results in

Section 5.

2. Preliminaries

In this section we (cid:12)rst present the basic notions regarding concept languages. Then we

describe knowledge bases built up using concept languages, and reasoning services that

must be provided for extracting information from such knowledge bases.

2.1 Concept Languages

In concept languages, concepts represent the classes of ob jects in the domain of interest,

while roles represent binary relations between ob jects. Complex concepts and roles can be

112

Decidable Reasoning in Terminological KR Systems

de(cid:12)ned by means of suitable constructors applied to concept names and role names.

In

particular, concepts and roles in ALCN R can be formed by means of the following syntax

(where P

(for i = 1; : : : ; k) denotes a role name, C and D denote arbitrary concepts, and

i

R an arbitrary role):

C; D (cid:0)! A j

(concept name)

> j

(top concept)

? j

(bottom concept)

(C u D) j

(conjunction)

(C t D) j

(disjunction)

:C j

(complement)

8R.C j

(universal quanti(cid:12)cation)

9R.C j

(existential quanti(cid:12)cation)

((cid:21) n R) j ((cid:20) n R)

(number restrictions)

R (cid:0)! P

u (cid:1) (cid:1) (cid:1) u P

(role conjunction)

1

k

When no confusion arises we drop the brackets around conjunctions and disjunctions.

We interpret concepts as subsets of a domain and roles as binary relations over a domain.

More precisely, an interpretation I = ((cid:1)

; (cid:1)

) consists of a nonempty set (cid:1)

(the domain

I

I

I

of I ) and a function (cid:1)

(the extension function of I ), which maps every concept to a subset

I

I

I

I

of (cid:1)

and every role to a subset of (cid:1)

(cid:2) (cid:1)

. The interpretation of concept names and

role names is thus restricted by A

(cid:18) (cid:1)

, and P

(cid:18) (cid:1)

(cid:2) (cid:1)

, respectively. Moreover,

I

I

I

I

I

the interpretation of complex concepts and roles must satisfy the following equations (]fg

denotes the cardinality of a set):

I

I

>

= (cid:1)

I

?

= ;

(C u D)

= C

\ D

I

I

I

I

I

I

(C t D)

= C

[ D

(1)

I

I

I

(:C )

= (cid:1)

n C

(8R.C )

= fd

2 (cid:1)

j 8d

: (d

; d

) 2 R

! d

2 C

g

1

2

1

2

2

I

I

I

I

(9R.C )

= fd

2 (cid:1)

j 9d

: (d

; d

) 2 R

^ d

2 C

g

1

2

1

2

2

I

I

I

I

((cid:21) n R)

= fd

2 (cid:1)

j

]fd

j (d

; d

) 2 R

g (cid:21) ng

1

2

1

2

I

I

I

((cid:20) n R)

= fd

2 (cid:1)

j

]fd

j (d

; d

) 2 R

g (cid:20) ng

1

2

1

2

I

I

I

(P

u (cid:1) (cid:1) (cid:1) u P

)

= P

\ (cid:1) (cid:1) (cid:1) \ P

1

k

1

k

I

I

I

2.2 Knowledge Bases

A knowledge base built by means of concept languages is generally formed by two compo-

nents: The intensional one, called TBox, and the extensional one, called ABox.

We (cid:12)rst turn our attention to the TBox. As we said before, the intensional level spec-

i(cid:12)es the properties of the concepts of interest in a particular application. Syntactically,

such properties are expressed in terms of what we call inclusion statements. An inclusion

113

Buchheit, Donini, & Schaerf

statement (or simply inclusion) has the form

C v D

where C and D are two arbitrary ALCN R-concepts. Intuitively, the statement speci(cid:12)es

that every instance of C is also an instance of D. More precisely, an interpretation I satis(cid:12)es

the inclusion C v D if C

(cid:18) D

.

I

I

A TBox is a (cid:12)nite set of inclusions. An interpretation I is a model for a TBox T if I

satis(cid:12)es all inclusions in T .

In general, TKRSs provide the user with mechanisms for stating concept introductions

(e.g., Nebel, 1990, Section 3.2) of the form A

= D (concept de(cid:12)nition, interpreted as set

:

equality), or A

(cid:20) D (concept speci(cid:12)cation, interpreted as set inclusion), with the restrictions

_

that the left-hand side concept A must be a concept name, that for each concept name

at most one introduction is allowed, and that no terminological cycles are allowed, i.e.,

no concept name may occur|neither directly nor indirectly|within its own introduction.

These restrictions make it possible to substitute an occurrence of a de(cid:12)ned concept by its

de(cid:12)nition.

We do not impose any of these restrictions to the form of inclusions, obtaining statements

that are syntactically more expressive than concept introductions. In particular, a de(cid:12)nition

of the form A

= D can be expressed in our system using the pair of inclusions A v D

:

and D v A and a speci(cid:12)cation of the form A

(cid:20) D can be simply expressed by A v D.

_

Conversely, an inclusion of the form C v D, where C and D are arbitrary concepts, cannot

be expressed with concept introductions. Moreover, cyclic inclusions are allowed in our

statements, realizing terminological cycles.

As shown in (Nebel, 1991), there are at least three types of semantics for terminolog-

ical cycles, namely the least (cid:12)xpoint, the greatest (cid:12)xpoint, and the descriptive semantics.

Fixpoint semantics choose particular models among the set of interpretations that satisfy a

statement of the form A

= D. Such models are chosen as the least and the greatest (cid:12)xpoint

:

of the above equation. The descriptive semantics instead considers all interpretations that

satisfy the statement (i.e., all (cid:12)xpoints) as its models.

However, (cid:12)xpoint semantics naturally apply only to (cid:12)xpoint statements like A

= D,

:

where D is a \function"" of A, i.e., A may appear in D, and there is no obvious way to

extend them to general inclusions. In addition, since our language includes the constructor

for complement of general concepts, the \function"" D may be not monotone, and therefore

the least and the greatest (cid:12)xpoints may be not unique. Whether there exists or not a

de(cid:12)nitional semantics that is suitable for cyclic de(cid:12)nitions in expressive languages is still

unclear.

Conversely, the descriptive semantics interprets statements as just restricting the set of

possible models, with no de(cid:12)nitional import. Although it is not completely satisfactory in all

practical cases (Baader, 1990b; Nebel, 1991), the descriptive semantics has been considered

to be the most appropriate one for general cyclic statements in powerful concept languages.

Hence, it seems to be the most suitable to be extended to our case and it is exactly the one

we have adopted above.

Observe that our decision to put general inclusions in the TBox is not a standard one. In

fact, in TKRS like krypton such statements were put in the ABox. However, we conceive

114

Decidable Reasoning in Terminological KR Systems

inclusions as a generalization of traditional TBox statements: acyclic concept introductions,

with their de(cid:12)nitional import, can be perfectly expressed with inclusions; and cyclic concept

introductions can be expressed as well, if descriptive semantics is adopted. Therefore, we

believe that inclusions should be part of the TBox.

Notice that role conjunction allows one to express the practical feature of subroles. For

example, the role ADOPTEDCHILD can be written as CHILD u ADOPTEDCHILD

, where ADOPTED-

0

CHILD' is a role name, making it a subrole of CHILD. Following such idea, every hierarchy

of role names can be rephrased with a set of role conjunctions, and vice versa.

Actual systems usually provide for the construction of hierarchies of roles by means of

role introductions (i.e., statements of the form P

= R and P

(cid:20) R) in the TBox. However,

:

_

in our simple language for roles, cyclic de(cid:12)nitions of roles can be always reduced to acyclic

de(cid:12)nitions, as explained in (Nebel, 1990, Sec.5.3.1). When role de(cid:12)nitions are acyclic, one

can always substitute in every concept each role name with its de(cid:12)nition, obtaining an

equivalent concept. Therefore, we do not consider role de(cid:12)nitions in this paper, and we

conceive the TBox just as a set of concept inclusions.

Even so, it is worth to notice that concept inclusions can express knowledge about roles.

In particular, domain and range restrictions of roles can be expressed, in a way similar to

the one in (Catarci & Lenzerini, 1993). Restricting the domain of a role R to a concept C

and its range to a concept D can be done by the two inclusions

9R.> v C; > v 8R.D

It is straightforward to show that if an interpretation I satis(cid:12)es the two inclusions, then

I

I

I

R

(cid:18) C

(cid:2) D

.

Combining subroles with domain and range restrictions it is also possible to partially

express the constructor for role restriction, which is present in various proposals (e.g.,

the language F L in Brachman & Levesque, 1984). Role restriction, written as R : C , is

de(cid:12)ned by (R : C )

= f(d

; d

) 2 (cid:1)

(cid:2) (cid:1)

j (d

; d

) 2 R

^ d

2 C

g. For example the

1

2

1

2

2

I

I

I

I

I

role DAUGHTER, which can be formulated as CHILD:Female, can be partially simulated by

CHILD u DAUGHTER

, with the inclusion > v 8DAUGHTER

.Female. However, this simulation

0

0

would not be complete in number restrictions: E.g., if a mother has at least three daughters,

then we know she has at least three female children; if instead we know that she has three

female children we cannot infer that she has three daughters.

We can now turn our attention to the extensional level, i.e., the ABox. The ABox

essentially allows one to specify instance-of relations between individuals and concepts, and

between pairs of individuals and roles.

Let O be an alphabet of symbols, called individuals. Instance-of relationships are ex-

pressed in terms of membership assertions of the form:

C (a);

R(a; b);

where a and b are individuals, C is an ALCN R-concept, and R is an ALCN R-role. Intu-

itively, the (cid:12)rst form states that a is an instance of C , whereas the second form states that

a is related to b by means of the role R.

115

Buchheit, Donini, & Schaerf

In order to assign a meaning to membership assertions, the extension function (cid:1)

of an

I

interpretation I is extended to individuals by mapping them to elements of (cid:1)

in such a

I

way that a

6= b

if a 6= b. This property is called Unique Name Assumption; it ensures

I

I

that di(cid:11)erent individuals are interpreted as di(cid:11)erent ob jects.

An interpretation I satis(cid:12)es the assertion C (a) if a

2 C

, and satis(cid:12)es R(a; b) if

I

I

I

I

I

(a

; b

) 2 R

. An ABox is a (cid:12)nite set of membership assertions. I is a model for an ABox

A if I satis(cid:12)es all the assertions in A.

An ALCN R-know ledge base (cid:6) is a pair (cid:6) = hT ; Ai where T is a TBox and A is an

ABox. An interpretation I is a model for (cid:6) if it is both a model for T and a model for A.

We can now formally de(cid:12)ne the problems 1{4 mentioned in the introduction. Let (cid:6) be

an ALCN R-knowledge base.

1. KB-satis(cid:12)ability : (cid:6) is satis(cid:12)able, if it has a model;

2. Concept Satis(cid:12)ability : C is satis(cid:12)able w.r.t (cid:6), if there exists a model I of (cid:6) such that

I

C

6= ;;

3. Subsumption : C is subsumed by D w.r.t. (cid:6), if C

(cid:18) D

for every model I of (cid:6);

I

I

4. Instance Checking : a is an instance of C , written (cid:6) j= C (a), if the assertion C (a) is

satis(cid:12)ed in every model of (cid:6).

In (Nebel, 1990, Sec.3.3.2) it is shown that the ABox plays no active role when checking

concept satis(cid:12)ability and subsumption. In particular, Nebel shows that the ABox (sub ject

to its satis(cid:12)ability) can be replaced by an empty one without a(cid:11)ecting the result of those

services. Actually, in (Nebel, 1990), the above property is stated for a language less expres-

sive than ALCN R. However, it is easy to show that it extends to ALCN R. It is important

to remark that such a property is not valid for all concept languages. In fact, there are

languages that include some constructors that refer to the individuals in the concept lan-

guage, e.g., the constructor one-of (Borgida et al., 1989) that forms a concept from a set of

enumerated individuals. If a concept language includes such a constructor the individuals

in the TBox can interact with the individuals in the ABox, as shown in (Schaerf, 1993b).

As a consequence, both concept satis(cid:12)ability and subsumption depend also on the ABox.

Example 2.1 Consider the following knowledge base (cid:6) = hT ; Ai:

T = f9TEACHES.Course v (Student u 9DEGREE.BS) t Prof;

Prof v 9DEGREE.MS;

9DEGREE.MS v 9DEGREE.BS;

MS u BS v ?g

A = fTEACHES(john; cs156); ((cid:20) 1 DEGREE)(john); Course(cs156)g

(cid:6) is a fragment of a hypothetical knowledge base describing the organization of a university.

The (cid:12)rst inclusion, for instance, states that the persons teaching a course are either graduate

students (students with a BS degree) or professors. It is easy to see that (cid:6) is satis(cid:12)able. For

example, the following interpretation I satis(cid:12)es all the inclusions in T and all the assertions

116

Decidable Reasoning in Terminological KR Systems

in A, and therefore it is a model for (cid:6):

I

I

I

(cid:1)

= fjohn; cs156; csbg; john

= john; cs156

= cs156

Student

= fjohng; Prof

= ;; Course

= fcs156g; BS

= fcsbg

I

I

I

I

I

I

I

MS

= ;; TEACHES

= f(john; cs156)g; DEGREE

= f(john; csb)g

We have described the interpretation I by giving only (cid:1)

, and the values of I on

I

concept names and role names. It is straightforward to see that all values of I on complex

concepts and roles are uniquely determined by imposing that I must satisfy the Equations 1

on page 113.

Notice that it is possible to draw several non-trivial conclusions from (cid:6). For example, we

can infer that (cid:6) j= Student(john). Intuitively this can be shown as follows: John teaches

a course, thus he is either a student with a BS or a professor. But he can't be a professor

since professors have at least two degrees (BS and MS) and he has at most one, therefore

he is a student.

Given the previous semantics, the problems 1{4 can all be reduced to KB-satis(cid:12)ability

(or to its complement) in linear time. In fact, given a knowledge base (cid:6) = hT ; Ai, two

concepts C and D, an individual a, and an individual b not appearing in (cid:6), the following

equivalences hold:

C is satis(cid:12)able w:r:t (cid:6) i(cid:11) hT ; A [ fC (b)gi is satis(cid:12)able:

C is subsumed by D w:r:t: (cid:6) i(cid:11) hT ; A [ f(C u :D)(b)gi is not satis(cid:12)able:

(cid:6) j= C (a)

i(cid:11) hT ; A [ f(:C )(a)gi is not satis(cid:12)able:

A slightly di(cid:11)erent form of these equivalences has been given in (Hollunder, 1990). The

equivalences given here are a straightforward consequence of the ones given by Hollunder.

However, the above equivalences are not valid for languages including constructors that refer

to the individuals in the concept language. The equivalences between reasoning services in

such languages are studied in (Schaerf, 1993b).

Based on the above equivalences,

in the next section we concentrate just on KB-

satis(cid:12)ability.

3. Decidability Result

In this section we provide a calculus for deciding KB-satis(cid:12)ability. In particular, in Subsec-

tion 3.1 we present the calculus and we state its correctness. Then, in Subsection 3.2, we

prove the termination of the calculus. This will be su(cid:14)cient to assess the decidability of all

problems 1{4, thanks to the relationships between the four problems.

3.1 The calculus and its correctness

Our method makes use of the notion of constraint system (Donini et al., 1991a; Schmidt-

Schau(cid:25) & Smolka, 1991; Donini, Lenzerini, Nardi, & Schaerf, 1991c), and is based on a

tableaux-like calculus (Fitting, 1990) that tries to build a model for the logical formula

corresponding to a KB.

117

Buchheit, Donini, & Schaerf

We introduce an alphabet of variable symbols V together with a well-founded total

ordering `(cid:30)' on V . The alphabet V is disjoint from the other ones de(cid:12)ned so far. The

purpose of the ordering will become clear later. The elements of V are denoted by the

letters x; y ; z ; w. From this point on, we use the term object as an abstraction for individual

and variable (i.e., an ob ject is an element of O [ V ). Ob jects are denoted by the symbols

s; t and, as in Section 2, individuals are denoted by a; b.

A constraint is a syntactic entity of one of the forms:

s: C;

sP t;

8x.x: C ;

s 6

= t;

:

where C is a concept and P is a role name. Concepts are assumed to be simple, i.e., the

only complements they contain are of the form :A, where A is a concept name. Arbitrary

ALCN R-concepts can be rewritten into equivalent simple concepts in linear time (Donini

et al., 1991a). A constraint system is a (cid:12)nite nonempty set of constraints.

Given an interpretation I , we de(cid:12)ne an I -assignment (cid:11) as a function that maps every

variable of V to an element of (cid:1)

, and every individual a to a

(i.e., (cid:11)(a) = a

for all

I

I

I

a 2 O).

A pair (I ; (cid:11)) satis(cid:12)es the constraint s: C if (cid:11)(s) 2 C

, the constraint sP t if ((cid:11)(s); (cid:11)(t))

I

I

I

I

:

2 P

, the constraint s 6

= t if (cid:11)(s) 6= (cid:11)(t), and (cid:12)nally, the constraint 8x.x: C if C

= (cid:1)

(notice that (cid:11) does not play any role in this case). A constraint system S is satis(cid:12)able if

there is a pair (I ; (cid:11)) that satis(cid:12)es every constraint in S .

An ALCN R-knowledge base (cid:6) = hT ; Ai can be translated into a constraint system

S

by replacing every inclusion C v D 2 T with the constraint 8x.x: :C t D, every

(cid:6)

membership assertion C (a) with the constraint a: C , every R(a; b) with the constraints

aP

b; : : : ; aP

b if R = P

u : : : u P

, and including the constraint a 6

= b for every pair (a; b)

1

k

1

k

:

of individuals appearing in A.

It is easy to see that (cid:6) is satis(cid:12)able if and only if S

is

(cid:6)

satis(cid:12)able.

In order to check a constraint system S for satis(cid:12)ability, our technique adds constraints

to S until either an evident contradiction is generated or an interpretation satisfying it can

be obtained from the resulting system. Constraints are added on the basis of a suitable set

of so-called propagation rules.

Before providing the rules, we need some additional de(cid:12)nitions. Let S be a constraint

system and R = P

u : : : u P

(k (cid:21) 1) be a role. We say that t is an R-successor of s in S

1

k

if sP

t; : : : ; sP

t are in S . We say that t is a direct successor of s in S if for some role R,

1

k

t is an R-successor of s. We call direct predecessor the inverse relation of direct successor.

If S is clear from the context we omit it. Moreover, we denote by successor the transitive

closure of the relation direct successor, and we denote by predecessor its inverse.

We assume that variables are introduced in a constraint system according to the ordering

`(cid:30)'. This means, if y is introduced in a constraint system S then x (cid:30) y for all variables x

that are already in S .

We denote by S [x=s] the constraint system obtained from S by replacing each occurrence

of the variable x by the ob ject s.

We say that s and t are separated in S if the constraint s 6

= t is in S .

:

Given a constraint system S and an ob ject s, we de(cid:12)ne the function (cid:27) ((cid:1); (cid:1)) as follows:

(cid:27) (S; s) := fC j s: C 2 S g. Moreover, we say that two variables x and y are S -equivalent,

118

Decidable Reasoning in Terminological KR Systems

written x (cid:17)

y , if (cid:27) (S; x) = (cid:27) (S; y ). Intuitively, two S-equivalent variables can represent the

s

same element in the potential interpretation built by the rules, unless they are separated.

The propagation rules are:

1. S !

fs: C

; s: C

g [ S

u

1

2

if 1. s: C

u C

is in S ,

1

2

2. s: C

and s: C

are not both in S

1

2

2. S !

fs: Dg [ S

t

if 1. s: C

t C

is in S ,

1

2

2. neither s: C

nor s: C

is in S ,

1

2

3. D = C

or D = C

1

2

3. S !

ft: C g [ S

8

if 1. s: 8R.C is in S ,

2. t is an R-successor of s,

3. t: C is not in S

4. S !

fsP

y ; : : : ; sP

y ; y : C g [ S

9

1

k

if 1. s: 9R.C is in S ,

2. R = P

u : : : u P

,

1

k

3. y is a new variable,

4. there is no t such that t is an R-successor of s in S and t: C is in S ,

5. if s is a variable there is no variable w such that w (cid:30) s and s (cid:17)

w

s

5. S !

fsP

y

; : : : ; sP

y

j i 2 1::ng [ fy

= y

j i; j 2 1::n; i 6= j g [ S

(cid:21)

1

i

k

i

i

j

:

if 1. s: ((cid:21) n R) is in S ,

2. R = P

u : : : u P

,

1

k

3. y

; : : : ; y

are new variables,

1

n

4. there do not exist n pairwise separated R-successors of s in S ,

5. if s is a variable there is no variable w such that w (cid:30) s and s (cid:17)

w

s

6. S !

S [y=t]

(cid:20)

if 1. s: ((cid:20) n R) is in S ,

2. s has more than n R-successors in S ,

3. y ; t are two R-successors of s which are not separated

7. S !

fs: C g [ S

8x

if 1. 8x.x: C is in S ,

2. s appears in S ,

3. s: C is not in S .

We call the rules !

and !

nondeterministic rules, since they can be applied in

t

(cid:20)

di(cid:11)erent ways to the same constraint system (intuitively, they correspond to branching

rules of tableaux). All the other rules are called deterministic rules. Moreover, we call the

rules !

and !

generating rules, since they introduce new variables in the constraint

9

(cid:21)

system. All other rules are called nongenerating ones.

119

6
Buchheit, Donini, & Schaerf

The use of the condition based on the S -equivalence relation in the generating rules

(condition 5) is related to the goal of keeping the constraint system (cid:12)nite even in presence

of potentially in(cid:12)nite chains of applications of generating rules. Its role will become clearer

later in the paper.

One can verify that rules are always applied to a system S either because of the presence

in S of a given constraint s: C (condition 1), or, in the case of the !

-rule, because of the

8x

presence of an ob ject s in S . When no confusion arises, we will say that a rule is applied

to the constraint s: C or the ob ject s (instead of saying that it is applied to the constraint

system S ).

Proposition 3.1 (Invariance) Let S and S

be constraint systems. Then:

0

1. If S

is obtained from S by application of a deterministic rule, then S is satis(cid:12)able if

0

and only if S

is satis(cid:12)able.

0

2. If S

is obtained from S by application of a nondeterministic rule, then S is satis(cid:12)-

0

able if S

is satis(cid:12)able. Conversely, if S is satis(cid:12)able and a nondeterministic rule is

0

applicable to an object s in S , then it can be applied to s in such a way that it yields

a satis(cid:12)able constraint system.

Proof. The proof is mainly a rephrasing of typical soundness proofs for tableaux meth-

ods (e.g., Fitting, 1990, Lemma 6.3.2). The only non-standard constructors are number

restrictions.

1. \("" Considering the deterministic rules one can directly check that S is a subset of S

.

0

So it is obvious that S is satis(cid:12)able if S

is satis(cid:12)able.

0

\)"" In order to show that S

is satis(cid:12)able if this is the case for S we consider in turn

0

each possible deterministic rule application leading from S to S

. We assume that (I ; (cid:11))

0

satis(cid:12)es S .

If the !

-rule is applied to s: C

u C

in S , then S

= S [ fs: C

; s: C

g. Since (I ; (cid:11))

u

1

2

1

2

0

satis(cid:12)es s: C

u C

, (I ; (cid:11)) satis(cid:12)es s: C

and s: C

and therefore S

.

1

2

1

2

0

If the !

-rule is applied to s: 8R.C , there must be an R-successor t of s in S such that

8

0

I

S

= S [ ft: C g. Since (I ; (cid:11)) satis(cid:12)es S , it holds that ((cid:11)(s); (cid:11)(t)) 2 R

. Since (I ; (cid:11)) satis(cid:12)es

s: 8R.C , it holds that (cid:11)(t) 2 C

. So (I ; (cid:11)) satis(cid:12)es t: C and therefore S

.

I

0

If the !

-rule is applied to an s because of the presence of 8x.x: C in S , then S

=

8x

0

S [ fs: C g. Since (I ; (cid:11)) satis(cid:12)es S it holds that C

= (cid:1)

. Therefore (cid:11)(s) 2 C

and so

I

I

I

(I ; (cid:11)) satis(cid:12)es S

.

0

If the !

-rule is applied to s: 9R.C , then S

= S [ fsP

y ; : : : ; sP

y ; y : C g. Since (I ; (cid:11))

9

1

k

0

satis(cid:12)es S , there exists a d such that ((cid:11)(s); d) 2 R

and d 2 C

. We de(cid:12)ne the I -assignment

I

I

0

0

0

0

0

(cid:11)

as (cid:11)

(y ) := d and (cid:11)

(t) := (cid:11)(t) for t 6= y . It is easy to show that (I ; (cid:11)

) satis(cid:12)es S

.

If the !

-rule is applied to s: ((cid:21) n R), then S

= S [ fsP

y

; : : : ; sP

y

j i 2 1::ng [

(cid:21)

1

i

k

i

0

:

fy

= y

j i; j 2 1::n; i 6= j g. Since (I ; (cid:11)) satis(cid:12)es S , there exist n distinct elements

i

j

d

; : : : ; d

2 (cid:1)

such that ((cid:11)(s); d

) 2 R

. We de(cid:12)ne the I -assignment (cid:11)

as (cid:11)

(y

) := d

1

n

i

i

i

I

I

0

0

for i 2 1::n and (cid:11)

(t) := (cid:11)(t) for t 62 fy

; : : : ; y

g. It is easy to show that (I ; (cid:11)

) satis(cid:12)es S

.

1

n

0

0

0

2. \("" Assume that S

is satis(cid:12)ed by (I ; (cid:11)

). We show that S is also satis(cid:12)able. If S

0

0

0

is obtained from S by application of the !

-rule, then S is a subset of S

and therefore

0

satis(cid:12)ed by (I ; (cid:11)

).

0

t

120

6
Decidable Reasoning in Terminological KR Systems

0

If S

is obtained from S by application of the !

-rule to s: ((cid:20) n R) in S , then there

(cid:20)

are y ; t in S such that S

= S [y=t]. We de(cid:12)ne the I -assignment (cid:11) as (cid:11)(y ) := (cid:11)

(t) and

0

0

(cid:11)(v ) := (cid:11)

(v ) for every ob ject v with v 6= y . Obviously (I ; (cid:11)) satis(cid:12)es S .

0

\)"" Now suppose that S is satis(cid:12)ed by (I ; (cid:11)) and a nondeterministic rule is applicable

to an ob ject s.

If the !

-rule is applicable to s: C

t C

then, since S is satis(cid:12)able, (cid:11)(s) 2 (C

t C

)

.

t

1

2

1

2

I

It follows that either (cid:11)(s) 2 C

or (cid:11)(s) 2 C

(or both). Hence, the !

-rule can obviously

1

2

t

I

I

be applied in a way such that (I ; (cid:11)) satis(cid:12)es the resulting constraint system S

.

0

If the !

-rule is applicable to s: ((cid:20) n R), then|since (I ; (cid:11)) satis(cid:12)es S|it holds that

(cid:20)

(cid:11)(s) 2 ((cid:20) n R)

and therefore the set fd 2 (cid:1)

j ((cid:11)(s); d) 2 R

g has at most n elements.

I

I

I

On the other hand, there are more than n R-successors of s in S and for each R-successor t

of s we have ((cid:11)(s); (cid:11)(t)) 2 R

. Thus, we can conclude by the Pigeonhole Principle (see e.g.,

I

Lewis & Papadimitriou, 1981, page 26) that there exist at least two R-successors t; t

of s

0

such that (cid:11)(t) = (cid:11)(t

). Since (I ; (cid:11)) satis(cid:12)es S , the constraint t 6

= t

is not in S . Therefore

0

:

0

one of the two must be a variable, let's say t

= y . Now obviously (I ; (cid:11)) satis(cid:12)es S [y=t].

0

Given a constraint system S , more than one rule might be applicable to it. We de(cid:12)ne

the following strategy for the application of rules:

1. apply a rule to a variable only if no rule is applicable to individuals;

2. apply a rule to a variable x only if no rule is applicable to a variable y such that y (cid:30) x;

3. apply generating rules only if no nongenerating rule is applicable.

The above strategy ensures that the variables are processed one at a time according to

the ordering `(cid:30)'.

From this point on, we assume that rules are always applied according to this strategy

and that we always start with a constraint system S

coming from an ALCN R-knowledge

(cid:6)

base (cid:6). The following lemma is a direct consequence of these assumptions.

Lemma 3.2 (Stability) Let S be a constraint system and x be a variable in S . Let a

generating rule be applicable to x according to the strategy. Let S

be any constraint system

0

derivable from S by any sequence (possibly empty) of applications of rules. Then

1. No rule is applicable in S

to a variable y with y (cid:30) x

0

2. (cid:27) (S; x) = (cid:27) (S

; x)

0

3. If y is a variable in S with y (cid:30) x then y is a variable in S

, i.e., the variable y is not

0

substituted by another variable or by a constant.

Proof. 1. By contradiction: Suppose S (cid:17) S

!

S

!

(cid:1) (cid:1) (cid:1) !

S

(cid:17) S

, where (cid:3) 2

0

(cid:3)

1

(cid:3)

(cid:3)

n

0

ft; u; 9; 8; (cid:21); (cid:20); 8xg and a rule is applicable to a variable y such that y (cid:30) x in S

. Then

0

there exists a minimal i, where i (cid:20) n, such that this is the case in S

. Note that i 6= 0; in

i

fact, because of the strategy, if a rule is applicable to x in S no rule is applicable to y in S .

So no rule is applicable to any variable z such that z (cid:30) x in S

; : : : ; S

. It follows that

0

i(cid:0)1

from S

to S

a rule is applied to x or to a variable w such that x (cid:30) w. By an exhaustive

i(cid:0)1

i

121

Buchheit, Donini, & Schaerf

analysis of all rules we see that|whichever is the rule applied from S

to S

|no new

i(cid:0)1

i

constraint of the form y : C or yRz can be added to S

, and therefore no rule is applicable

i(cid:0)1

to y in S

, contradicting the assumption.

i

0

2. By contradiction: Suppose (cid:27) (S; x) 6= (cid:27) (S

; x). Call y the direct predecessor of x, then a

rule must have been applied either to y or to x itself. Obviously we have y (cid:30) x, therefore

the former case cannot be because of point 1. A case analysis shows that the only rules

which can have been applied to x are generating ones and the !

and the !

rules. But

8

(cid:20)

these rules add new constraints only to the direct successors of x and not to x itself and

therefore do not change (cid:27) ((cid:1); x).

3. This follows from point 1. and the strategy.

Lemma 3.2 proves that for a variable x which has a direct successor, (cid:27) ((cid:1); x) is stable,

i.e., it will not change because of subsequent applications of rules.

In fact, if a variable

has a direct successor it means that a generating rule has been applied to it, therefore

(Lemma 3.2.2) from that point on (cid:27) ((cid:1); x) does not change.

A constraint system is complete if no propagation rule applies to it. A complete system

derived from a system S is also called a completion of S . A clash is a constraint system

having one of the following forms:

(cid:15) fs: ?g

(cid:15) fs: A; s: :Ag, where A is a concept name.

(cid:15) fs: ((cid:20) n R)g [ fsP

t

; : : : ; sP

t

j i 2 1::n + 1g

1

i

k

i

:

[ ft

= t

j i; j 2 1::n + 1; i 6= j g,

i

j

where R = P

u : : : u P

.

1

k

A clash is evidently an unsatis(cid:12)able constraint system. For example, the last case

represents the situation in which an ob ject has an at-most restriction and a set of R-

successors that cannot be identi(cid:12)ed (either because they are individuals or because they

have been created by some at-least restrictions).

Any constraint system containing a clash is obviously unsatis(cid:12)able. The purpose of the

calculus is to generate completions, and look for the presence of clashes inside. If S is a

completion of S

and S contains no clash, we prove that it is always possible to construct

(cid:6)

a model for (cid:6) on the basis of S . Before looking at the technical details of the proof, let us

consider an example of application of the calculus for checking satis(cid:12)ability.

Example 3.3 Consider the following knowledge base (cid:6) = hT ; Ai:

T = fItalian v 9FRIEND.Italiang

A = fFRIEND(peter; susan);

8FRIEND.:Italian(peter);

9FRIEND.Italian(susan)g

The corresponding constraint system S

is:

(cid:6)

S

= f8x.x: :Italian t 9FRIEND.Italian;

(cid:6)

peterFRIENDsusan;

122

6
Decidable Reasoning in Terminological KR Systems

peter: 8FRIEND.:Italian;

susan: 9FRIEND.Italian

:

peter 6

= susang

A sequence of applications of the propagation rules to S

is as follows:

(cid:6)

S

= S

[ fsusan: :Italiang (!

-rule)

1

(cid:6)

8

S

= S

[ fpeter: :Italian t 9FRIEND.Italiang (!

-rule)

2

1

8x

S

= S

[ fsusan: :Italian t 9FRIEND.Italiang (!

-rule)

3

2

8x

S

= S

[ fpeter: :Italiang (!

-rule)

4

3

t

S

= S

[ fsusanFRIENDx; x: Italiang (!

-rule)

5

4

9

S

= S

[ fx: :Italian t 9FRIEND.Italiang (!

-rule)

6

5

8x

S

= S

[ fx: 9FRIEND.Italiang (!

-rule)

7

6

t

S

= S

[ fxFRIENDy; y: Italiang (!

-rule)

8

7

9

S

= S

[ fy: :Italian t 9FRIEND.Italiang (!

-rule)

9

8

8x

S

= S

[ fy: 9FRIEND.Italiang (!

-rule)

10

9

t

One can verify that S

is a complete clash-free constraint system. In particular, the !

-

10

9

rule is not applicable to y . In fact, since x (cid:17)

y condition 5 is not satis(cid:12)ed. From S

one

S

10

10

can build an interpretation I , as follows (again, we give only the interpretation of concept

and role names):

I

(cid:1)

= fpeter; susan; x; yg

peter

= peter, susan

= susan, (cid:11)(x) = x, (cid:11)(y) = y,

I

I

Italian

= fx; yg

I

I

FRIEND

= f(peter; susan); (susan; x); (x; y); (y; y)g

It is easy to see that I is indeed a model for (cid:6).

In order to prove that it is always possible to obtain an interpretation from a complete

clash-free constraint system we need some additional notions. Let S be a constraint system

and x, w variables in S . We call w a witness of x in S if the three following conditions hold:

1. x (cid:17)

w

s

2. w (cid:30) x

3. there is no variable z such that z (cid:30) w and z satis(cid:12)es conditions 1. and 2., i.e., w is

the least variable w.r.t. (cid:30) satisfying conditions 1. and 2.

We say x is blocked (by w) in S if x has a witness (w) in S . The following lemma states a

property of witnesses.

Lemma 3.4 Let S be a constraint system, x a variable in S . If x is blocked then

1. x has no direct successor and

2. x has exactly one witness.

123

Buchheit, Donini, & Schaerf

Proof. 1. By contradiction: Suppose that x is blocked in S and xP y is in S . During the

completion process leading to S a generating rule must have been applied to x in a system

0

0

S

. It follows from the de(cid:12)nition of the rules that in S

for every variable w (cid:30) x we had

w. Now from Lemma 3.2 we know, that for the constraint system S derivable from

x6(cid:17)

0

s

0

S

and for every w (cid:30) x in S we also have x6(cid:17)

w. Hence there is no witness for x in S ,

s

contradicting the hypothesis that x is blocked.

2. This follows directly from condition 3. for a witness.

As a consequence of Lemma 3.4, in a constraint system S , if w

is a witness of x then w

1

1

cannot have a witness itself, since both the relations `(cid:30)' and S -equivalence are transitive.

The uniqueness of the witness for a blocked variable is important for de(cid:12)ning the following

particular interpretation out of S .

Let S be a constraint system. We de(cid:12)ne the canonical interpretation I

and the canon-

S

ical I

-assignment (cid:11)

as follows:

S

S

1. (cid:1)

:= fs j s is an ob ject in S g

I

S

2. (cid:11)

(s) := s

S

3. s 2 A

if and only if s: A is in S

I

S

4. (s; t) 2 P

if and only if

I

S

(a) sP t is in S

or

(b) s is a blocked variable, w is the witness of s in S and wP t is in S .

We call (s; t) a P-role-pair of s in I

if (s; t) 2 P

, we call (s; t) a role-pair of s in I

S

S

I

S

if (s; t) is a P-role-pair for some role P . We call a role-pair explicit if it comes up from case

4.(a) of the de(cid:12)nition of the canonical interpretation and we call it implicit if it comes up

from case 4.(b).

From Lemma 3.4 it is obvious that a role-pair cannot be both explicit and implicit.

Moreover, if a variable has an implicit role-pair then all its role-pairs are implicit and they

all come from exactly one witness, as stated by the following lemma.

Lemma 3.5 Let S be a completion and x a variable in S . Let I

be the canonical inter-

S

pretation for S . If x has an implicit role-pair (x; y ), then

1. al l role-pairs of x in I

are implicit

S

2. there is exactly one witness w of x in S such that for al l roles P in S and al l P -role-

pairs (x,y) of x, the constraint wP y is in S .

Proof. The (cid:12)rst statement follows from Lemma 3.4 (point 1 ). The second statement follows

from Lemma 3.4 (point 2 ) together with the de(cid:12)nition of I

.

S

We have now all the machinery needed to prove the main theorem of this subsection.

Theorem 3.6 Let S be a complete constraint system. If S contains no clash then it is

satis(cid:12)able.

124

Decidable Reasoning in Terminological KR Systems

Proof. Let I

and (cid:11)

be the canonical interpretation and canonical I -assignment for S .

S

S

We prove that the pair (I

; (cid:11)

) satis(cid:12)es every constraint c in S . If c has the form sP t or

S

S

:

s 6

= t, then (I

; (cid:11)

) satis(cid:12)es them by de(cid:12)nition of I

and (cid:11)

. Considering the !

-rule and

S

S

S

S

(cid:21)

:

the !

-rule we see that a constraint of the form s 6

= s can not be in S . If c has the form

(cid:20)

s: C , we show by induction on the structure of C that s 2 C

.

I

S

We (cid:12)rst consider the base cases. If C is a concept name, then s 2 C

by de(cid:12)nition

I

S

of I

. If C = >, then obviously s 2 >

. The case that C = ? cannot occur since S is

I

S

S

clash-free.

Next we analyze in turn each possible complex concept C . If C is of the form :C

then

1

C

is a concept name since all concepts are simple. Then the constraint s: C

is not in S

1

I

S

I

S

I

S

1

I

S

since S is clash-free. Then s 62 C

, that is, s 2 (cid:1)

n C

. Hence s 2 (:C

)

.

1

1

1

If C is of the form C

u C

then (since S is complete) s: C

is in S and s: C

is in S . By

1

2

1

2

induction hypothesis, s 2 C

and s 2 C

. Hence s 2 (C

u C

)

.

1

2

1

2

I

S

I

S

I

S

If C is of the form C

t C

then (since S is complete) either s: C

is in S or s: C

is in

1

2

1

2

S . By induction hypothesis, either s 2 C

or s 2 C

. Hence s 2 (C

t C

)

.

1

2

1

2

I

I

S

S

I

S

If C is of the form 8R.D, we have to show that for all t with (s; t) 2 R

it holds that

I

S

t 2 D

. If (s; t) 2 R

, then according to Lemma 3.5 two cases can occur. Either t is an

I

S

I

S

R-successor of s in S or s is blocked by a witness w in S and t is an R-successor of w in S .

In the (cid:12)rst case t: D must also be in S since S is complete. Then by induction hypothesis

we have t 2 D

. In the second case by de(cid:12)nition of witness, w: 8R.D is in S and then

I

S

because of completeness of S , t: D must be in S . By induction hypothesis we have again

t 2 D

.

I

S

If C is of the form 9R.D we have to show that there exists a t 2 (cid:1)

with (s; t) 2 R

I

S

I

S

and t 2 D

. Since S is complete, either there is a t that is an R-successor of s in S and

I

S

t: D is in S , or s is a variable blocked by a witness w in S . In the (cid:12)rst case, by induction

hypothesis and the de(cid:12)nition of I

, we have t 2 D

and (s; t) 2 R

. In the second case

S

I

S

I

S

w: 9R.D is in S . Since w cannot be blocked and S is complete, we have that there is a

t that is an R-successor of w in S and t: D is in S . So by induction hypothesis we have

t 2 D

and by the de(cid:12)nition of I

we have (s; t) 2 R

.

S

I

S

I

S

If C is of the form ((cid:20) n R) we show the goal by contradiction. Assume that s 62 ((cid:20)

I

S

I

S

n R)

. Then there exist atleast n + 1 distinct ob jects t

; : : : ; t

with (s; t

) 2 R

; i 2

1

n+1

i

1::n + 1. This means that, since R = P

u : : : u P

, there are pairs (s; t

) 2 P

, where

1

k

i

j

I

S

i 2 1::n + 1 and j 2 1::k. Then according to Lemma 3.5 one of the two following cases must

occur. Either all sP

t

for j 2 1::k; i 2 1::n + 1 are in S or there exists a witness w of s in

j

i

S with all wP

t

for j 2 1::k and i 2 1::n + 1 are in S . In the (cid:12)rst case the !

-rule can not

i

i

(cid:20)

be applicable because of completeness. This means that all the t

's are pairwise separated,

i

:

i.e., that S contains the constraints t

= t

; i; j 2 1::n + 1; i 6= j . This contradicts the fact

i

j

that S is clash-free. And the second case leads to an analogous contradiction.

If C is of the form ((cid:21) n R) we show the goal by contradiction. Assume that s 62 ((cid:21)

I

S

n R)

. Then there exist atmost m < n (m possibly 0) distinct ob jects t

; : : : ; t

with

1

m

(s; t

) 2 R

; i 2 1::m. We have to consider two cases. First case: s is not blocked in

i

I

S

S . Since there are only m R-successors of s in S , the !

-rule is applicable to s. This

(cid:21)

contradicts the fact that S is complete. Second case: s is blocked by a witness w in S .

Since there are m R-successors of w in S , the !

-rule is applicable to w. But this leads to

(cid:21)

the same contradiction.

125

6
Buchheit, Donini, & Schaerf

If c has the form 8x.x: D then, since S is complete, for each ob ject t in S , t: D is in

S|and, by the previous cases, t 2 D

. Therefore, the pair (I

; (cid:11)

) satis(cid:12)es 8x.x: D.

S

S

I

S

Finally, since (I

; (cid:11)

) satis(cid:12)es all constraints in S , (I

; (cid:11)

) satis(cid:12)es S .

S

S

S

S

Theorem 3.7 (Correctness) A constraint system S is satis(cid:12)able if and only if there exists

at least one clash-free completion of S .

Proof. \("" Follows immediately from Theorem 3.6. \)"" Clearly, a system containing

a clash is unsatis(cid:12)able. If every completion of S is unsatis(cid:12)able, then from Proposition 3.1

S , is unsatis(cid:12)able.

3.2 Termination and complexity of the calculus

Given a constraint system S , we call n

the number of concepts appearing in S , including

S

also all the concepts appearing as a substring of another concept. Notice that n

is bounded

S

by the length of the string expressing S .

Lemma 3.8 Let S be a constraint system and let S

be derived from S by means of the

0

propagation rules. In any set of variables in S

including more than 2

variables there are

0

n

S

at least two variables x,y such that x (cid:17)

y .

0

s

Proof. Each constraint x: C 2 S

may contain only concepts of the constraint system S .

0

Since there are n

such concepts, given a variable x there cannot be more than 2

di(cid:11)erent

n

S

sets of constraints x: C in S

.

S

0

Lemma 3.9 Let S be a constraint system and let S

be any constraint system derived from

0

S by applying the propagation rules with the given strategy. Then, in S

there are at most

0

n

S

2

non-blocked variables.

Proof. Suppose there are 2

+ 1 non-blocked variables. From Lemma 3.8, we know that

n

S

0

in S

there are at least two variables y

, y

such that y

(cid:17)

y

. Obviously either y

(cid:30) y

or

1

2

1

s

2

1

2

y

(cid:30) y

holds; suppose that y

(cid:30) y

. From the de(cid:12)nitions of witness and blocked either y

2

1

1

2

1

is a witness of y

or there exists a variable y

such that y

(cid:30) y

and y

is a witness of y

.

2

3

3

1

3

2

In both cases y

is blocked, contradicting the hypothesis.

2

Theorem 3.10 (Termination and space complexity) Let (cid:6) be an ALCN R-know ledge

base and let n be its size. Every completion of S

is (cid:12)nite and its size is O(2

).

(cid:6)

4n

Proof. Let S be a completion of S

. From Lemma 3.9 it follows that there are at most 2

(cid:6)

n

non-blocked variables in S . Therefore there are at most m (cid:2) 2

total variables in S , where

n

m is the maximum number of direct successors for a variable in S .

Observe that m is bounded by the number of 9R.C concepts (at most n) plus the sum of

all numbers appearing in number restrictions. Since these numbers are expressed in binary,

their sum is bounded by 2

. Hence, m (cid:20) 2

+ n. Since the number of individuals is also

n

n

bounded by n, the total number of ob jects in S is at most m (cid:2) (2

+ n) (cid:20) (2

+ n) (cid:2) (2

+ n),

n

n

n

that is, O(2

).

2n

126

Decidable Reasoning in Terminological KR Systems

The number of di(cid:11)erent constraints of the form s: C , 8x.x: C in which each ob ject s can

be involved is bounded by n, and each constraint has size linear in n. Hence, the total size

of these constraints is bounded by n (cid:2) n (cid:2) 2

, that is O(2

).

2n

3n

The number of constraints of the form sP t, s 6

= t is bounded by (2

)

= 2

, and each

:

2n

2

4n

constraint has constant size.

In conclusion, we have that the size of S is O(2

).

4n

Notice that the above one is just a coarse upper bound, obtained for theoretical purposes.

In practical cases we expect the actual size to be much smaller than that. For example,

if the numbers involved in number restrictions were either expressed in unary notation, or

limited by a constant (the latter being a reasonable restriction in practical systems) then

an argumentation analogous to the above one would lead to a bound of 2

.

3n

Theorem 3.11 (Decidability) Given an ALCN R-know ledge base (cid:6), checking whether (cid:6)

is satis(cid:12)able is a decidable problem.

Proof. This follows from Theorems 3.7 and 3.10 and the fact that (cid:6) is satis(cid:12)able if and

only if S

is satis(cid:12)able.

(cid:6)

We can re(cid:12)ne the above theorem, by giving tighter bounds on the time required to

decide satis(cid:12)ability.

Theorem 3.12 (Time complexity) Given an ALCN R-know ledge base (cid:6), checking

whether (cid:6) is satis(cid:12)able can be done in nondeterministic exponential time.

Proof. In order to prove the claim it is su(cid:14)cient to show that each completion is obtained

with an exponential number of applications of rules. Since the number of constraints of

each completion is exponential (Theorem 3.10) and each rule, but the !

-rule, adds new

(cid:20)

constraints to the constraint system, it follows that all such rules are applied at most an

exponential number of times. Regarding the !

-rule, it is applied for each ob ject at most as

(cid:20)

many times as the number of its direct successors. Since such number is at most exponential

(if numbers are coded in binary) w.r.t. the size of the knowledge base, the claim follows.

A lower bound of the complexity of KB-satis(cid:12)ability is obtained exploiting previous

results about the language ALC , which is a sublanguage of ALCN R that does not include

number restrictions and role conjunction. We know from McAllester (1991), and (indepen-

dently) from an observation by Nutt (1992) that KB-satis(cid:12)ability in ALC -knowledge bases

is EXPTIME-hard (see (Garey & Johnson, 1979, page 183) for a de(cid:12)nition) and hence it

is hard for ALCN R-knowledge bases, too. Hence, we do not expect to (cid:12)nd any algorithm

solving the problem in polynomial space, unless PSPACE=EXPTIME. Therefore, we do

not expect to substantially improve space complexity of our calculus, which already works

in exponential space. We now discuss possible improvements on time complexity.

The proposed calculus works in nondeterministic exponential time, and hence improves

the one we proposed in (Buchheit, Donini, & Schaerf, 1993, Sec.4), which works in deter-

ministic double exponential time. The key improvement is that we showed that a KB has

a model if and only if it has a model of exponential size. However, it may be argued that

as it is, the calculus cannot yet be turned into a practical procedure, since such a proce-

dure would simply simulate nondeterminism by a second level of exponentiality, resulting

127

Buchheit, Donini, & Schaerf

in a double exponential time procedure. However, the di(cid:11)erent combinations of concepts

are only exponentially many (this is just the cardinality of the powerset of the set of con-

cepts). Hence, a double exponential time procedure wastes most of the time re-analyzing

over and over ob jects with di(cid:11)erent names yet with the same (cid:27) ((cid:1); (cid:1)), in di(cid:11)erent constraint

systems. This could be avoided if we allow a variable to be blocked by a witness that is

in a previously analyzed constraint system. This technique would be similar to the one

used in (Pratt, 1978), and to the tree-automata technique used in (Vardi & Wolper, 1986),

improving on simple tableaux methods for variants of propositional dynamic logics. Since

our calculus considers only one constraint system at a time, a modi(cid:12)cation of the calculus

would be necessary to accomplish this task in a formal way, which is outside the scope of

this paper. The formal development of such a deterministic exponential time procedure will

be a sub ject for future research.

Notice that, since the domain of the canonical interpretation (cid:1)

is always (cid:12)nite, we

I

S

have also implicitly proved that ALCN R-knowledge bases have the (cid:12)nite model property,

i.e., any satis(cid:12)able knowledge base has a (cid:12)nite model. This property has been extensively

studied in modal logics (Hughes & Cresswell, 1984) and dynamic logics (Harel, 1984). In

particular, a technique, called (cid:12)ltration, has been developed both to prove the (cid:12)nite model

property and to build a (cid:12)nite model for a satis(cid:12)able formula. This technique allows one to

build a (cid:12)nite model from an in(cid:12)nite one by grouping the worlds of a structure in equivalence

classes, based on the set of formulae that are satis(cid:12)ed in each world. It is interesting to

observe that our calculus, based on witnesses, can be considered as a variant of the (cid:12)ltration

technique where the equivalence classes are determined on the basis of our S -equivalence

relation. However, because of number restrictions, variables that are S -equivalent cannot

be grouped, since they might be separated (e.g., they might have been introduced by the

same application of the !

-rule). Nevertheless, they can have the same direct successors,

(cid:21)

as stated in point 4.(b) of the de(cid:12)nition of canonical interpretation on page 124. This would

correspond to grouping variables of an in(cid:12)nite model in such a way that separations are

preserved.

4. Relation to previous work

In this section we discuss the relation of our paper to previous work about reasoning with in-

clusions. In particular, we (cid:12)rst consider previously proposed reasoning techniques that deal

with inclusions and terminological cycles, then we discuss the relation between inclusions

and terminological cycles.

4.1 Reasoning Techniques

As mentioned in the introduction, previous results were obtained by Baader et al. (1990),

Baader (1990a, 1990b), Nebel (1990, 1991), Schild (1991) and Dionne et al. (1992, 1993).

Nebel (1990, Chapter 5) considers the language T F , containing concept conjunction,

universal quanti(cid:12)cation and number restrictions, and TBoxes containing (possibly cyclic)

concept de(cid:12)nitions, role de(cid:12)nitions and disjointness axioms (stating that two concept names

are disjoint). Nebel shows that subsumption of T F -concepts w.r.t. a TBox is decidable.

However, the argument he uses is non-constructive: He shows that it is su(cid:14)cient to con-

128

Decidable Reasoning in Terminological KR Systems

sider (cid:12)nite interpretations of a size bounded by the size of the TBox in order to decide

subsumption.

In (Baader, 1990b) the e(cid:11)ect of the three types of semantics|descriptive, greatest (cid:12)x-

point and least (cid:12)xpoint semantics|for the language F L

, containing concept conjunction

0

and universal quanti(cid:12)cation, is described with the help of (cid:12)nite automata. Baader reduces

subsumption of F L

-concepts w.r.t. a TBox containing (possibly cyclic) de(cid:12)nitions of the

:

0

form A

= C (which he calls terminological axioms) to decision problems for (cid:12)nite automata.

In particular, he shows that subsumption w.r.t. descriptive semantics can be decided in poly-

nomial space using B(cid:127)uchi automata. Using results from (Baader, 1990b), in (Nebel, 1991)

a characterization of the above subsumption problem w.r.t. descriptive semantics is given

with the help of deterministic automata (whereas B(cid:127)uchi automata are nondeterministic).

This also yields a PSPACE-algorithm for deciding subsumption.

In (Baader et al., 1990) the attention is restricted to the language ALC . In particular,

that paper considers the problem of checking the satis(cid:12)ability of a single equation of the

form C = >, where C is an ALC -concept. This problem, called the universal satis(cid:12)abil-

ity problem, is shown to be equivalent to checking the satis(cid:12)ability of an ALC -TBox (see

Proposition 4.1).

In (Baader, 1990a), an extension of ALC , called ALC

, is introduced, which supports

reg

a constructor to express the transitive closure of roles. By means of transitive closure of

roles it is possible to replace cyclic inclusions of the form A v D with equivalent acyclic

ones. The problem of checking the satis(cid:12)ability of an ALC

-concept is solved in that

reg

paper. It is also shown that using transitive closure it is possible to reduce satis(cid:12)ability

of an ALC -concept w.r.t. an ALC -TBox T = fC

v D

; : : : ; C

v D

g into the concept

1

1

n

n

satis(cid:12)ability problem in ALC

(w.r.t. the empty TBox). Since the problem of concept

reg

satis(cid:12)ability w.r.t. a TBox is trivially harder than checking the satis(cid:12)ability of a TBox,

that paper extends the result given in (Baader et al., 1990).

The technique exploited in (Baader et al., 1990) and (Baader, 1990a) is based on the

notion of concept tree. A concept tree is generated starting from a concept C in order

to check its satis(cid:12)ability (or universal satis(cid:12)ability). The way a concept tree is generated

from a concept C is similar in (cid:13)avor to the way a complete constraint system is generated

from the constraint system fx: C g. However, the extension of the concept tree method to

deal with number restrictions and individuals in the knowledge base is neither obvious, nor

suggested in the cited papers; on the other hand, the extension of the calculus based on

constraint systems is immediate, provided that additional features have a counterpart in

First Order Logic.

In (Schild, 1991) some results more general than those in (Baader, 1990a) are obtained

by considering languages more expressive than ALC

and dealing with the concept satis(cid:12)a-

reg

bility problem in such languages. The results are obtained by establishing a correspondence

between concept languages and Propositional Dynamic Logics (PDL), and reducing the

given problem to a satis(cid:12)ability problem in PDL. Such an approach allows Schild to (cid:12)nd

several new results exploiting known results in the PDL framework. However, it cannot be

used to deal with every concept language. In fact, the correspondence cannot be established

when the language includes some concept constructors having no counterpart in PDL (e.g.,

number restrictions, or individuals in an ABox).

129

Buchheit, Donini, & Schaerf

Recently, an algebraic approach to cycles has been proposed in (Dionne et al., 1992), in

which (possibly cyclic) de(cid:12)nitions are interpreted as determining an equivalence relation over

the terms describing concepts. The existence and uniqueness of such an equivalence relation

derives from Aczel's results on non-well founded sets. In (Dionne et al., 1993) the same

researchers prove that subsumption based on this approach is equivalent to subsumption in

greatest (cid:12)xpoint semantics. The language analyzed is a small fragment of the one used in the

TKRS k-rep, and contains conjunction and existential-universal quanti(cid:12)cations combined

into one construct (hence it is similar to F L

). The di(cid:14)culty of extending these results

0

lies in the fact that it is not clear how individuals can be interpreted in this algebraic

setting. Moreover, we believe that constructive approaches like the algebraic one, give

counterintuitive results when applied to non-constructive features of concept languages|as

negation and number restrictions.

In conclusion, all these approaches, i.e., reduction to automata problems, concept trees,

reduction to PDL and algebraic semantics, deal only with TBoxes and they don't seem to be

suitable to deal also with ABoxes. On the other hand, the constraint system technique, even

though it was conceived for TBox-reasoning, can be easily extended to ABox-reasoning, as

also shown in (Hollunder, 1990; Baader & Hollunder, 1991; Donini et al., 1993).

4.2 Inclusions versus Concept De(cid:12)nitions

Now we compare the expressive power of TBoxes de(cid:12)ned as a set of inclusions (as done in

this paper) and TBoxes de(cid:12)ned as a set of (possibly cyclic) concept introductions of the

form A

(cid:20) D and A

= D.

_

:

Unlike (Baader, 1990a) and (Schild, 1991), we consider reasoning problems dealing with

TBox and ABox together. Moreover, we use the descriptive semantics for the concept intro-

ductions, as we do for inclusions. The result we have obtained is that inclusion statements

and concept introductions actually have the same expressive power. In detail, we show that

the satis(cid:12)ability of a knowledge base (cid:6) = hA; T i, where T is a set of inclusion statements,

can be reduced to the satis(cid:12)ability of a knowledge base (cid:6)

= hA

; T

i such that T

is a set

0

0

0

0

of concept introductions. The other direction, from concept introductions to inclusions, is

trivial since introductions of the form A

= D can be expressed by the pair of inclusions

:

A v D and D v A, while a concept name speci(cid:12)cation A

(cid:20) D can be rewritten as the

_

inclusion A v D (as already mentioned in Section 2).

As a notation, given a TBox T = fC

v D

; : : : ; C

v D

g, we de(cid:12)ne the concept C

1

1

n

n

T

as C

= (:C

t D

) u (cid:1) (cid:1) (cid:1) u (:C

t D

). As pointed out in (Baader, 1990a) for ALC , an

T

1

1

n

n

interpretation satis(cid:12)es a TBox T if and only if it satis(cid:12)es the equation C

= >. This result

T

easily extends to ALCN R, as stated in the following proposition.

Proposition 4.1 Given an ALCN R-TBox T = fC

v D

; : : : ; C

v D

g, an interpreta-

1

1

n

n

tion I satis(cid:12)es T if and only if it satis(cid:12)es the equation C

= >.

T

Proof. An interpretation I satis(cid:12)es an inclusion C v D if and only if it satis(cid:12)es the equation

:C t D = >; I satis(cid:12)es the set of equations :C

t D

= >,: : : , :C

t D

= > if and only

1

1

n

n

if I satis(cid:12)es (:C

t D

) u (cid:1) (cid:1) (cid:1) u (:C

t D

) = >. The claim follows.

1

1

n

n

130

Decidable Reasoning in Terminological KR Systems

Given a knowledge base (cid:6) = hA; T i and a concept A not appearing in (cid:6), we de(cid:12)ne the

knowledge base (cid:6)

= hA

; T

i as follows:

0

0

0

0

A

= A [ fA(b) j b is an individual in (cid:6)g

0

_

T

= fA

(cid:20) C

u 8P

.A u (cid:1) (cid:1) (cid:1) u 8P

.Ag

T

1

n

where P

; P

; : : : ; P

are all the role names appearing in (cid:6). Note that T

has a single

1

2

n

0

inclusion, which could be also thought of as one primitive concept speci(cid:12)cation.

Theorem 4.2 (cid:6) = hA; T i is satis(cid:12)able if and only if (cid:6)

= hA

; T

i is satis(cid:12)able.

0

0

0

Proof.

In order to simplify the machinery of the proof, we will use for T

the following

0

(logically equivalent) form:

0

T

= fA v C

; A v 8P

.A; : : : ; A v 8P

.Ag

T

1

n

(Note that we use the symbol `v' instead of `

(cid:20)' because now the concept name A appears

_

as the left-hand side of many statements, we must consider these statements as inclusions).

\)"" Suppose (cid:6) = hA; T i satis(cid:12)able. From Theorem 3.7, there exists a complete

constraint system S without clash, which de(cid:12)nes a canonical interpretation I

which is a

S

model of (cid:6). De(cid:12)ne the constraint system S

as follows:

0

0

S

= S [ fw: A j w is an ob ject in Sg

and call I

the canonical interpretation associated to S

. We prove that I

is a model of

0

0

S

S

0

0

(cid:6)

.

First observe that every assertion in A is satis(cid:12)ed by I

since I

is equal to I

except

0

0

S

S

S

for the interpretation of A, and A does not appear in A. Therefore, every assertion in A

0

is also satis(cid:12)ed by I

0

S

, either because it is an assertion of A, or (if it is an assertion of the

form A(b)) by de(cid:12)nition of S

.

0

Regarding T

, note that by de(cid:12)nition of S

, we have A

= (cid:1)

= (cid:1)

; therefore both

0

0

I

0

I

0

S

S

I

S

sides of the inclusions of the form A v 8P

.A (i = 1; : : : ; n) are interpreted as (cid:1)

, hence

i

I

0

S

they are satis(cid:12)ed by I

. Since A does not appear in C

, we have that (C

)

= (C

)

.

0

S

T

T

T

I

0

S

I

S

Moreover, since I

satis(cid:12)es T , we also have, by Proposition 4.1, that (C

)

= (cid:1)

,

S

T

I

I

S

S

therefore (C

)

= (C

)

= (cid:1)

= (cid:1)

. It follows that also both sides of the inclusion

T

T

I

0

S

I

I

S

S

I

0

S

A v C

are interpreted as (cid:1)

. In conclusion, I

satis(cid:12)es T

.

0

T

S

I

0

S

0

\("" Suppose (cid:6)

= hA

; T

i satis(cid:12)able. Again, because of Theorem 3.7, there exists a

0

0

0

complete constraint system S

without clash, which de(cid:12)nes a canonical interpretation I

0

S

0

which is a model of (cid:6)

. We show that I

is also a model of (cid:6).

0

S

0

First of all, the assertions in A are satis(cid:12)ed because A (cid:18) A

, and I

0

S

satis(cid:12)es every

0

assertion in A

. To prove that I

satis(cid:12)es T , we (cid:12)rst prove the following equation:

0

S

0

I

I

0

0

S

S

A

= (cid:1)

(2)

Equation 2 is proved by showing that, for every ob ject s 2 (cid:1)

, s is in A

. In order to do

I

0

I

0

S

S

that, observe a general property of constraint systems: Every variable in S

is a successor of

0

an individual. This comes from the de(cid:12)nition of the generating rules, which add variables

to the constraint system only as direct successors of existing ob jects, and at the beginning

S

contains only individuals.

0

(cid:6)

Then, Equation 2 is proved by observing the following three facts:

131

Buchheit, Donini, & Schaerf

1. for every individual b in (cid:1)

, b 2 A

;

I

0

I

0

S

S

2. if an ob ject s is in A

, then because I

satis(cid:12)es the inclusions A

(cid:18) (8P

.A)

; : : : ;

0

I

0

S

I

0

I

0

S

S

S

1

I

0

I

0

S

S

I

0

S

A

(cid:18) (8P

.A)

, every direct successor of s is in A

;

n

3. the successor relation is closed under the direct successor relation

From the Fundamental Theorem on Induction (see e.g., Wand, 1980, page 41) we con-

clude that every ob ject s of (cid:1)

is in A

. This proves that Equation 2 holds.

I

0

I

0

S

S

From Equation 2, and the fact that I

0

S

satis(cid:12)es the inclusion A

(cid:18) (C

)

, we derive

T

I

0

I

0

S

S

that (C

)

= (cid:1)

, that is I

satis(cid:12)es the equation C

= >. Hence, from Proposition

0

T

S

T

I

I

0

0

S

S

4.1, I

0

S

satis(cid:12)es T , and this completes the proof of the theorem.

The machinery present in this proof is not new. In fact, realizing that the inclusions

A v 8P

.A; : : : ; A v 8P

.A simulate a transitive closure on the roles P

; : : : ; P

, one can

1

n

1

n

recognize similarities with the proofs given by Schild (1991) and Baader (1990a). The di(cid:11)er-

ence is that their proofs rely on the notion of connected model (Baader uses the equivalent

notion of rooted model). In contrast, the models we obtain are not connected, when the

individuals in the knowledge base are not. What we exploit is the weaker property that

every variable in the model is a successor of an individual.

Note that the above reduction strongly relies on the fact that disjunction `t' and com-

plement `:' are within the language.

In fact, disjunction and complement are necessary

in order to express all the inclusions of a TBox T inside the concept C

. Therefore, the

T

proof holds for ALC -knowledge bases, but does not hold for TKRSs not allowing for these

constructors of concepts (e.g., back).

Furthermore, for the language F L

introduced in Section 4.1, the opposite result holds.

0

In fact, McAllester (1991) proves that computing subsumption w.r.t. a set of inclusions is

EXPTIME-hard, even in the small language F L

. Conversely, Nebel (1991) proves that

0

subsumption w.r.t. a set of cyclic de(cid:12)nitions in F L

can be done in PSPACE. Combining

0

the two results, we can conclude that for F L

subsumption w.r.t. a set of inclusions and

0

subsumption w.r.t. a set of de(cid:12)nitions are in di(cid:11)erent complexity classes, hence (assuming

EXPTIME 6= PSPACE) inclusion statements are strictly more expressive than concept

de(cid:12)nitions in F L

.

0

It is still open whether inclusions and de(cid:12)nitions are equivalent in languages whose

expressivity is between F L

and ALC .

0

5. Discussion

In this paper we have proved the decidability of the main inference services of a TKRS based

on the concept language ALCN R. We believe that this result is not only of theoretical

importance, but bears some impact on existing TKRSs, because a complete procedure can

be easily devised from the calculus provided in Section 3. From this procedure, one can build

more e(cid:14)cient (but still complete) ones, as described at the end of Section 3.2, and also by

applying standard optimization techniques such as those described in (Baader, Hollunder,

Nebel, Pro(cid:12)tlich, & Franconi, 1992). An optimized procedure can perform well for small

sublanguages where reasoning is tractable, while still being complete when solving more

complex tasks. However, such a complete procedure will still take exponential time and

132

Decidable Reasoning in Terminological KR Systems

space in the worst case, and it may be argued what could be its practical applicability. We

comment in following on this point.

Firstly, a complete procedure (possibly optimized) o(cid:11)ers a benchmark for comparing

incomplete procedures, not only in terms of performance, but also in terms of missed infer-

ences. Let us illustrate this point in detail, by providing a blatant paradox: consider the

mostly incomplete constant-time procedure, answering always \No"" to any check. Obvi-

ously this useless procedure outperforms any other one, if missed inferences are not taken

into account. This paradox shows that incomplete procedures can be meaningfully com-

pared only if missed inferences are considered. But to recognize missed inferences over large

examples, one needs exactly a complete procedure|even if not an e(cid:14)cient one|like ours.

We believe that a fair detection of missed inferences would be of great help even when the

satisfaction of end users is the primary criterion for judging incomplete procedures.

Secondly, a complete procedure can be used for \anytime classi(cid:12)cation"", as proposed

in (MacGregor, 1992). The idea is to use a fast, but incomplete algorithm as a (cid:12)rst step

in analyzing the input knowledge, and then do more reasoning in background.

In the

cited paper, resolution-based theorem provers are proposed for performing this background

reasoning. We argue that any specialized complete procedure will perform better than a

general theorem prover. For instance, theorem provers are usually not speci(cid:12)cally designed

to deal with (cid:12)ltration techniques.

Moreover, our calculus can be easily adapted to deal with rules. As outlined in the

introduction, rules are often used in practical TKRSs. Rules behave like one-way concept

inclusions|no contrapositive is allowed|and they are applied only to known individuals.

Our result shows that rules in ALCN R can be applied also to unknown individuals (our

variables in a constraint system) without endangering decidability. This result is to be

compared with the negative result in (Baader & Hollunder, 1992), where it is shown that

subsumption becomes undecidable if rules are applied to unknown individuals in classic.

Finally, the calculus provides a new way of building incomplete procedures, by modifying

some of the propagation rules. Since the rules build up a model, modi(cid:12)cations to them

have a semantical counterpart which gives a precise account of the incomplete procedures

obtained. For example, one could limit the size of the canonical model by a polynomial in

the size of the KB. Semantically, this would mean to consider only \small"" models, which

is reasonable when the intended models for the KB are not much bigger than the size of the

KB itself. We believe that this way of designing incomplete procedures \from above"", i.e.,

starting with the complete set of inferences and weakening it, is dual to the way incomplete

procedures have been realized so far \from below"", i.e., starting with already incomplete

inferences and adding inference power by need.

Further research is still needed to address problems issuing from practical systems. For

example, to completely express role restrictions inside number restrictions, quali(cid:12)ed number

restrictions (Hollunder & Baader, 1991) should be taken into account. Also, the language

resulting from the addition of enumerated sets (called one-of in classic), and role (cid:12)llers

to ALCN R is still to be studied, although it does not seem to endanger the (cid:12)ltration

method we used. Instead, a di(cid:11)erent method might be necessary if inverse roles are added

to ALCN R, since the (cid:12)nite model property is lost (as shown in Schild, 1991). Finally, the

addition of concrete domains (Baader & Hanschke, 1991) remains open.

133

Buchheit, Donini, & Schaerf

Acknowledgements

We thank Maurizio Lenzerini for the inspiration of this work, as well as for several discus-

sions that contributed to the paper. Werner Nutt pointed out to us the observation men-

tioned at the end of Section 3, and we thank him and Franz Baader for helpful comments

on earlier drafts. We thank also the anonymous reviewers, whose stimulating comments

helped us in improving on the submitted version.

The research was partly done while the (cid:12)rst author was visiting the Dipartimento di In-

formatica e Sistemistica, Universit(cid:18)a di Roma \La Sapienza"". The third author also acknowl-

edges Yoav Shoham for his hospitality at the Computer Science Department of Stanford

University, while the author was developing part of this research.

This work has been supported by the ESPRIT Basic Research Action N.6810 (COM-

PULOG 2) and by the Progetto Finalizzato Sistemi Informatici e Calcolo Parallelo of the

CNR (Italian Research Council), LdR \Ibridi"".

References

Abrial, J. (1974). Data semantics.

In Klimbie, J., & Ko(cid:11)eman, K. (Eds.), Data Base

Management, pp. 1{59. North-Holland Publ. Co., Amsterdam.

Baader, F. (1990a). Augmenting concept languages by transitive closure of roles: An alter-

native to terminological cycles. Tech. rep. RR-90-13, Deutsches Forschungszentrum

f(cid:127)ur K(cid:127)unstliche Intelligenz (DFKI), Kaiserslautern, Germany. An abridged version ap-

peared in Proc. of the 12th Int. Joint Conf. on Arti(cid:12)cial Intel ligence IJCAI-91, pp.

446{451.

Baader, F. (1990b). Terminological cycles in KL-ONE-based knowledge representation lan-

guages. Tech. rep. RR-90-01, Deutsches Forschungszentrum f(cid:127)ur K(cid:127)unstliche Intelligenz

(DFKI), Kaiserslautern, Germany. An abridged version appeared in Proc. of the 8th

Nat. Conf. on Arti(cid:12)cial Intel ligence AAAI-90, pp. 621{626.

Baader, F., B(cid:127)urkert, H.-J., Hollunder, B., Nutt, W., & Siekmann, J. H. (1990). Concept

logics.

In Lloyd, J. W. (Ed.), Computational Logics, Symposium Proceedings, pp.

177{201. Springer-Verlag.

Baader, F., & Hanschke, P. (1991). A schema for integrating concrete domains into concept

languages. In Proc. of the 12th Int. Joint Conf. on Arti(cid:12)cial Intel ligence IJCAI-91,

pp. 452{457 Sydney.

Baader, F., & Hollunder, B. (1991). A terminological knowledge representation system with

complete inference algorithm.

In Proc. of the Workshop on Processing Declarative

Know ledge, PDK-91, Lecture Notes in Arti(cid:12)cial Intelligence, pp. 67{86. Springer-

Verlag.

Baader, F., & Hollunder, B. (1992). Embedding defaults into terminological knowledge

representation formalisms. In Proc. of the 3rd Int. Conf. on Principles of Know ledge

Representation and Reasoning KR-92, pp. 306{317. Morgan Kaufmann, Los Altos.

134

Decidable Reasoning in Terminological KR Systems

Baader, F., Hollunder, B., Nebel, B., Pro(cid:12)tlich, H.-J., & Franconi, E. (1992). An empirical

analisys of optimization techniques for terminological representation systems. In Proc.

of the 3rd Int. Conf. on Principles of Know ledge Representation and Reasoning KR-

92, pp. 270{281. Morgan Kaufmann, Los Altos.

Beck, H. W., Gala, S. K., & Navathe, S. B. (1989). Classi(cid:12)cation as a query processing

technique in the CANDIDE semantic data model. In Proc. of the 5th IEEE Int. Conf.

on Data Engineering.

Borgida, A., Brachman, R. J., McGuinness, D. L., & Alperin Resnick, L. (1989). CLASSIC:

A structural data model for ob jects. In Proc. of the ACM SIGMOD Int. Conf. on

Management of Data, pp. 59{67.

Brachman, R. J., & Levesque, H. J. (1984). The tractability of subsumption in frame-

based description languages. In Proc. of the 4th Nat. Conf. on Arti(cid:12)cial Intel ligence

AAAI-84, pp. 34{37.

Brachman, R. J., Pigman Gilbert, V., & Levesque, H. J. (1985). An essential hybrid

reasoning system: Knowledge and symbol level accounts in KRYPTON. In Proc. of

the 9th Int. Joint Conf. on Arti(cid:12)cial Intel ligence IJCAI-85, pp. 532{539 Los Angeles.

Brachman, R. J., & Schmolze, J. G. (1985). An overview of the KL-ONE knowledge repre-

sentation system. Cognitive Science, 9 (2), 171{216.

Buchheit, M., Donini, F. M., & Schaerf, A. (1993). Decidable reasoning in terminological

knowledge representation systems. Tech. rep. RR-93-10, Deutsches Forschungszen-

trum f(cid:127)ur K(cid:127)unstliche Intelligenz (DFKI), Saarbr(cid:127)ucken, Germany. An abridged version

appeared in Proc. of the 13th Int. Joint Conf. on Arti(cid:12)cial Intel ligence IJCAI-93 pp.

704{709.

Catarci, T., & Lenzerini, M. (1993). Representing and using interschema knowledge in

cooperative information systems. Journal of Intel ligent and Cooperative Inf. Syst. To

appear.

Dionne, R., Mays, E., & Oles, F. J. (1992). A non-well-founded approach to terminological

cycles. In Proc. of the 10th Nat. Conf. on Arti(cid:12)cial Intel ligence AAAI-92, pp. 761{766.

AAAI Press/The MIT Press.

Dionne, R., Mays, E., & Oles, F. J. (1993). The equivalence of model theoretic and structural

subsumption in description logics. In Proc. of the 13th Int. Joint Conf. on Arti(cid:12)cial

Intel ligence IJCAI-93, pp. 710{716 Chambery, France. Morgan Kaufmann, Los Altos.

Donini, F. M., Hollunder, B., Lenzerini, M., Marchetti Spaccamela, A., Nardi, D., & Nutt,

W. (1992). The complexity of existential quanti(cid:12)cation in concept languages. Arti(cid:12)cial

Intel ligence, 2{3, 309{327.

Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1991a). The complexity of concept

languages.

In Allen, J., Fikes, R., & Sandewall, E. (Eds.), Proc. of the 2nd Int.

Conf. on Principles of Know ledge Representation and Reasoning KR-91, pp. 151{162.

Morgan Kaufmann, Los Altos.

135

Buchheit, Donini, & Schaerf

Donini, F. M., Lenzerini, M., Nardi, D., & Nutt, W. (1991b). Tractable concept languages.

In Proc. of the 12th Int. Joint Conf. on Arti(cid:12)cial Intel ligence IJCAI-91, pp. 458{463

Sydney.

Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1991c). A hybrid system integrating

datalog and concept languages. In Proc. of the 2nd Conf. of the Italian Association

for Arti(cid:12)cial Intel ligence, No. 549 in Lecture Notes in Arti(cid:12)cial Intelligence. Springer-

Verlag. An extended version appeared also in the Working Notes of the AAAI Fall

Symposium \Principles of Hybrid Reasoning"".

Donini, F. M., Lenzerini, M., Nardi, D., & Schaerf, A. (1993). Deduction in concept lan-

guages: From subsumption to instance checking. Journal of Logic and Computation.

To appear.

Fitting, M. (1990). First-Order Logic and Automated Theorem Proving. Springer-Verlag.

Garey, M., & Johnson, D. (1979). Computers and Intractability|A guide to NP-

completeness. W.H. Freeman and Company, San Francisco.

Harel, D. (1984). Dynamic logic. In Handbook of Philosophical Logic, Vol. 2, pp. 497{640.

D. Reidel, Dordrecht, Holland.

Heinsohn, J., Kudenko, D., Nebel, B., & Pro(cid:12)tlich, H.-J. (1992). An empirical analysis of

terminological representation systems. In Proc. of the 10th Nat. Conf. on Arti(cid:12)cial

Intel ligence AAAI-92, pp. 767{773. AAAI Press/The MIT Press.

Hollunder, B. (1990). Hybrid inferences in KL-ONE-based knowledge representation sys-

tems. In Proc. of the German Workshop on Arti(cid:12)cial Intel ligence, pp. 38{47. Springer-

Verlag.

Hollunder, B., & Baader, F. (1991). Qualifying number restrictions in concept languages.

Tech. rep. RR-91-03, Deutsches Forschungszentrum f(cid:127)ur K(cid:127)unstliche Intelligenz (DFKI),

Kaiserslautern, Germany. An abridged version appeared in Proc. of the 2nd Int. Conf.

on Principles of Know ledge Representation and Reasoning KR-91.

Hughes, G. E., & Cresswell, M. J. (1984). A Companion to Modal Logic. Methuen, London.

Kaczmarek, T. S., Bates, R., & Robins, G. (1986). Recent developments in NIKL. In Proc.

of the 5th Nat. Conf. on Arti(cid:12)cial Intel ligence AAAI-86, pp. 978{985.

Lenzerini, M., & Schaerf, A. (1991). Concept languages as query languages. In Proc. of the

9th Nat. Conf. on Arti(cid:12)cial Intel ligence AAAI-91, pp. 471{476.

Levesque, H. J. (1984). Foundations of a functional approach to knowledge representation.

Arti(cid:12)cial Intel ligence, 23, 155{212.

Lewis, H. R., & Papadimitriou, C. H. (1981). Elements of the Theory of Computation.

Prentice-Hall, Englewood Cli(cid:11)s, New Jersey.

MacGregor, R. (1991). Inside the LOOM description classi(cid:12)er. SIGART Bul letin, 2 (3),

88{92.

136

Decidable Reasoning in Terminological KR Systems

MacGregor, R. (1992). What's needed to make a description logic a good KR citizen. In

Working Notes of the AAAI Fal l Symposium on Issues on Description Logics: Users

meet Developers, pp. 53{55.

MacGregor, R., & Bates, R. (1987). The Loom knowledge representation language. Tech.

rep. ISI/RS-87-188, University of Southern California, Information Science Institute,

Marina del Rey, Cal.

MacGregor, R., & Brill, D. (1992). Recognition algorithms for the LOOM classi(cid:12)er.

In

Proc. of the 10th Nat. Conf. on Arti(cid:12)cial Intel ligence AAAI-92, pp. 774{779. AAAI

Press/The MIT Press.

Mays, E., Dionne, R., & Weida, R. (1991). K-REP system overview. SIGART Bul letin,

2 (3).

McAllester, D. (1991). Unpublished manuscript.

McGuinness, D. L. (1992). Making description logic based knowledge representation systems

more usable. In Working Notes of the AAAI Fal l Sysmposium on Issues on Description

Logics: Users meet Developers, pp. 56{58.

Mylopoulos, J., Bernstein, P., & Wong, E. (1980). A language facility for designing database-

intensive applications. ACM Trans. on Database Syst., 5 (2), 185{207.

Nebel, B. (1988). Computational complexity of terminological reasoning in BACK. Arti(cid:12)cial

Intel ligence, 34 (3), 371{383.

Nebel, B. (1990). Reasoning and Revision in Hybrid Representation Systems. Lecture Notes

in Arti(cid:12)cial Intelligence. Springer-Verlag.

Nebel, B. (1991). Terminological cycles: Semantics and computational properties. In Sowa,

J. F. (Ed.), Principles of Semantic Networks, pp. 331{361. Morgan Kaufmann, Los

Altos.

Nutt, W. (1992). Personal communication.

Patel-Schneider, P. F. (1984). Small can be beautiful in knowledge representation. In Proc.

of the IEEE Workshop on Know ledge-Based Systems. An extended version appeared

as Fairchild Tech. Rep. 660 and FLAIR Tech. Rep. 37, October 1984.

Patel-Schneider, P. (1989). Undecidability of subsumption in NIKL. Arti(cid:12)cial Intel ligence,

39, 263{272.

Pratt, V. R. (1978). A practical decision method for propositional dynamic logic. In Proc.

of the 10th ACM SIGACT Symp. on Theory of Computing STOC-78, pp. 326{337.

Quantz, J., & Kindermann, C. (1990). Implementation of the BACK system version 4. Tech.

rep. KIT-Report 78, FB Informatik, Technische Universit(cid:127)at Berlin, Berlin, Germany.

Rich, editor, C. (1991). SIGART bulletin. Special issue on implemented knowledge repre-

sentation and reasoning systems. (2)3.

137

Buchheit, Donini, & Schaerf

Schaerf, A. (1993a). On the complexity of the instance checking problem in concept lan-

guages with existential quanti(cid:12)cation. Journal of Intel ligent Information Systems, 2,

265{278. An abridged version appeared in Proc. of the 7th Int. Symp. on Methodolo-

gies for Intel ligent Systems ISMIS-93.

Schaerf, A. (1993b). Reasoning with individuals in concept languages. Tech. rep. 07.93,

Dipartimento di Informatica e Sistemistica, Universit(cid:18)a di Roma \La Sapienza"". An

abridged version appeared in Proc. of the 3rd Conf. of the Italian Association for

Arti(cid:12)cial Intel ligence AI*IA-93.

Schild, K. (1988). Undecidability of subsumption in U . Tech. rep. KIT-Report 67, FB

Informatik, Technische Universit(cid:127)at Berlin, Berlin, Germany.

Schild, K. (1991). A correspondence theory for terminological logics: Preliminary report.

In Proc. of the 12th Int. Joint Conf. on Arti(cid:12)cial Intel ligence IJCAI-91, pp. 466{471

Sydney.

Schmidt-Schau(cid:25), M. (1989). Subsumption in KL-ONE is undecidable. In Brachman, R. J.,

Levesque, H. J., & Reiter, R. (Eds.), Proc. of the 1st Int. Conf. on Principles of

Know ledge Representation and Reasoning KR-89, pp. 421{431. Morgan Kaufmann,

Los Altos.

Schmidt-Schau(cid:25), M., & Smolka, G. (1991). Attributive concept descriptions with comple-

ments. Arti(cid:12)cial Intel ligence, 48 (1), 1{26.

Vardi, M., & Wolper, P. (1986). Automata-theoretic techniques for modal logics of pro-

grams. Journal of Computer and System Science, 32, 183{221. A preliminary version

appeared in Proc. of the 16th ACM SIGACT Symp. on Theory of Computing STOC-

84.

Vilain, M. (1991). Deduction as parsing: Tractable classi(cid:12)cation in the KL-ONE framework.

In Proc. of the 9th Nat. Conf. on Arti(cid:12)cial Intel ligence AAAI-91, pp. 464{470.

Wand, M. (1980).

Induction, Recursion, and Programming. North-Holland Publ. Co.,

Amsterdam.

Woods, W. A., & Schmolze, J. G. (1992). The KL-ONE family.

In Lehmann, F. (Ed.),

Semantic Networks in Arti(cid:12)cial Intel ligence, pp. 133{178. Pergamon Press. Published

as a special issue of Computers & Mathematics with Applications, Volume 23, Number

2{9.

138

","Terminological knowledge representation systems (TKRSs) are tools for
designing and using knowledge bases that make use of terminological languages
(or concept languages). We analyze from a theoretical point of view a TKRS
whose capabilities go beyond the ones of presently available TKRSs. The new
features studied, often required in practical applications, can be summarized
in three main points. First, we consider a highly expressive terminological
language, called ALCNR, including general complements of concepts, number
restrictions and role conjunction. Second, we allow to express inclusion
statements between general concepts, and terminological cycles as a particular
case. Third, we prove the decidability of a number of desirable TKRS-deduction
services (like satisfiability, subsumption and instance checking) through a
sound, complete and terminating calculus for reasoning in ALCNR-knowledge
bases. Our calculus extends the general technique of constraint systems. As a
byproduct of the proof, we get also the result that inclusion statements in
ALCNR can be simulated by terminological cycles, if descriptive semantics is
adopted."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1994) 139-158

Submitted 9/93; published 1/94

Teleo-Reactive Programs for Agent Control

Nils J. Nilsson

nilsson@cs.stanford.edu

Robotics Laboratory, Department of Computer Science

Stanford University, Stanford, CA 94305 USA

Abstract

A formalism is presented for computing and organizing actions for autonomous agents

in dynamic environments. We introduce the notion of teleo-reactive (T-R) programs whose

execution entails the construction of circuitry for the continuous computation of the param-

eters and conditions on which agent action is based. In addition to continuous feedback,

T-R programs support parameter binding and recursion. A primary di(cid:11)erence between

T-R programs and many other circuit-based systems is that the circuitry of T-R programs

is more compact; it is constructed at run time and thus does not have to anticipate al l

the contingencies that might arise over all possible runs. In addition, T-R programs are

intuitive and easy to write and are written in a form that is compatible with automatic

planning and learning methods. We brie(cid:13)y describe some experimental applications of T-R

programs in the control of simulated and actual mobile robots.

1. Introduction

Autonomous agents, such as mobile robots, typically operate in dynamic and uncertain

environments. Such environments can be sensed only imperfectly, e(cid:11)ects on them are not

always completely predictable, and they may be sub ject to changes not under the agent's

control. Designing agents to operate in these environments has presented challenges to the

standard methods of arti(cid:12)cial intelligence, which are based on explicit declarative repre-

sentations and reasoning processes. Prominent among the alternative approaches are the

so-called behavior-based, situated, and animat methods (Brooks, 1986; Maes, 1989; Kael-

bling & Rosenschein, 1990; Wilson, 1991), which convert sensory inputs into actions in a

much more direct fashion than do AI systems based on representation and reasoning. Many

of these alternative approaches share with control theory the central notion that continuous

feedback from the environment is a necessary component of e(cid:11)ective action.

Perhaps it is relatively easier for control theorists than it is for computer scientists

to deal with continuous feedback because control theorists are accustomed to thinking of

their controlling mechanisms as composed of analog electrical circuits or other physical

systems rather than as automata with discrete read-compute-write cycles. The notions of

goal-seeking servo-mechanisms, homeostasis, feedback, (cid:12)ltering, and stability|so essential

to control in dynamic environments|were all developed with analog circuitry in mind.

Circuits, by their nature, are continously responsive to their inputs.

In contrast, some of the central ideas of computer science, namely sequences, events,

discrete actions, and subroutines, seem at odds with the notion of continuous feedback.

For example, in conventional programming when one program calls another, the calling

program is suspended until the called program returns control. This feature is awkward

in applications in which the called program might encounter unexpected environmental

(cid:13)1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Nilsson

circumstances with which it was not designed to cope. In such cases, the calling program

can regain control only through interrupts explicitly provided by the programmer.

To be sure, there have been attempts to blend control theory and computer science. For

example, the work of Ramadge and Wonham (Ramadge & Wonham, 1989) on discrete-event

systems has used the computer science notions of events, grammars, and discrete states to

study the control of processes for which those ideas are appropriate. A book by Dean

and Wellman (Dean & Wellman, 1991) focusses on the overlap between control theory and

arti(cid:12)cial intelligence. But there has been little e(cid:11)ort to import fundamental control-theory

ideas into computer science. That is precisely what I set out to do in this paper.

I propose a computational system that works di(cid:11)erently than do conventional ones. The

formalism has what I call circuit semantics (Nilsson, 1992); program execution produces

(at least conceptually) electrical circuits, and it is these circuits that are used for control.

While importing the control-theory concept of continuous feedback, I nevertheless want to

retain useful ideas of computer science. My control programs will have parameters that can

be bound at run time and passed to subordinate routines. They can have a hierarchical

organization, and they can be recursive.

In contrast with some of the behavior-based

approaches, I want the programs to be responsive to stored models of the environment as

well as to their immediate sensory inputs.

The presentation of these ideas will be somewhat informal in line with my belief that

formalization is best done after a certain amount of experience has been obtained. Although

preliminary experiments indicate that the formalism works quite well, more work remains

to be done to establish its place in agent control.

2. Teleo-Reactive Sequences

2.1 Condition-Action Rules

A teleo-reactive (T-R) sequence is an agent control program that directs the agent toward a

goal (hence teleo) in a manner that takes into account changing environmental circumstances

(hence reactive). In its simplest form, it consists of an ordered set of production rules:

K

! a

1

1

K

! a

2

2

(cid:1) (cid:1) (cid:1)

K

! a

i

i

(cid:1) (cid:1) (cid:1)

K

! a

m

m

The K

are conditions (on sensory inputs and on a model of the world), and the a

i

i

are actions (on the world or which change the model). A T-R sequence is interpreted in a

manner roughly similar to the way in which some production systems are interpreted. The

list of rules is scanned from the top for the (cid:12)rst rule whose condition part is satis(cid:12)ed, and

the corresponding action is executed. T-R sequences di(cid:11)er substantively from conventional

production systems, however. T-R actions can be durative rather than discrete. A durative

140

Teleo-Reactive Programs

action is one that continues inde(cid:12)nitely. For example, a mobile robot is capable of executing

the durative action move, which propels the robot ahead (say at constant speed) inde(cid:12)nitely.

Such an action contrasts with a discrete one, such as move forward one meter. In a T-R

sequence, a durative action continues so long as its corresponding condition remains the (cid:12)rst

true condition. When the (cid:12)rst true condition changes, the action changes correspondingly.

Thus, unlike production systems in computer science, the conditions must be continuously

evaluated; the action associated with the currently (cid:12)rst true condition is always the one

being executed. An action terminates only when its energizing condition ceases to be the

(cid:12)rst true condition.

Indeed, rather than thinking of T-R sequences in terms of the computer science idea of

discrete events, it is more appropriate to think of them as being implemented by circuitry.

For example, the sequence above can be implemented by the circuit shown in (cid:12)gure 1.

Furthermore, we imagine that the conditions, K

, are also being continuously computed.

i

sensors
and
model

condition-
computing
circuits

K1

K2

K3

Km

¬

¬

a1

a2

a3

^

^

¬

^

am

Figure 1: Implementing a T-R Sequence in Circuitry

The actions, a

, of a T-R sequence can either be primitive actions, or they can be T-R

i

sequences themselves. Thus, programs written in this formalism can be hierarchical (even

recursive, as we shall see later). In the case of hierarchical programs, it is important to

realize that al l conditions at al l levels of the hierarchy are continuously being evaluated; a

high level sequence can redirect control through a di(cid:11)erent path of lower level sequences as

dictated by the values of the conditions at the various levels.

141

Nilsson

In writing a T-R sequence, a programmer ordinarily works backward from whatever goal

condition the sequence is being designed to achieve. The condition K

is taken to be the

1

goal condition, and the corresponding action, a

, is the null action. The condition K

is

1

2

the weakest condition such that when it is satis(cid:12)ed (and K

is not), the durative execution

1

of a

will (all other things being equal) eventually achieve K

. And so on. Each non-null

2

1

action, a

, is supposed to achieve a condition, K

, strictly higher in the list (j < i). The

i

j

conditions are therefore regressions (Nilsson, 1980) of higher conditions through the actions

that achieve those higher conditions.

Formally, we say that a T-R sequence satis(cid:12)es the regression property if each condition,

K

(m (cid:21) i > 1), is the regression of some higher condition in the sequence, K

(j < i),

i

j

through the action a

. We say that a T-R sequence is complete if and only if K

_ (cid:1) (cid:1) (cid:1) _ K

_

i

1

i

(cid:1) (cid:1) (cid:1) _ K

is a tautology. A T-R sequence is universal if it satis(cid:12)es the regression property

m

and is complete. It is easy to see that a universal T-R sequence will always achieve its goal

condition, K

, if there are no sensing or execution errors.

1

Sometimes an action does not have the e(cid:11)ect that was anticipated by the agent's designer

(the normal e(cid:11)ect), and sometimes exogenous events (separate from the actions of the

agent) change the world in unexpected ways. These phenomena, of course, are the reason

continuous feedback is required. Universal T-R sequences, like universal plans (Schoppers,

1987), are robust in the face of occasional deviations from normal execution. They can

also exploit serendipitous e(cid:11)ects; it may accidentally happen that an action achieves a

condition higher in the list of condition/action rules than normally expected. Even if an

action sometimes does not achieve its normal e(cid:11)ect (due to occasional sensing or execution

errors), nevertheless some action will be executed. So long as the environment does not

too often frustrate the achievement of the normal e(cid:11)ects of actions, the goal condition of a

universal T-R sequence will ultimately be achieved.

2.2 An Example

The following rather simple example should make these ideas more concrete. Consider the

simulated robots in (cid:12)gure 2. Let's suppose that these robots can move bars around in

their two-dimensional world. The robot on the right is holding a bar, and we want the

other robot to go to and grab the bar marked A. We presume that this robot can sense its

environment and can evaluate conditions which tell it whether or not it is already grabbing

bar A (is-grabbing), facing toward bar A (facing-bar), positioned with respect to bar A

so that it can reach and grab it (at-bar-center), on the perpendicular bisector of bar A

(on-bar-mid line), and facing a zone on the perpendicular bisector of bar A from which it

would be appropriate to move toward bar A (facing-mid line-zone). Let's assume also that

these conditions have some appropriate amount of hysteresis so that hunting behavior is

dampened. Suppose the robot is capable of executing the primitive actions grab-bar, move,

and rotate with the obvious e(cid:11)ects. Execution of the following T-R sequence will result in

the robot grabbing bar A:

142

Teleo-Reactive Programs

is-grabbing ! nil

at-bar-center ^ facing-bar ! grab-bar

on-bar-mid line ^ facing-bar ! move

on-bar-mid line ! rotate

facing-mid line-zone ! move

T ! rotate

bar-midline

bar-center

A

midline-zone

Figure 2: Robots and Bars

Notice how each properly executed action in this sequence achieves the condition in

the rule above it. In this way, the actions inexorably proceed toward the goal. Occasional

setbacks merely cause delays in achieving the goal so long as the actions usually

achieve

1

their normal e(cid:11)ects.

3. Teleo-Reactive Programs

3.1 Rules with Variables

We can generalize the notion of a T-R sequence by permitting the rules to contain free

variables that are bound when the sequence is \called."" We will call such a sequence a T-R

program. Additional generality is obtained if we assume that the variables are not necessarily

bound to constants but to quantities whose values are continuously being computed (as if

by circuitry) as the environment changes.

A simple example involving having a robot go to a designated goal location in two

dimensions will serve to illustrate. Suppose the goal location is given by the value of the

variable loc. At run time, loc will be bound to a pair of X; Y coordinates, although we allow

the binding to change during run time. At any time during the process, the robot's X; Y

position is given by the value of the variable position. (We assume that the robot has some

kind of navigational aid that reliably and continuously computes the value of position.)

From the instantaneous values of loc and position, the robot can compute the direction that

1. We do not choose to de(cid:12)ne usual ly more precisely here, although a probabilistic analysis could be given.

143

Nilsson

it should face to proceed in a straight line toward loc. Let the value of this direction at any

time be given by the value of the function course(position, loc). At any time during the

process, the robot's angular heading is given by the value of the variable heading. Using

these variables, the T-R program to drive the robot to loc is:

goto(loc)

equal(position, loc) ! nil

equal(heading, course(position, loc)) ! move

T ! rotate

Implementing goto(loc) in circuitry is straightforward. The single parameter of the

program is loc whose (possibly changing) value is speci(cid:12)ed at run time by a user, by a

higher level program, or by circuitry. The other (global) parameters, position and heading,

are provided by circuitry, and we assume that the function course is continuously being

computed by circuitry. Given the values of all of these parameters, computing which action

to energize is then computed by circuitry in the manner of (cid:12)gure 1.

3.2 Hierarchical Programs

Our formalism allows writing hierarchical and recursive programs in which the actions in

the rules are themselves T-R programs. As an example, we can write a recursive navigation

program that calls goto. Our new navigation program requires some more complex sensory

functions.

Imagine a function clear-path(place1, place2) that has value T if and only if

the direct path is clear between place1 and place2. (We assume the robot can compute

this function, continuously, for place1 = position, and place2 equal to any target location.)

Also imagine a function new-point(place1, place2) that computes an intermediate position

between place1 and place2 whenever clear-path does not have value T . The value of new-

point lies appropriately to the side of the obstacle determined to be between place1 and

place2 (so that if the robot heads toward new-point (cid:12)rst and then toward place2, it can

navigate around the obstacle). Both clear-path and new-point are continuously computed

by perceptual systems with which we endow the robot. We'll name our new navigation

program amble(loc). Here is the code:

amble(loc)

equal(position, loc) ! nil

clear-path(position, loc) ! goto(loc)

T ! amble(new-point(position, loc))

We show in (cid:12)gure 3 the path that a robot controlled by this program might take in

navigating around the obstacles shown. (The program doesn't necessarily compute shortest

paths; we present the program here simply as an illustration of recursion.) Note that if the

obstacle positions or goal location change during execution, these changes will be re(cid:13)ected

in the values of the parameters used by the program, and program execution will proceed in

a manner appropriate to the changes. In particular, if a clear path ever becomes manifest

144

Teleo-Reactive Programs

between the robot and the goal location, the robot will abandon moving to any subgoals

that it might have considered and begin moving directly to the goal.

goal location

Figure 3: Navigating using amble

The continuous computation of parameters involved in T-R programs and the ability of

high level programs to redirect control account for the great robustness of this formalism.

A formal syntax for T-R programs is given in (Nilsson, 1992).

3.3 Implementational Issues

The T-R formalism, with its implicit assumption of continuous computation of conditions

and parameters, should be thought of as a fully legitimate \level"" in the hierarchy of program

structure controlling the agent, regardless of how this level is implemented by levels below|

just as computer scientists think of list processing as a level of actual operation even though

it is implemented by more primitive logical operations below. If we assume (as we do) that

the pace of events in the agent's environment is slow compared with the amount of time

taken to perform the \continuous"" computations required in a T-R program, then the T-R

programmer is justi(cid:12)ed in assuming \real"" continuous sensing as s/he writes programs (even

though the underlying implentation may involve discrete sampling). We recommend the

T-R formalism only for those applications for which this assumption is justi(cid:12)ed. For those

applications, the T-R level shields the programmer from having to worry about how that

level is implemented and greatly facilitates program construction.

There are several di(cid:11)erent ways in which T-R programs can be interpreted into lower

level implementations. It is beyond the scope of this paper to do more than point out some

obvious methods, and we leave important questions about the properties of these methods

to subsequent research. One method of implementation involves the construction of actual

or simulated circuits according to the basic scheme of (cid:12)gure 1. First, the top level condition-

computing circuits (including circuits for computing parameters used in the conditions) are

constructed and allowed to function. A speci(cid:12)c action, say a

, is energized as a result. If a

i

i

145

Nilsson

is primitive, it is turned on, keeping the circuitry in place and functioning until some other

top-level action is energized, and so on. If a

is a T-R sequence, the circuitry needed to

i

implement it is constructed (just as was done at the top level), an action is selected, and

so on|and all the while levels of circuitry above are left functioning. As new lower level

circuitry is constructed, any circuitry no longer functioning (that is, circuitry no longer

\called"" by functioning higher level circuitry) can be garbage collected.

There are important questions of parameter passing and of timing in this process which

I do not deal with here|relying on the assumption that the times needed to create cir-

cuitry and for the circuitry to function are negligible compared to the pace of events in the

world. This assumption is similar to the synchrony hypothesis in the ESTEREL program-

ming language (Berry & Gonthier, 1992) where it is assumed that a program's reaction \. . .

takes no time with respect to the external environment, which remains invariant during [the

reaction].""

Although there is no reason in principle that circuitry could not be simulated or actually

constructed (using some sort of programmable network of logic gates), it is also straight-

forward to implement a T-R program using more standard computational techniques. T-R

programs can be written as LISP cond statements, and durative actions can be simulated

by iterating very short action increments. For example, the increment for the move action

for a simulated robot might move the robot ahead by a small amount. After each action

increment, the top level LISP cond is executed anew, and of course all of the functions and

parameters that it contains are evaluated anew.

In our simulations of robots moving in

two-dimensional worlds (to be discussed below), the computations involved are su(cid:14)ciently

fast to e(cid:11)ect a reasonable pace with apparent smooth motion.

This implementation method essentially involves sampling the environment at irregular

intervals. Of course, there are questions concerning how the computation times (and thus

the sampling rate) a(cid:11)ect the real-time aspects of agent behavior which we do not address

here|again assuming the sampling rate to be very short.

Whatever method is used to interpret T-R programs, care must be taken not to con(cid:13)ate

the T-R level with the levels below. The programmer ought not to have to think about

circuit simulators or sampling intervals but should imagine that sensing is done continuously

and immediately.

3.4 Graphical Representations

The goto program can be represented by a graph as well as by the list of rules used earlier.

The graphical representation of this program is shown in (cid:12)gure 4. The nodes are labeled

by conditions, and the arcs by actions. To execute the graphical version of the program, we

look for the shallowest true node (taking the goal condition as the root) and execute the

action labeling the arc leading out from that node.

In the graph of (cid:12)gure 4, each action normally achieves the condition at the head of its arc

(when the condition at the tail of the arc is the shallowest true condition). If there is more

than one action that can achieve a condition, we would have a tree instead of a single-path

graph. A more general graph, then, is a teleo-reactive tree such as that depicted in (cid:12)gure 5.

T-R trees are executed by searching for the shallowest true node and executing the action

labeling the arc leaving that node. Alternatively, we could search for that true node judged

146

Teleo-Reactive Programs

equal(position, loc)

move

equal(heading, course(position, loc))

rotate

T

Figure 4: Graphical Representation of goto

to be on a path of least cost to the goal, where some appropriate heuristic measure of cost

is used. [For simplicity, the phrase \shallowest true node"" will be taken to mean either the

shallowest true node (literally) or the true node on a path of least cost to the goal.] Ties

among several equally shallow true nodes are broken according to a (cid:12)xed tie-breaking rule.

In (cid:12)gure 5 we see that, in particular, there are at least two ways to achieve condition K

.

1

One way uses action a

(when K

is the shallowest true node), and one way uses action a

2

2

3

(when K

is the shallowest true node).

3

In analogy with the de(cid:12)nitions given for T-R sequences, a T-R tree satis(cid:12)es the regression

property if every non-root node is the regression of its parent node through the action linking

it with its parent. A T-R tree is complete if the disjunction of all of its conditions is a

tautology. A T-R tree is universal if and only if it satis(cid:12)es the regression property and is

also complete. With a (cid:12)xed tie-breaking rule, a T-R tree becomes a T-R sequence. If a

T-R tree is universal, then so will be the corresponding T-R sequence.

One might at (cid:12)rst ob ject to this method for executing a T-R tree on the grounds that

the sequence of actions that emerge will hop erratically from one path to another. But

if the tree satis(cid:12)es the regression property, and if the heuristic for measuring cost to the

goal is reasonable, then (however erratic the actions may appear to be), each successfully

executed action brings the agent closer to the goal.

4. Experiments

We have carried out several preliminary experiments with agents programmed in this lan-

guage (using LISP cond statements and short action increments). One set of experiments

uses simulated robots acting in a two-dimensional space, called Botworld

, of construction

2

2. The original Botworld interface, including the primitive perceptual functions and actions for its robots,

was designed and implemented by Jonas Karlsson for the NeXT computer system (Karlsson, 1990). Sub-

147

Nilsson

K1

a2

K2

a3

K3

Km -1

am

Km

Figure 5: A T-R Tree

materials, structures made from these materials, and other robots. The construction mate-

rials are bars, and the robots are to build structures by connecting the bars in various ways.

A robot can turn and move, can grab and release a suitably adjacent bar, can turn and move

a grabbed bar, and can connect a bar to other bars or structures. The robots continuously

sense whether or not they are holding a bar, and they \see"" in front of them (giving them

information about the location of bars and structures). Because of the existence of other

robots which may change the world in sometimes unexpected ways, it is important for each

robot to sense certain critical aspects of its environment continuously.

A typical Botworld graphical display is shown in (cid:12)gure 6.

We have written various T-R programs that cause the robots to build structures of

various kinds (like the triangle being constructed in (cid:12)gure 6). A robot controlled by one

of these programs exhibits homeostatic behavior. So long as the main goal (whatever it is)

is satis(cid:12)ed, the robot is inactive. Whenever the goal (for whatever reason) is not satis(cid:12)ed,

the robot becomes active and persists until it achieves the goal. If another agent achieves

part or all of the goal, the robot carries on appropriately from the situation it (cid:12)nds itself

in to complete the process.

In our experiments, the conditions used in the T-R rules are conditions on a model of

the environment that the robot constructs from its sensory system and maintains separately

from the T-R mechanism. The use of a model permits a robot to perform its actions in

response to all the sensory stimuli (past and present) that have been used to help construct

the model. But, if the T-R actions include direct changes to the model (in addition to those

sequently, Patrick Teo implemented a version that runs under X-windows on any of several di(cid:11)erent work-

stations (Teo, 1991, 1992). The latter version allows the simulation of several robots simultaneously|

each under the control of its own independently running process.

148

Teleo-Reactive Programs

Figure 6: Botworld Display

changes resulting from perceived changes to the environment), then there is a potential for

undesirable instabilities (as with any system with positive feedback). (The problem of how

to model the environment and how this model should be updated in response to sensory

data is a separate ma jor research problem outside the scope of the work reported here.)

In other experiments, we have used the Nomadic Technologies 100 series mobile robot.

The robot is equipped with a ring of 16 infrared sensors and a ring of 16 sonar sensors.

It is controlled via a radio modem by a Macintosh II running Allegro Common Lisp. We

have implemented robust T-R programs for some simple o(cid:14)ce-environment tasks, such as

wall-following and corridor-following (Galles, 1993). The programs were initially developed

and debugged using the Nomadics simulator of the actual robot; very few changes had to be

made in porting the programs from the simulator to the robot. In performing these tasks,

the robot is highly reactive and persistent even in the face of occasional extreme sonar or

infrared range errors and deliberate attempts to confuse it. The robot quickly adapts to

sudden changes in the environment, such as those caused by people sharing the hallways.

In writing T-R programs, one need only be concerned with inventing the appropriate

predicates using the available perceptual functions and model database. One does not need

to worry about providing interrupts of lower level programs so higher level ones can regain

control. We have found that debugging T-R programs presents some challenges, though.

Since they are designed to be quite robust in the face of environmental uncertainty, they

also sometimes work rather well even though they are not completely debugged. These

residual errors might not have undesirable e(cid:11)ects until the programs are used in higher

level programs|making the higher ones more di(cid:14)cult to debug.

149

Nilsson

5. Other Approaches for Specifying Behavior

There have been several formalisms proposed for prescribing sensory-directed, real-time

activity in dynamic environments. Some of these are closely related to the T-R formalism

proposed here. In this section I point out the ma jor similarities and di(cid:11)erences between T-R

programs and a representative, though not complete, sample of their closest relatives. The

other reactive formalisms are of two types, namely, those that sample their environments

at discrete intervals (perhaps rapidly enough to be su(cid:14)ciently reactive), and those that

create circuitry (like T-R programs). The discrete-sampling systems do not abstract this

activity into a higher level in which the environment is monitored continuously, and most

of the circuitry-creating systems do so prior to run time (unlike T-R programs which create

circuitry at run time).

5.1 Discrete-Sampling Systems

5.1.1 Production Systems

As has already been mentioned, T-R programs are similar to production systems (Water-

man & Hayes-Roth, 1978). The intermediate-level actions (ILAs) used in the SRI robot

Shakey (Nilsson, 1984) were programmed using production rules and were very much like

T-R programs. A T-R program also resembles a plan represented in triangle-table form

constructed by STRIPS (Fikes, Hart & Nilsson, 1972). Each of the conditions of a T-R

sequence corresponds to a triangle table kernel. In the PLANEX execution system for tri-

angle tables, the action corresponding to the highest-numbered satis(cid:12)ed kernel is executed.

A ma jor di(cid:11)erence between all of these previous production-system style programs and T-

R programs is that T-R programs are continuously responsive to the environment while

ordinary production systems are not.

5.1.2 Reactive Plans

Several researchers have adopted the approach of using the current situation to index into

a set of pre-arranged action sequences (George(cid:11) & Lansky, 1987; Schoppers, 1987; Firby,

1987). This set can either be large enough to cover a substantial number of the situations

in which an agent is likely to (cid:12)nd itself or it can cover all possible situations. In the latter

case, the plan set is said to be universal. Unlike T-R programs, these systems explicitly

sample their environments at discrete time steps rather than continuously. As with T-R

programs, time-space trade-o(cid:11)s must be taken into account when considering how many

di(cid:11)erent conditions must be anticipated in providing reactive plans. Ginsberg has noted

that in several domains, the number of situations likely to be encountered by the agent is

so intractably large that the agent is forced to postpone most of its planning until run time

when situations are actually encountered (Ginsberg, 1989). (For further discussion of this

point, see (Selman, 1993).) T-R programs have the advantage that at least a rudimentary

form of planning, namely parameter binding, is done at run time. The PRS system (George(cid:11)

& Lansky, 1987) is capable of more extensive planning at run time as well as reacting

appropriately to its current situation.

150

Teleo-Reactive Programs

5.1.3 Situated Control Rules

Drummond (Drummond, 1989) introduces the notion of a plan net which is a kind of Petri

net (Reisig, 1985) for representing the e(cid:11)ects of actions (which can be executed in parallel).

Taking into account the possible interactions of actions, he then pro jects the e(cid:11)ects of all

possible actions from a present state up to some horizon. These e(cid:11)ects are represented in

a structure called a plan projection. The plan pro jection is analyzed to see, for each state

in it, which states possibly have a path to the goal state. This analysis is a forward version

of the backward analysis used by a programmer in producing a T-R tree. Situated control

rules are the result of this analysis; they constrain the actions that might be taken at any

state to those which will result in a state that still possibly has a path to the goal. Plan

nets and Petri nets are based on discrete events and thus are not continuously responsive

to their environments in the way that T-R programs are.

5.2 Circuit-Based Systems

Kaelbling has proposed a formalism called GAPPS (Kaelbling, 1988; Kaelbling & Rosen-

schein, 1990), involving goal reduction rules, for implicitly describing how to achieve goals.

The GAPPS programmer de(cid:12)nes the activity of an agent by providing su(cid:14)cient goal re-

duction rules to connect the agent's goals with the situations in which it might (cid:12)nd itself.

These rules are then compiled into circuitry for real-time control of the agent. Rosenschein

and Kaelbling (Rosenschein & Kaelbling, 1986) call such circuitry situated automata.

A collection of GAPPS rules for achieving a goal can be thought of as an implicit

speci(cid:12)cation of a T-R program in which the computations needed to construct the program

are performed when the rules are compiled. The GAPPS programmer typically exerts less

speci(cid:12)c control over the agent's activity|leaving some of the work to the search process

performed by the GAPPS compiler. For example, a T-R program to achieve a goal, p , can

be implicitly speci(cid:12)ed by the following GAPPS rule:

(defgoalr (ach ?p)

(if ((holds ?p) (do nil))

((holds (regress ?a ?p)) (do ?a))

(T ach (regress ?a ?p)) ))

The recursion de(cid:12)ned by this rule bottoms out in rules of the form:

(defgoalr (ach (cid:30))

((holds  ) (do (cid:11))) )

where (cid:30) and   are conditions and (cid:11) is a speci(cid:12)c action.

GAPPS compiles its rules into circuitry before run time, whereas the circuit implemen-

tation of a T-R program depends on parameters that are bound at run time. Both systems

result in control that is continuously responsive to the environment.

In implementing a system to play a video game, Chapman (Chapman, 1990) compiles

production-like rules into digital circuitry for real-time control using an approach that he

calls \arbitration macrology."" As in situated automata, the compilation process occurs

prior to run time.

Brooks has developed a behavior language, BL, (Brooks, 1989), for writing reactive

robot control programs based on his \subsumption architecture"" (Brooks, 1986). A similar

language, ALFA, has been implemented by Gat (Gat, 1991). Programs written in these

151

Nilsson

languages compile into structures very much like circuits. Again, compilation occurs prior

to run time. It has been relatively straightforward to translate examples of subsumption-

architecture programs into T-R programs.

In all of these circuit-based systems, pre-run-time compiling means that more circuitry

must be built than might be needed in any given run because all possible contingencies

must be anticipated at compile time.

But in T-R programs, parameters are bound at run

3

time, and only that circuitry required for these speci(cid:12)c bindings is constructed.

6. Future Work

The T-R formalism might easily be augmented to embody some features that have not been

discussed in this paper. Explicit reference to time in specifying actions might be necessary.

For example, we might want to make sure that some action a is not initiated until after some

time t

and ceases after some time t

. Time predicates, whose time terms are evaluated

1

2

using an internal clock, may su(cid:14)ce for this purpose.

Also, in some applications we may want to control which conditions in a T-R program

are actually tested. It may be, for example, that some conditions won't have to be checked

because their truth or falsity can be guessed with compelling accuracy.

Simultaneous and asynchronous execution of multiple actions can be achieved by al-

lowing the right-hand side of rules to contain sets of actions. Each member of the set is

then duratively executed asynchronously and independently (so long as the condition in the

rule that sustains this set remains the highest true condition). Of course, the programmer

must decide under what conditions it is appropriate to call for parallel actions. Future

work on related formalisms might reveal ways in which parallel actions might emerge from

the interaction of the program and its environment rather than having to be explicitly

programmed.

Although we intend that T-R programs for agent control be written by human pro-

grammers, we are also interested in methods for modifying them by automatic planning

and machine learning. We will brie(cid:13)y discuss some of our preliminary ideas on planning

and learning here.

T-R trees resemble the search trees constructed by those planning systems that work

backwards from a goal condition. The overall goal is the root of the tree; any non-root

node g

is the regression of its parent node, g

through the action, a

, connecting them.

i

j

k

This similarity suggests that T-R trees can be constructed (and modi(cid:12)ed) by an automatic

planning system capable of regressing conditions through durative actions. Indeed triangle

tables (Fikes, Hart & Nilsson, 1972), a degenerate form of T-R tree consisting of only a

single path, were constructed by an automatic planning system and an EBL-style generalizer

(Mitchell, Keller & Kedar-Cabelli, 1986).

The reader might ob ject that there is no reason to suppose that the search trees pro-

duced by an automatic planning process will contain nodes whose conditions are those that

the agent is likely to encounter in its behavior. A process of incremental modi(cid:12)cation, how-

ever, should gradually make these constructed trees more and more matched to the agent's

environment. If a tree for achieving a desired goal has no true nodes in a certain situation,

3. Agre's \running arguments"" construct (Agre, 1989) is one example of a circuit-based system that can

add circuitry at run time as needed.

152

Teleo-Reactive Programs

it is as if the search process employed by an automatic planner had not yet terminated

because no subgoal in the search tree was satis(cid:12)ed in the current state. In this case, the

planning system can be called upon to continue to search; that is, the existing T-R tree will

be expanded until a true node is produced. Pruning of T-R trees can be accomplished by

keeping statistics on how often their nodes are satis(cid:12)ed. Portions of the trees that are never

or seldom used can be erased. Early unpublished work by Scott Benson indicates that T-R

programs can be e(cid:11)ectively generated by automatic planning methods (Benson, 1993).

In considering learning mechanisms, we note (cid:12)rst that T-R sequences are related to a

class of Boolean functions that Rivest has termed k-decision lists (Rivest, 1987; Kohavi &

Benson, 1993). A k-decision list is an ordered list of condition-value pairs in which each

condition is a conjunction of Boolean variables of length at most k , and each value is a truth

value (T or F ). The value of the Boolean function represented by a k-decision list is that

value associated with the highest true condition. Rivest has shown that such functions are

polynomially PAC learnable and has presented a supervised learning procedure for them.

We can see that a T-R sequence whose conditions are limited to k-length conjunctions of

Boolean features is a slight generalization of k-decision lists. The only di(cid:11)erence is that

such a T-R sequence can have more than two di(cid:11)erent \values"" (that is, actions). We

observe that such a T-R sequence (with, say, n di(cid:11)erent actions) is also PAC learnable

since its actions can be encoded with log

n decision lists. George John (John, 1993) has

2

investigated a supervised learning mechanism for learning T-R sequences.

Typically, the conditions used in T-R programs are conjunctions of propositional fea-

tures of the robot's world and/or model. Because a linear threshold function can implement

conjunctions, one is led to propose a neural net implementation of a T-R sequence. The neu-

ral net implementation, in turn, evokes ideas about possible learning mechanisms. Consider

the T-R sequence:

K

! a

1

1

K

! a

2

2

(cid:1) (cid:1) (cid:1)

K

! a

i

i

(cid:1) (cid:1) (cid:1)

K

! a

m

m

Suppose we stipulate that the K

are linear threshold functions of a set of propositional

i

features. The a

are not all necessarily distinct; in fact we will assume that there are only

i

k (cid:20) m distinct actions. Let these be denoted by b

; (cid:1) (cid:1) (cid:1) ; b

. The network structure in (cid:12)gure

1

k

7 implements such a T-R sequence.

The propositional features tested by the conditions are grouped into an n-dimensional

binary (0,1) vector, X called the input vector. The m conditions are implemented by m

threshold elements having weighted connections to the components of the input vector. The

process of (cid:12)nding the (cid:12)rst true condition is implemented by a layer containing appropriate

inhibitory weights and AND units such that only one AND unit can ever have an output

value of 1, and that unit corresponds to the (cid:12)rst true condition. A unique action is associated

with each condition through a layer of binary-valued weights and OR-unit associators. Each

153

Nilsson

X

K1

K2
. . .

Ki
. . .

Km

input
vector

conditions

inhibitory weights

1 or 0 weights

b1

b2

bi

V

V
. . .

V
. . .

V

bk

associators

actions

(OR
units)

. . .

. . .

AND
units

Figure 7: A Neural Net that Implements a T-R Sequence

AND unit is connected to one and only one associator by a non-zero weight. Since only

one AND unit can have a non-zero output, only that unit's associator can have a non-zero

output. (But each associator could be connected to multiple AND units.) For example, if

action b

is to be associated with conditions K

and K

, then there will be unit weights from

i

j

l

the j-th and l-th AND units to the associator representing action b

and zero-valued weights

i

from all other AND units to that associator. The action selected for execution is the action

corresponding to the single associator having the non-zero output. We are investigating

various learning methods suggested by this neural net implementation.

Work must also be done on the question of what constitutes a goal. I have assumed

goals of achievement. Can mechanisms be found that continously avoid making certain

conditions true (or false) while attempting to achieve others? Or suppose priorities on a

number of possibly mutually contradictory conditions are speci(cid:12)ed; what are reasonable

methods for attending to those achievable goals having the highest priorities?

Also, it will be interesting to ask in what sense T-R programs can be proved to be correct.

It would seem that veri(cid:12)cation would have to make assumptions about the dynamics of the

environment; some environments might be so malevolent that agents in them could never

achieve their goals. Even so, a veri(cid:12)er equipped with a model of the e(cid:11)ects of actions could

at least check to see that the regression property was satis(cid:12)ed and note any lapses.

More work remains on methods of implementing or interpreting T-R programs and

the real-time properties of implementations. These properties will, of course, depend on

the depth of the T-R program hierarchy and on the conditions and features that must be

evaluated.

154

Teleo-Reactive Programs

Finally, it might be worthwhile to investigate \fuzzy"" versions of T-R trees. One could

imagine fuzzy predicates that would energize actions with a \strength"" that depends on

the degree to which the predicates are true. The SRI robot, Flakey, uses a fuzzy controller

(Sa(cid:14)otti, Ruspini & Konolige, 1993).

7. Conclusions

I have presented a formalism for specifying actions in dynamic and uncertain domains. Since

this work rests on ideas somewhat di(cid:11)erent than those of conventional computer science, I

expect that considerably more analysis and experimentation will be required before the T-R

formalism can be fully evaluated. The need in robotics for control-theoretic ideas such as

homeostasis, continuous feedback, and stability appears to be su(cid:14)ciently strong, however,

that it seems appropriate for candidate formalisms embodying these ideas to be put forward

for consideration.

Experiments with the language will produce a stock of advice about how to write T-R

programs e(cid:11)ectively. Already, for example, it is apparent that a sustaining condition in a

T-R sequence must be carefully speci(cid:12)ed so that it is no more restrictive than it really needs

to be; an overly restrictive condition is likely to be rendered false by the very action that

it is supposed to sustain before that action succeeds in making a higher condition in the

sequence true. But, of course, overly restrictive conditions won't occur in T-R programs

that satisfy the regression property.

To be usefully employed, T-R programs (or any programs controlling agent action)

need to be embodied in an overall agent architecture that integrates perceptual processing,

goal selection, action computation, environmental modeling, and planning and learning

mechanisms. Several architectural schemes have been suggested, and we will not summarize

them here except to say that three layers of control are often delineated. A typical example

is the SSS architecture of Connell (Connell, 1993). His top (Symbolic) layer does overall

goal setting and sequencing, the middle (Subsumption) level selects speci(cid:12)c actions, and

the lower (Servo) level exerts standard feedback control over the e(cid:11)ectors. We believe T-R

programs would most appropriately be used in the middle level of such architectures.

The ma jor limitation of T-R programs is that they involve much more computation

than do programs that check only relevant conditions. Most of the conditions computed by

a T-R program in selecting an action are either irrelevant to the situation at hand or have

values that might be accurately predicted (if the programmer wanted to take the trouble

to do so). We are essentially trading computing time for ease of programming, and our

particular trade will only be advantageous in certain applications. Among these, I think, is

the mid-level control of robots and (possibly) software agents.

In conclusion, there are three main features embodied in the T-R formalism. One is

continuous computation of the parameters and conditions on which action is based. T-

R programs allow for continuous feedback while still supporting parameter binding and

recursion. The second feature is the regression relationship between conditions in a T-R

program. Each condition is the regression of some condition closer to the goal through an

action that normally achieves that closer-to-the-goal condition. The regression property

assures robust goal-seeking behavior. Third, the conceptual circuitry controlling action is

constructed at run time, and this feature permits programs to be universal while still being

155

Nilsson

compact. In addition, T-R programs are intuitive and easy to write and are written in a

formalism that is compatible with automatic planning and learning methods.

Acknowledgements

I trace my interest in reactive, yet purposive, systems to my early collaborative work on

triangle tables and ILAs. Several former Stanford students, including Jonas Karlsson, Eric

Ly, Rebecca Moore, and Mark Torrance, helped in the early stages of this work.

I also

want to thank my sabbatical hosts, Prof. Rodney Brooks at MIT, Prof. Barbara Grosz at

Harvard, and the people at the Santa Fe Institute. More recently, I have bene(cid:12)tted from

discussions with Scott Benson, George John, and Ron Kohavi. I also thank the anonymous

referees for their helpful suggestions. This work was performed under NASA Grant NCC2-

494 and NSF Grant IRI-9116399.

References

Agre, P. (1989). The Dynamic Structure of Everyday Life. Tech. rep. TR 1085, AI Lab.,

Massachusetts Institute of Technology.

Benson, S. (1993). Unpublished working paper. Robotics Laboratory, Stanford University.

Berry, G., & Gonthier, G. (1992). The ESTEREL Synchronous Programming Language.

Science of Computer Programming, 19, no. 2, 87-152, November.

Brooks, R. (1986). A Robust Layered Control System for a Mobile Robot. IEEE Journal of

Robotics and Automation, March.

Brooks, R. (1989). The Behavior Language User's Guide. Seymour Implementation Note 2,

AI Lab., Massachusetts Institute of Technology.

Chapman, D. (1990). Vision, Instruction and Action. Tech. rep. 1204, AI Lab., Mas-

sachusetts Institute of Technology.

Connell, J. (1993). SSS: A Hybrid Architecture Applied to Robot Navigation. Research

Report, IBM Research Division, T. J. Watson Research Center, Yorktown Heights,

NY 10598.

Dean, T., & Wellman, M. (1991). Planning and Control. San Francisco, CA: Morgan Kauf-

mann.

Drummond, M. (1989). Situated Control Rules. In Proc. First International Conf. on Prin-

ciples of Know ledge Representation and Reasoning. San Francisco, CA: Morgan Kauf-

mann.

Fikes, R., Hart, P., & Nilsson, N. (1972). Learning and Executing Generalized Robot Plans.

Arti(cid:12)cial Intel ligence, 3, 251-288.

Firby, R. (1987). An Investigation into Reactive Planning in Complex Domains. In Proc.

AAAI-87. San Francisco, CA: Morgan Kaufmann.

156

Teleo-Reactive Programs

Galles, D. (1993). Map Building and Following Using Teleo-Reactive Trees. In Intel ligent

Autonomous Systems: IAS-3, Groen, F. C. A., Hirose, S. & Thorpe, C. E. (Eds.),

390-398. Washington: IOS Press.

Gat, E. (1991). ALFA: A Language for Programming Reactive Robotic Control Systems.

In Proceedings 1991 IEEE Robotics and Automation Conference.

George(cid:11), M., & Lansky, A. (1989). Reactive Reasoning and Planning. In Proc. AAAI-87.

San Francisco, CA: Morgan Kaufmann.

Ginsberg, M. L. (1989). Universal Planning: An (Almost) Universally Bad Idea. AAAI

Magazine, 10, no. 4, 40-44, Winter.

John, G. (1993). `SQUISH: A Preprocessing Method for Supervised Learning of T-R Trees

from Solution Paths, (unpublished). Robotics Laboratory, Stanford University.

Kaelbling, L. P. (1988). Goals as Parallel Program Speci(cid:12)cations. In Proceedings AAAI-88,

60-65. Menlo Park, CA: American Association for Arti(cid:12)cial Intelligence.

Kaelbling, L. P., & Rosenschein, S. J. (1990). Action and Planning in Embedded Agents.

Robotics and Autonomous Systems, 6, nos. 1 and 2, 35-48, June.

Karlsson, J. (1990). Building a Triangle Using Action Nets. Unpublished pro ject paper.

Computer Science Dept., Stanford University. June.

Kohavi, R., & Benson, S. (1993). Research Note on Decision Lists. Machine Learning, 13,

131-134.

Maes, P. (1989). How to Do the Right Thing. Connection Science, 1, no.3, 291-323.

Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. (1986). Explanation-based General-

ization: A Unifying View. Machine Learning, 1, 47-80.

Nilsson, N. J. (1980). Principles of Arti(cid:12)cial Intel ligence. San Francisco, CA: Morgan Kauf-

mann.

Nilsson, N. (Ed.) (1984). Shakey the Robot. Tech. Note 323, Arti(cid:12)cial Intelligence Center,

SRI International, Menlo Park, CA 94025.

Nilsson, N. (1992). Toward Agent Programs with Circuit Semantics. Tech. rep. STAN-CS-

92-1412, Department of Computer Science, Stanford University.

Ramadge, P. J. G., & Wonham, W. M. (1989). The Control of Discrete Event Systems.

Proceedings of the IEEE, 77, no. 1, 81-98, January.

Reisig, W. (1985). Petri Nets: An Introduction, Springer Verlag.

Rivest, R. L. (1987). Learning Decision Lists. Machine Learning, 2, 229-246.

157

Nilsson

Rosenschein, S. J. & Kaelbling, L.P. (1986). The Synthesis of Machines with Provable

Epistemic Properties. In Proceedings of the 1986 Conference on Theoretical Aspects

of Reasoning about Know ledge. Halpern, J. (Ed.), 83-98, San Francisco, CA: Morgan

Kaufmann. (Updated version: Technical Note 412, Arti(cid:12)cial Intelligence Center, SRI

International, Menlo Park, CA.)

Sa(cid:14)otti, A., Ruspini, E., & Konolige, K. (1993). Integrating Reactivity and Goal-

directedness in a Fuzzy Controller. In Proc. of the 2nd Fuzzy-IEEE Conference, San

Francisco, CA.

Schoppers, M. J. (1987). Universal Plans for Reactive Robots in Unpredictable Domains.

In Proceedings of IJCAI-87. San Francisco, CA: Morgan Kaufmann.

Selman, B. (1993). Near-Optimal Plans, Tractability, and Reactivity. Tech. rep., AI Dept.,

AT&T Bell Laboratories.

Teo, P. C-S. (1991). \Botworld,"" (unpublished). Robotics Laboratory, Computer Science

Dept., Stanford University, December.

Teo, P. C-S. (1992). Botworld Structures, (unpublished). Robotics Laboratory, Computer

Science Dept., Stanford University, June.

Waterman, D. A. & Hayes-Roth, F. (1978). An Overview of Pattern-Directed Inference

Systems. In Pattern-Directed Inference Systems, Waterman, D. A. & Hayes-Roth, F.

(Eds.), 3-22. New York:Academic Press.

Wilson, S. (1991). The Animat Path to AI. In From Animals to Animats; Proceedings of

the First International Conference on the Simulation of Adaptive Behavior, Meyer, J.

A., & Wilson, S. (Eds.). Cambridge, MA: The MIT Press/Bradford Books.

158

","A formalism is presented for computing and organizing actions for autonomous
agents in dynamic environments. We introduce the notion of teleo-reactive (T-R)
programs whose execution entails the construction of circuitry for the
continuous computation of the parameters and conditions on which agent action
is based. In addition to continuous feedback, T-R programs support parameter
binding and recursion. A primary difference between T-R programs and many other
circuit-based systems is that the circuitry of T-R programs is more compact; it
is constructed at run time and thus does not have to anticipate all the
contingencies that might arise over all possible runs. In addition, T-R
programs are intuitive and easy to write and are written in a form that is
compatible with automatic planning and learning methods. We briefly describe
some experimental applications of T-R programs in the control of simulated and
actual mobile robots."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1994) 209-229

Submitted 11/93; published 2/94

Learning the Past Tense of English Verbs:

The Symbolic Pattern Associator vs. Connectionist Models

Charles X. Ling

ling@csd.uwo.ca

Department of Computer Science

The University of Western Ontario

London, Ontario, Canada N6A 5B7

Abstract

Learning the past tense of English verbs | a seemingly minor aspect of language ac-

quisition | has generated heated debates since 1986, and has become a landmark task

for testing the adequacy of cognitive modeling. Several arti(cid:12)cial neural networks (ANNs)

have been implemented, and a challenge for better symbolic models has been posed. In

this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based upon

the decision-tree learning algorithm ID3. We conduct extensive head-to-head comparisons

on the generalization ability between ANN models and the SPA under di(cid:11)erent represen-

tations. We conclude that the SPA generalizes the past tense of unseen verbs better than

ANN models by a wide margin, and we o(cid:11)er insights as to why this should be the case.

We also discuss a new default strategy for decision-tree learning algorithms.

1. Introduction

Learning the past tense of English verbs, a seemingly minor aspect of language acquisition,

has generated heated debates since the (cid:12)rst connectionist implementation in 1986 (Rumel-

hart & McClelland, 1986). Based on their results, Rumelhart and McClelland claimed that

the use and acquisition of human knowledge of language can best be formulated by ANN

(Arti(cid:12)cial Neural Network) models without symbol processing that postulates the existence

of explicit symbolic representation and rules. Since then, learning the past tense has be-

come a landmark task for testing the adequacy of cognitive modeling. Over the years a

number of criticisms of connectionist modeling appeared (Pinker & Prince, 1988; Lachter &

Bever, 1988; Prasada & Pinker, 1993; Ling, Cherwenka, & Marinov, 1993). These criticisms

centered mainly upon the issues of high error rates and low reliability of the experimental re-

sults, the inappropriateness of the training and testing procedures, \hidden"" features of the

representation and the network architecture that facilitate learning, as well as the opaque

knowledge representation of the networks. Several subsequent attempts at improving the

original results with new ANN models have been made (Plunkett & Marchman, 1991; Cot-

trell & Plunkett, 1991; MacWhinney & Leinbach, 1991; Daugherty & Seidenberg, 1993).

Most notably, MacWhinney and Leinbach (1991) constructed a multilayer neural network

with backpropagation (BP), and attempted to answer early criticisms. On the other hand,

supporters of the symbolic approach believe that symbol structures such as parse trees,

propositions, etc., and the rules for their manipulations, are critical at the cognitive level,

while the connectionist approach may only provide an account of the neural structures

in which the traditional symbol-processing cognitive architecture is implemented (Fodor

& Pylyshyn, 1988). Pinker (1991) and Prasada and Pinker (1993) argue that a proper

(cid:13)1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Ling

accounting for regular verbs should be dependent upon production rules, while irregular

past-tense in(cid:13)ections may be generalized by ANN-like associative memory.

The proper way of debating the adequacy of symbolic and connectionist modeling is by

contrasting competitive implementations. Thus, a symbolic implementation is needed that

can be compared with the ANN models. This is, in fact, a challenge posed by MacWhinney

and Leinbach (1991), who assert that no symbolic methods would work as well as their own

model. In a section titled \Is there a better symbolic model?"" they claim:

If there were some other approach that provided an even more accurate

characterization of the learning process, we might still be forced to reject the

connectionist approach, despite its successes. The proper way of debating con-

ceptualizations is by contrasting competitive implementations. To do this in the

present case, we would need a symbolic implementation that could be contrasted

with the current implementation. (MacWhinney & Leinbach, 1991, page 153)

In this paper, we present a general-purpose Symbolic Pattern Associator (SPA) based

upon the symbolic decision tree learning algorithm ID3 (Quinlan, 1986). We have shown

(Ling & Marinov, 1993) that the SPA's results are much more psychologically realistic than

ANN models when compared with human sub jects. On the issue of the predictive accuracy,

MacWhinney and Leinbach (1991) did not report important results of their model on unseen

regular verbs. To reply to our criticism, MacWhinney (1993) re-implemented the ANN

model, and claimed that its raw generalization power is very close to that of our SPA. He

believed that this should be the case because both systems learn from the same data set:

There is a very good reason for the equivalent performance of these two

models.

[...] When two computationally powerful systems are given the same

set of input data, they both extract every bit of data regularity from that input.

Without any further processing, there is only so much blood that can be squeezed

out of a turnip, and each of our systems [SPA and ANN] extracted what they

could. (MacWhinney, 1993, page 295)

We will show that this is not the case; obviously there are reasons why one learning

algorithm outperforms another (otherwise why do we study di(cid:11)erent learning algorithms?).

The Occam's Razor Principle | preferring the simplest hypothesis over more complex

ones | creates preference biases for learning algorithms. A preference bias is a preference

order among competitive hypotheses in the hypothesis space. Di(cid:11)erent learning algorithms,

however, employ di(cid:11)erent ways of measuring simplicity, and thus concepts that they bias

to are di(cid:11)erent. How well a learning program generalizes depends upon the degree to which

the regularity of the data (cid:12)ts with its bias. We study and compare the raw generalization

ability of symbolic and ANN models on the task of learning the past tense of English

verbs. We perform extensive head-to-head comparisons between ANN and SPA, and show

the e(cid:11)ects of di(cid:11)erent representations and encodings on their generalization abilities. Our

experimental results demonstrate clearly that

1. the distributed representation, a feature that connectionists have been advocating,

does not lead to better generalization when compared with the symbolic representa-

tion, or with arbitrary error-correcting codes of a proper length;

210

Learning the Past Tense: Symbolic vs Connectionist Models

2. ANNs cannot learn the identity mapping that preserves the verb stem in the past

tense as well as the SPA can;

3. a new representation suggested by MacWhinney (1993) improves the predictive accu-

racy of both SPA and ANN, but SPA still outperforms ANN models;

4. in sum, the SPA generalizes the past tense of unseen verbs better than ANN models

by a wide margin.

In Section 5 we discuss reasons as to why the SPA is a better learning model for the

the task of English past-tense acquisition. Our results support the view that many such

rule-governed cognitive processes should be better modeled by symbolic, rather than con-

nectionist, systems.

2. Review of Previous Work

In this section, we review brie(cid:13)y the two main connectionist models of learning the past

tenses of English verbs, and the subsequent criticisms.

2.1 Rumelhart and McClelland's Model

Rumelhart and McClelland's model is based on a simple perceptron-based pattern associ-

ator interfaced with an input/output encoding/decoding network which allows the model

to associate verb stems with their past tenses using a special Wickelphone/Wickelfeature

phoneme-representation format. The learning algorithm is the classical perceptron conver-

gence procedure. The training and the testing sets are mutually disjoint in the experiments.

The errors made by the model during the training process broadly follow the U-shaped learn-

ing curve in the stages of acquisition of the English past tense exhibited by young children.

The testing sample consists of 86 \unseen"" low frequency verbs (14 irregular and 72 regular)

that are not randomly chosen. The testing sample results have a 93% error rate for the

irregulars. The regulars fare better with a 33.3% error rate. Thus, the overall error rate for

the whole testing sample is 43% | 37 wrong or ambiguous past tense forms out of 86 tested.

Rumelhart and McClelland (1986) claim that the outcome of their experiment discon(cid:12)rms

the view that there exist explicit (though inaccessible) rules that underlie human knowledge

of language.

2.2 MacWhinney and Leinbach's Model

MacWhinney and Leinbach (1991) report a new connectionist model on the learning of the

past tenses of English verbs. They claim that the results from the new simulation are far

superior to Rumelhart and McClelland's results, and that they can answer most of the crit-

icisms aimed at the earlier model. The ma jor departure from Rumelhart and McClelland's

model is that the Wickelphone/Wickelfeature representational format is replaced with the

UNIBET (MacWhinney, 1990) phoneme representational system which allows the assign-

ment of a single alphabetic/numerical letter to each of the total 36 phonemes. MacWhinney

and Leinbach use special templates with which to code each phoneme and its position in a

word. The actual input to the network is created by coding the individual phonemes as sets

211

Ling

of phonetic features in a way similar to the coding of Wickelphones as Wickelfeatures (cf

Section 4.3). The network has two layers of 200 \hidden"" units fully connected to adjacent

layers. This number was arrived at through trial and error. In addition, the network has a

special-purpose set of connections that copy the input units directly onto the output units.

Altogether, 2062 regular and irregular English verbs are selected for the experiment

| 1650 of them are used for training (1532 regular and 118 irregular), but only 13 low

frequency irregular verbs are used for testing (MacWhinney & Leinbach, 1991, page 144).

Training the network takes 24,000 epochs. At the end of training there still are 11 errors

on the irregular pasts. MacWhinney and Leinbach believe that if they allow the network

to run for several additional days and give it additional hidden unit resources, it probably

can reach complete convergence (MacWhinney & Leinbach, 1991, page 151). The only

testing error rate reported is based on a very small and biased test sample of 13 unseen

irregular verbs; 9 out of 13 are predicted incorrectly. They do not test their model on any

of the unseen regular verbs: \Unfortunately, we did not test a similar set of 13 regulars.""

(MacWhinney & Leinbach, 1991, page 151).

2.3 Criticism of the Connectionist Models

Previous and current criticisms of the connectionist models of learning the past tenses of

English verbs center mainly on several issues. Each of these issues is summarized in the

following subsections.

2.3.1 Error Rates

The error rate in producing the past tenses of the \unseen"" test verbs is very high in

both ANN models, and important tests were not carried out in MacWhinney and Leinbach

(1991) model. The experimental results indicate that neither model reaches the level of

adult competence. In addition, relatively large numbers of the errors are not psychologically

realistic since humans rarely make them.

2.3.2 Training and Testing Procedures

In both Rumelhart and McClelland's model, and MacWhinney and Leinbach's model, the

generalization ability is measured on only one training/testing sample. Further, the testing

sets are not randomly chosen, and they are very small. The accuracy in testing irregular

verbs can vary greatly depending upon the particular set of testing verbs chosen, and thus

multiple runs with large testing samples are necessary to assess the true generalization

ability of a learning model. Therefore, the results of the previous connectionist models are

not reliable. In Section 4, we set up a reliable testing procedure to compare connectionist

models with our symbolic approach. Previous connectionist simulations have also been

criticized for their crude training processes (for example, the sudden increase of regular

verbs in the training set), which create such behavior as the U-shaped learning curves.

2.3.3 Data Representation and Network Architecture

Most of the past criticisms of the connectionist models have been aimed at the data-

representation formats employed in the simulations. Lachter and Bever (1988) pointed

212

Learning the Past Tense: Symbolic vs Connectionist Models

out that the results achieved by Rumelhart and McClelland's model would have been im-

possible without the use of several TRICS (The Representations It Crucially Supposes)

introduced with the adoption of the Wickelphone/Wickelfeature representational format.

MacWhinney and Leinbach claim that they have improved upon the earlier connectionist

model by getting rid of the Wickelphone/Wickelfeature representation format, and thus

to have responded to the many criticisms that this format entailed. However, MacWhin-

ney and Leinbach also introduce several TRICS in their data-representation format. For

example, instead of coding predecessor and successor phonemes as Wickelphones, they in-

troduce special templates with which to code positional information. This means that the

network will learn to associate patterns of phoneme/positions within a predetermined con-

sonant/vowel pattern. Further, the use of restrictive templates gets rid of many English

verbs that do not (cid:12)t the chosen template. This may bias the model in favour of shorter

verbs, predominantly of Anglo-Saxon origin, and against longer verbs, predominantly com-

posite or of Latin and French origin. Another TRICS introduced is the phonetic feature

encoding (a distributed representation). It is not clear why phonetic features such as front,

centre, back, high, etc. are chosen. Do they represent (cid:12)ner grained \microfeatures"" that

help to capture the regularities in English past tenses? In Section 4.5, we will show that

the straightforward symbolic representation leads to better generalization than does the

carefully engineered distributed representation. This undermines the claimed advantages of

the distributed representation of connectionist models.

2.3.4 Knowledge Representation and Integration of Acquired Knowledge

Pinker and Prince (1988), and Lachter and Bever (1988) point out that Rumelhart and

McClelland try to model the acquisition of the production of the past tense in isolation

from the rest of the English morphological system. Rumelhart and McClelland, as well

as MacWhinney and Leinbach, assume that the acquisition process establishes a direct

mapping from the phonetic representation of the stem to the phonetic representation of

the past tense form. This direct mapping collapses some well-established distinctions such

as lexical item vs. phoneme string, and morphological category vs. morpheme. Simply

remaining at the level of phonetic patterns, it is impossible to express new categorical

information in (cid:12)rst-order (predicate/function/variable) format. One of the inherent de(cid:12)cits

of the connectionist implementations is that there is no such thing as a variable for verb

stem, and hence there is no way for the model to attain the knowledge that one could

add su(cid:14)x to a stem to get its past tense (Pinker & Prince, 1988, page 124). Since the

acquired knowledge in such networks is a large weight matrix, which usually is opaque to the

human observer, it is unclear how the phonological levels processing that the connectionist

models carry out can be integrated with the morphological, lexical, and syntactical level

of processing. Neither Rumelhart and McClelland nor MacWhinney and Leinbach address

this issue.

In contrast to ANNs whose internal representations are entirely opaque, the

SPA can represent the acquired knowledge in the form of production rules, and allow for

further processing, resulting in higher-level categories such as the verb stem and the voiced

consonants, linguistically realistic production rules using these new categories for regular

verbs, and associative templates for irregular verbs (Ling & Marinov, 1993).

213

Ling

3. The Symbolic Pattern Associator

We take up MacWhinney and Leinbach's challenge for a better symbolic model for learning

the past tense of English verbs, and present a general-purpose Symbolic Pattern Associator

(SPA)

that can generalize the past tense of unseen verbs much more accurately than

1

connectionist models in this section. Our model is symbolic for several reasons. First,

the input/output representation of the learning program is a set of phoneme symbols,

which are the basic elements governing the past-tense in(cid:13)ection. Second, the learning

program operates on those phoneme symbols directly, and the acquired knowledge can be

represented in the form of production rules using those phoneme symbols as well. Third,

those production rules at the phonological level can easily be further generalized into (cid:12)rst-

order rules that use more abstract, high-level symbolic categories such as morphemes and

the verb stem (Ling & Marinov, 1993).

In contrast, the connectionist models operate

on a distributed representation (phonetic feature vectors), and the acquired knowledge is

embedded in a large weight matrix; it is therefore hard to see how this knowledge can be

further generalized into more abstract representations and categories.

3.1 The Architecture of the Symbolic Pattern Associator

The SPA is based on C4.5 (Quinlan, 1993) which is an improved implementation of the ID3

learning algorithm (cf. (Quinlan, 1986)). ID3 is a program for inducing classi(cid:12)cation rules in

the form of decision trees from a set of classi(cid:12)ed examples. It uses information gain ratio as

a criterion for selecting attributes as roots of the subtrees. The divide-and-conquer strategy

is recursively applied in building subtrees until all remaining examples in the training set

belong to a single concept (class); then a leaf is labeled as that concept. The information

gain guides a greedy heuristic search for the locally most relevant or discriminating attribute

that maximally reduces the entropy (randomness) in the divided set of the examples. The

use of this heuristic usually results in building smal l decision trees instead of larger ones

that also (cid:12)t the training data.

If the task is to learn to classify a set of di(cid:11)erent patterns into a single class of several

mutually exclusive categories, ID3 has been shown to be comparable with neural networks

(i.e., within about 5% range on the predictive accuracy) on many real-world learning tasks

(cf. (Shavlik, Mooney, & Towell, 1991; Feng, King, Sutherland, & Henery, 1992; Ripley,

1992; Weiss & Kulikowski, 1991)). However, if the task is to classify a set of (input) patterns

into (output) patterns of many attributes, ID3 cannot be applied directly. The reason is

that if ID3 treats the di(cid:11)erent output patterns as mutually exclusive classes, the number of

classes would be exponentially large and, more importantly, any generalization of individual

output attributes within the output patterns would be lost.

To turn ID3 or any similar N-to-1 classi(cid:12)cation system into general purpose N-to-M

symbolic pattern associators, the SPA applies ID3 on all output attributes and combines

individual decision trees into a \forest"", or set of trees. A similar approach was proposed for

dealing with the distributed (binary) encoding in multiclass learning tasks such as NETtalk

(English text-to-speech mapping) (Dietterich, Hild, & Bakiri, 1990). Each tree takes as

input the set of all attributes in the input patterns, and is used to determine the value of

1. The SPA programs and relevant datasets can be obtained anonymously from ftp.csd.uwo.ca under

pub/SPA/ .

214

Learning the Past Tense: Symbolic vs Connectionist Models

one attribute in its output pattern. More speci(cid:12)cally, if a pair of input attributes ((cid:19)

to (cid:19)

)

1

n

and output attributes (!

to !

) is represented as:

1

m

(cid:19)

; :::; (cid:19)

! !

; :::; !

1

n

1

m

then the SPA will build a total of m decision trees, one for each output attribute !

(1 (cid:20)

i

i (cid:20) m) taking all input attributes (cid:19)

; :::; (cid:19)

per tree. Once all of m trees are built, the SPA

1

n

can use them jointly to determine the output pattern !

; :::; !

from any input pattern

1

m

(cid:19)

; :::; (cid:19)

.

1

n

An important feature of the SPA is explicit knowledge representation. Decision trees for

output attributes can easily be transformed into propositional production rules (Quinlan,

1993). Since entities of these rules are symbols with semantic meanings, the acquired

knowledge often is comprehensible to the human observer. In addition, further processing

and integration of these rules can yield high-level knowledge (e.g., rules using verb stems)

(Ling & Marinov, 1993). Another feature of the SPA is that the trees for di(cid:11)erent output

attributes contain identical components (branches and subtrees) (Ling & Marinov, 1993).

These components have similar roles as hidden units in ANNs since they are shared in the

decision trees of more than one output attribute. These identical components can also be

viewed as high-level concepts or feature combinations created by the learning program.

3.2 Default Strategies

An interesting research issue is how decision-tree learning algorithms handle the default

class. A default class is the class to be assigned to leaves which no training examples are

classi(cid:12)ed into. We call these leaves empty leaves. This happens when the attributes have

many di(cid:11)erent values, or when the training set is relatively small. In these cases, during the

tree construction, only a few branches are explored for some attributes. When the testing

examples fall into the empty leaves, a default strategy is needed to assign classes to those

empty leaves.

For easier understanding, we use the spelling form of verbs in this subsection to explain

how di(cid:11)erent default strategies work.

(In the actual learning experiment the verbs are

represented in phonetic form.) If we use consecutive left-to-right alphabetic representation,

the verb stems and their past tenses of a small training set can be represented as follows:

a,f,f,o,r,d,_,_,_,_,_,_,_,_,_ => a,f,f,o,r,d,e,d,_,_,_,_,_,_,_

e,a,t,_,_,_,_,_,_,_,_,_,_,_,_ => a,t,e,_,_,_,_,_,_,_,_,_,_,_,_

l,a,u,n,c,h,_,_,_,_,_,_,_,_,_ => l,a,u,n,c,h,e,d,_,_,_,_,_,_,_

l,e,a,v,e,_,_,_,_,_,_,_,_,_,_ => l,e,f,t,_,_,_,_,_,_,_,_,_,_,_

where

is used as a (cid:12)ller for empty space. The left-hand 15 columns are the input patterns

for the stems of the verbs; the right-hand 15 columns are the output patterns for their

corresponding correct past tense forms.

As we have discussed, 15 decision trees will be constructed, one for each output attribute.

The decision tree for the (cid:12)rst output attribute can be constructed (see Figure 1 (a)) from

the following 4 examples:

a,f,f,o,r,d,_,_,_,_,_,_,_,_,_ => a

e,a,t,_,_,_,_,_,_,_,_,_,_,_,_ => a

l,a,u,n,c,h,_,_,_,_,_,_,_,_,_ => l

215

Ling

l,e,a,v,e,_,_,_,_,_,_,_,_,_,_ => l

where the last column is the classi(cid:12)cation of the (cid:12)rst output attribute. However, many

other branches (such as (cid:19)

= c in Figure 1 (a)) are not explored, since no training example

1

has that attribute value. If a testing pattern has its (cid:12)rst input attribute equal to c, what

class should it be assigned to? ID3 uses the majority default. That is, the most popular

class in the whole subtree under (cid:19)

is assigned to the empty leaves. In the example above,

1

either class a or l will be chosen since they each have 2 training examples. However, this is

clearly not the right strategy for this task since a verb such as create would be output as

l...... or a......, which is incorrect. Because it is unlikely for a small training set to have all

variations of attribute values, the ma jority default strategy of ID3 is not appropriate for

this task.

ι 1

ι 6

<= Passthrough 

a

e

l

c

a

z

d

__

a:1

a:1

l:2

c:0

o:2

z:0

d:20

ι

5

<= Majority

x:n indicates that there are n examples
    classified in the leaf labelled as x.
x:0 (boxed) indicates the empty leaves.

p

r

l

k

t:10

d:2

d:5

t:0

Figure 1: (a) Passthrough default

(b) Various default

For applications such as verb past-tense learning, a new default heuristic | passthrough

| may be more suitable. That is, the classi(cid:12)cation of an empty leaf should be the same

as the attribute value of that branch. For example, using the passthrough default strategy,

create will be output as c....... The passthrough strategy gives decision trees some (cid:12)rst-order

(cid:13)avor, since the production rules for empty leaves can be represented as If Attribute = X

then Class = X where X can be any unused attribute values. Passthrough is a domain-

dependent heuristic strategy because the class labels may have nothing to do with the

attribute values in other applications.

Applying the passthrough strategy alone, however, is not adequate for every output

attribute. The endings of the regular past tenses are not identical to any of the input

patterns, and the irregular verbs may have vowel and consonant changes in the middle of

the verbs. In these cases, the ma jority default may be more suitable than the passthrough.

In order to choose the right default strategy | ma jority or passthrough | a decision is

made based upon the training data in the corresponding subtree. The SPA (cid:12)rst determines

the ma jority class, and counts the number of examples from all subtrees that belong to

this class. It then counts the number of examples in the subtrees that coincide with the

216

Learning the Past Tense: Symbolic vs Connectionist Models

passthrough strategy. These two numbers are compared, and the default strategy employed

by more examples is chosen. For instance, in the example above (see Figure 1 (a)), the

ma jority class is l (or a) having 2 instances. However, there are 3 examples coinciding with

the passthrough default: two l and one a. Thus the passthrough strategy takes over, and

assigns all empty leaves at this level. The empty attribute branch c would then be assigned

the class c. Note that the default strategy for empty leaves of attribute X depends upon

training examples falling into the subtree rooted at X . This localized method ensures that

only related ob jects have an in(cid:13)uence on calculating default classes. As a result, the SPA

can adapt the default strategy that is best suited at di(cid:11)erent levels of the decision trees. For

example, in Figure 1 (b), two di(cid:11)erent default strategies are used at di(cid:11)erent levels in the

same tree. We use the SPA with the adaptive default strategy throughout the remainder of

this paper. Note that the new default strategy is not a TRICS in the data representation;

rather, it represents a bias of the learning program. Any learning algorithm has a default

strategy independent of the data representation. The e(cid:11)ect of di(cid:11)erent data representations

on generalization is discussed in Sections 4.3, 4.5, and 4.6. The passthrough strategy can

be imposed on ANNs as well by adding a set of copy connections between the input units

and the twin output units. See Section 4.4 for detail.

3.3 Comparisons of Default Strategies of ID3, SPA, and ANN

Which default strategy do neural networks tend to take in generalizing default classes

when compared with ID3 and SPA? We conducted several experiments to determine neural

networks' default strategy. We assume that the domain has only one attribute X which

may take values a, b, c, and d. The class also can be one of the a, b, c, and d. The training

examples have attribute values a, b, and c but not d | it is reserved for testing the default

class. The training set contains multiple copies of the same example to form a certain

ma jority class. Table 1 shows two sets of training/testing examples that we used to test

and compare default strategies of ID3, SPA and neural networks.

Data set 1

Data set 2

Training examples

Training examples

Values of X Class # of copies Values of X Class # of copies

a

a

10

a

c

10

b

b

2

b

b

6

c

c

3

c

c

7

Testing example

Testing example

d

?

1

d

?

1

Table 1: Two data sets for testing default strategies of various methods.

The classi(cid:12)cation of the testing examples by ID3 and SPA is quite easy to decide. Since

ID3 takes only the ma jority default, the output class is a (with 10 training examples) for

the (cid:12)rst data set, and c (with 17 training examples) for the second data set. For SPA, the

number of examples using passthrough is 15 for the (cid:12)rst data set, and 13 for the second

217

Ling

data set. Therefore, the passthrough strategy wins in the (cid:12)rst case with the output class

d, and the ma jority strategy wins in the second case with the output class c.

For neural networks, various coding methods were used to represent values of the at-

tribute X . In the dense coding, we used 00 to represent a, 01 for b, 10 for c and 11 for

d. We also tried the standard one-per-class encoding, and real number encoding (0.2 for a,

0.4 for b, 0.6 for c and 0.8 for d). The networks were trained using as few hidden units as

possible in each case. We found that in most cases the classi(cid:12)cation of the testing exam-

ple is not stable; it varies with di(cid:11)erent random seeds that initialize the networks. Table

2 summarises the experimental results. For ANNs, various classi(cid:12)cations obtained by 20

di(cid:11)erent random seeds are listed with the (cid:12)rst ones occurring most frequently.

It seems

that not only do neural networks not have a consistent default strategy, but also that it

is neither the ma jority default as in ID3 nor the passthrough default as in SPA. This may

explain why connectionist models cannot generalize unseen regular verbs well even when

the training set contains only regular verbs (see Section 4.4). The networks have di(cid:14)culty

(or are underconstrained) in generalizing the identity mapping that copies the attributes of

the verb stems into the past tenses.

The classi(cid:12)cation for the testing example

Data set 1 Data set 2

ID3

a

c

SPA

d

c

ANN, dense coding

b; c

b

ANN, one-per-class

b; c; a

c; b

ANN, real numbers

c; d

d; c

Table 2: Default strategies of ID3, SPA and ANN on two data sets.

4. Head-to-head Comparisons between Symbolic and ANN Models

In this section, we perform a series of extensive head-to-head comparisons using several

di(cid:11)erent representations and encoding methods, and demonstrate that the SPA generalizes

the past tense of unseen verbs better than ANN models do by a wide margin.

4.1 Format of the data

Our verb set came from MacWhinney's original

list of verbs. The set contains about

1400 stem/past tense pairs. Learning is based upon the phonological UNIBET repre-

sentation (MacWhinney, 1990), in which di(cid:11)erent phonemes are represented by di(cid:11)erent

alphabetic/numerical letters. There is a total of 36 phonemes. The source (cid:12)le is transferred

into the standard format of pairs of input and output patterns. For example, the verbs in

Table 3 are represented as pairs of input and output patterns (verb stem => past tense):

6,b,&,n,d,6,n

=>

6,b,&,n,d,6,n,d

I,k,s,E,l,6,r,e,t => I,k,s,E,l,6,r,e,t,I,d

218

Learning the Past Tense: Symbolic vs Connectionist Models

6,r,3,z => 6,r,o,z

b,I,k,6,m => b,I,k,e,m

See Table 3 (The original verb set is available in Online Appendix 1). We keep only one

form of the past tense among multiple past tenses (such as hang-hanged and hang-hung)

in the data set. In addition, no homophones exist in the original data set. Consequently,

there is no noise (contradictory data which have the same input pattern but di(cid:11)erent output

patterns) in the training and testing examples. Note also that information as to whether

the verb is regular or irregular is not provided in training/testing processes.

base (stem)

UNIBET

b=base

1 = irregular

spelling form phonetic form d= past tense

0 = regular

abandon

6b&nd6n

b

0

abandoned

6b&nd6nd

d

0

bene(cid:12)t

bEn6fIt

b

0

bene(cid:12)ted

bEn6fItId

d

0

arise

6r3z

b

0

arose

6roz

d

1

become

bIk6m

b

0

became

bIkem

d

1

......

Table 3: Source (cid:12)le from MacWhinney and Leinbach.

4.2 Experiment Setup

To guarantee unbiased and reliable comparison results, we use training and testing samples

randomly drawn in several independent runs. Both SPA and ANN are provided with the

same sets of training/testing examples for each run. This allows us to achieve a reliable

estimate of the inductive generalization capabilities of each model on this task.

The neural network program we used is a package called Xerion, which was developed

at the University of Toronto. It has several more sophisticated search mechanisms than

the standard steepest gradient descent method with momentum. We found that training

with the conjugate-gradient method is much faster than with the standard backpropagation

algorithm. Using the conjugate-gradient method also avoids the need to search for proper

settings of parameters such as the learning rate. However, we do need to determine the

proper number of hidden units.

In the experiments with ANNs, we (cid:12)rst tried various

numbers of hidden units and chose the one that produced the best predictive accuracy in

a trial run, and then use the network with that number of hidden units in the actual runs.

The SPA, on the other hand, has no parameters to adjust.

One ma jor di(cid:11)erence in implementation between ANNs and SPA is that SPA can take

(symbolic) phoneme letters directly while ANNs normally encode each phoneme letter to

binary bits. (Of course, SPA also can apply to the binary representation). We studied

various binary encoding methods and compared results with SPA using symbolic letter

219

Ling

representation. Since outputs of neural networks are real numbers, we need to decode the

network outputs back to phoneme letters. We used the standard method of decoding: the

phoneme letter that has the minimal real-number Hamming distance (smallest angle) with

the network outputs was chosen. To see how binary encoding a(cid:11)ects the generalization,

the SPA was also trained with the binary representation. Since the SPA's outputs are

binary, the decoding process may tie with several phoneme letters.

In this case, one of

them is chosen randomly. This re(cid:13)ects the probability of the correct decoding at the level

of phoneme letters. When all of the phoneme letters are decoded, if one or more letters are

incorrect, the whole pattern is counted as incorrect at the word level.

4.3 Templated, Distributed Representation

This set of experiments was conducted using the distributed representation suggested by

MacWhinney and Leinbach (1991). According to MacWhinney and Leinbach, the output is

a left-justi(cid:12)ed template in the format of CCCVVCCCVVCCCVVCCC, where C stands for

consonant and V for vowel space holders. The input has two components: a left-justi(cid:12)ed

template in the same format as the input, and a right-justi(cid:12)ed template in the format of

VVCCC. For example, the verb bet, represented in UNIBET coding as bEt, is shown in the

template format as follows ( is the blank phoneme):

INPUT

bEt

b__E_t____________

_E__t

template:

CCCVVCCCVVCCCVVCCC

VVCCC

(left-justified)

(right-justified)

OUTPUT

bEt

b__E_t____________

template:

CCCVVCCCVVCCCVVCCC

(left-justified)

A speci(cid:12)c distributed representation | a set of (binary) phonetic features | is used

to encode all phoneme letters for the connectionist networks. Each vowel (V in the above

templates) is encoded by 8 phonetic features (front, centre, back, high, low, middle, round,

and diphthong) and each consonant (C in the above templates) by 10 phonetic features

(voiced, labial, dental, palatal, velar, nasal, liquid, trill, fricative and interdental). Note

that because the two feature sets of vowels and consonants are not identical, templates are

needed in order to decode the right type of the phoneme letters from the outputs of the

network.

In our experimental comparison, we decided not to use the right-justi(cid:12)ed template

(VVCCC) since this information is redundant. Therefore, we used only the left-justi(cid:12)ed

template (CCCVVCCCVVCCCVVCCC) in both input and output. (The whole verb set

in the templated phoneme representation is available in Online Appendix 1. It contains

1320 pairs of verb stems and past tenses that (cid:12)t the template). To ease implementation,

we added two extra features that always were assigned to 0 in the vowel phonetic feature

set. Therefore, both vowels and consonants were encoded by 10 binary bits. The ANN

thus had 18 (cid:2) 10 = 180 input bits and 180 output bits, and we found that one layer of 200

hidden units (same as MacWhinney (1993) model) reached the highest predictive accuracy

in a trial run. See Figure 2 for the network architecture used.

220

Learning the Past Tense: Symbolic vs Connectionist Models

(180 output units)

...

C

...

C

...

V

...

V

. . . . . . 

CCCVVCCCVV

(full connection between the two layers)

. . . . . .

(200 hidden units)

. . . . . .

(full connection between the two layers)

...

C

...

C

...

V

...

V

. . . . . . 

CCCVVCCCVV

(180 input units)

C

C

...

C

...

C

...

C

...

C

...

C

...

C

Figure 2: The architecture of the network used in the experiment.

The SPA was trained and tested on the same data sets but with phoneme letters directly;

that is, 18 decision trees were built for each of the phoneme letters in the output templates.

To see how phonetic feature encoding a(cid:11)ects the generalization, we also trained the SPA

with the the same distributed representation | binary bit patterns of 180 input bits and

180 output bits | exactly the same as those in the ANN simulation. In addition, to see how

the \symbolic"" encoding works in ANN, we also train another neural network (with 120

hidden units) with the \one-per-class"" encoding. That is, each phoneme letter (total of 37;

36 phoneme letters plus one for blank) is encoded by 37 bits, one for each phoneme letter.

We used 500 verb pairs (including both regular and irregular verbs) in the training and

testing sets. Sampling was done randomly without replacement, and training and testing

sets were disjoint. Three runs of SPA and ANN were conducted, and both SPA and ANN

were trained and tested on the same data set in each run. Training reached 100% accuracy

for SPA and around 99% for ANN.

Testing accuracy on novel verbs produced some interesting results. The ANN model

and the SPA using the distributed representation have very similar accuracy, with ANN

slightly better. This may well be caused by the binary outputs of SPA that suppress the

(cid:12)ne di(cid:11)erences in prediction. On the other hand, the SPA using phoneme letters directly

produces much higher accuracy on testing. The SPA outperforms neural networks (with

either distributed or one-per-class representations) by 20 percentage points! The testing

results of ANN and SPA can be found in Table 4. Our (cid:12)ndings clearly indicate that the

SPA using symbolic representation leads to much better generalization than ANN models.

4.4 Learning Regular Verbs

Predicting the past tense of an unseen verb, which can be either regular or irregular, is

not an easy task.

Irregular verbs are not learned by rote as traditionally thought since

221

Ling

Distributed representation

Symbolic representation

ANN: % Correct

SPA: % Correct

ANN: % Correct

SPA: % Correct

Reg

Irrg Comb Reg

Irrg Comb Reg

Irrg Comb Reg

Irrg Comb

65.3

14.6

60.4

62.2

18.8

58.0

63.3

18.8

59.2

83.0

29.2

77.8

59.7

8.6

53.8

57.9

8.2

52.2

58.8

10.3

53.2

83.3

22.4

76.2

60.0

16.0

55.6

58.0

8.0

53.0

58.7

16.0

54.4

80.9

20.0

74.8

61.7

13.1

56.6

59.4

11.7

54.4

60.3

15.0

55.6

82.4

23.9

76.3

Table 4: Comparisons of testing accuracy of SPA and ANN with distributed and symbolic

representations.

children and adults occasionally extend irregular in(cid:13)ection to irregular-sounding regular

verbs or pseudo verbs (such as cleef | cleft) (Prasada & Pinker, 1993). The more similar

the novel verb is to the cluster of irregular verbs with similar phonological patterns, the

more likely the prediction of an irregular past-tense form. Pinker (1991) and Prasada and

Pinker (1993) argue that regular past tenses are governed by rules, while irregulars may

be generated by the associated memory which has this graded e(cid:11)ect of irregular past-tense

generalization.

It is would be interesting, therefore, to compare SPA and ANN on the

past-tense generalization of regular verbs only. Because both SPA and ANN use the same,

position speci(cid:12)c, representation, learning regular past tenses would require learning di(cid:11)erent

su(cid:14)xes

at di(cid:11)erent positions, and to learn the identity mapping that copies the verb stem

2

to the past tenses for verbs of di(cid:11)erent lengths.

We used the same templated representation as in the previous section, but both training

and testing sets contained only regular verbs. Again samples were drawn randomly without

replacement. To maximize the size of the testing sets, testing sets simply consisted of all

regular verbs that were not sampled in the training sets. The same training and testing sets

were used for each of the following methods compared. To see the e(cid:11)ect of the adaptive

default strategy (as discussed in Section 3.2) on generalization, the SPA with the ma jority

default only and with the adaptive default were both tested. The ANN models were similar

to those used in the previous section (except with 160 one-layer hidden units, which turned

out to have the best predictive accuracy in a test run). The passthrough default strategy can

be imposed on neural networks by adding a set of copy connections that connect directly

from the input units to the twin output units. MacWhinney and Leinbach (1991) used

such copy connections in their simulation. We therefore tested the networks with the copy

connection to see if generalization would be improved as well.

The results on the predictive accuracy of the SPA and ANNs on one run with with

randomly sampled training and testing sets are summarized in Table 5. As we can see,

the SPA with the adaptive default strategy, which combines the ma jority and passthrough

default, outperforms the SPA with only the ma jority default strategy used in ID3. The

2. In phonological form there are three di(cid:11)erent su(cid:14)xes for regular verbs. When the verb stem ends with

t or d (UNIBET phonetic representations), then the su(cid:14)x is Id. For example, extend | extended (in

spelling form). When the verb stem ends with a unvoiced consonant, the su(cid:14)x is t. For example, talk

| talked. When the verb stem ends with a voiced consonant or vowel, the su(cid:14)x is d. For example,

solve | solved.

222

Learning the Past Tense: Symbolic vs Connectionist Models

ANNs with copy connections do generalize better than the ones without. However, even

ANN models with copy connections have a lower predictive accuracy than the SPA (ma-

jority). In addition, the di(cid:11)erences in the predictive accuracy are larger with smaller sets

of training examples. Smaller training sets make the di(cid:11)erence in testing accuracy more

evident. When the training set contains 1000 patterns (out of 1184), the testing accuracy

becomes very similar, and would approach asymptotically to 100% with larger training sets.

Upon examination, most of the errors made in ANN models occur in the identity mapping

(i.e., strange phoneme change and drop); the verb stems cannot be preserved in the past

tense if the phonemes are not previously seen in the training examples. This contradicts the

(cid:12)ndings of Prasada and Pinker (1993), which show that native English speakers generate

regular su(cid:14)x-adding past tenses equally well with unfamiliar-sounding verb stems (as long

as these verb stems do not sound close to irregular verbs). This also indicates that the bias

of the ANN learning algorithms is not suitable to this type of task. See further discussion

in Section 5.

Training

Percent correct on testing

size

SPA (adaptive) SPA (ma jority) ANN (copy con.) ANN (normal)

50

55.4

30.0

14.6

7.3

100

72.9

58.6

34.6

24.9

300

87.0

83.7

59.8

58.2

500

92.5

89.0

82.6

67.9

1000

93.5

92.4

92.0

87.3

Table 5: Predictive accuracy on learning the past tense of regular verbs

4.5 Error Correcting Codes

Dietterich and Bakiri (1991) reported an increase in the predictive accuracy when error-

correcting codes of large Hamming distances are used to encode values of the attributes.

This is because codes with larger Hamming distance (d) allow for correcting fewer than d=2

bits of errors. Thus, learning programs are allowed to make some mistakes at the bit level

without their outputs being misinterpreted at the word level.

We wanted to (cid:12)nd if performances of the SPA and ANNs are improved with the error-

correcting codes encoding all of the 36 phonemes. We chose error-correcting codes ranging

from ones with small Hamming distance to ones with very large Hamming distance (using

the BHC codes, see Dietterich and Bakiri (1991)). Because the number of attributes for

each phoneme is too large, the data representation was changed slightly for this experiment.

Instead of 18 phoneme holders with templates, 8 consecutive, left-to-right phoneme holders

were used. Verbs with stems or past tenses of more than 8 phonemes were removed from the

training/testing sets. (The whole verb set in the this representation is available in Online

Appendix 1. It contains 1225 pairs of verb stems and past tenses whose lengths are shorter

than 8). Both SPA and ANN take exactly the same training/testing sets, each contains 500

pairs of verb stems and past tenses, with the error-correcting codes encoding each phoneme

223

Ling

letter. Still, training networks with 92-bit or longer error-correcting codes takes too long to

run (there are 8 (cid:2) 92 = 736 input attributes and 736 output attributes). Therefore, only

two runs with 23- and 46-bit codes were conducted. Consistent with Dietterich and Bakiri

(1991)'s (cid:12)ndings, we found that the testing accuracy generally increases when the Hamming

distance increases. However, we also observed that the testing accuracy decreases very

slightly when the codes become too long. The accuracy using 46-bit codes (with Hamming

distance of 20) reaches the maximum value (77.2%), which is quite close to the accuracy

(78.3%) of SPA using the direct phoneme letter representation. It seems there is a trade-o(cid:11)

between tolerance of errors with large Hamming distance and di(cid:14)culty in learning with

longer codes. In addition, we found the testing accuracy of ANNs to be lower than the one

of SPA for both 23 bit- and 46-bit error-correcting codes. The results are summarized in

Table 6.

ANN

Hamming Distance Correct at bit level Correct at word level

23-bit codes

10

93.5%

65.6%

46-bit codes

20

94.1%

67.4%

SPA

Hamming Distance Correct at bit level Correct at word level

23-bit codes

10

96.3%

72.4%

46-bit codes

20

96.3%

77.2%

92-bit codes

40

96.1%

75.6%

127-bit codes

54

96.1%

75.4%

Table 6: Comparisons of the testing accuracy of SPA and ANNs with error-correcting codes

Our results in this and the previous two subsections undermine the advantages of the

distributed representation of ANNs, a unique feature advocated by connectionists. We have

demonstrated that, in this task, the distributed representation actually does not allow for

adequate generalization. Both SPA using direct symbolic phoneme letters and SPA with

error-correcting codes outperform ANNs with distributed representation by a wide margin.

However, neither phoneme symbols nor bits in the error-correcting codes encode, implicitly

or explicitly, any micro-features as in the distributed representation. It may be that the

distributed representation used was not optimally designed. Nevertheless, straightforward

symbolic format requires little representation engineering compared with the distributed

representation in ANNs.

4.6 Right-justi(cid:12)ed, Isolated Su(cid:14)x Representation

MacWhinney and Leinbach (1991) did not report important results of the predictive ac-

curacy of their model on unseen regular verbs. In his reply (MacWhinney, 1993) to our

paper (Ling & Marinov, 1993), MacWhinney re-implemented the ANN model. In his new

implementation, 1,200 verb stem and past-tense pairs were in the training set, among which

1081 were regular and 119 were irregular. Training took 4,200 epochs, and reached 100%

correct on regulars and 80% on irregulars. The testing set consisted of 87 regulars and 15

irregulars. The percent correct on testing at epoch 4,200 was 91% for regulars and 27% for

irregulars, with a combined 80.0% on the testing set. MacWhinney claimed that the raw

224

Learning the Past Tense: Symbolic vs Connectionist Models

generalization power of ANN model is very close to that of our SPA. He believes that this

should be the case simply because both systems were trained on the same data set.

We realize (via private communication) that a new representation used in MacWhinney's

recent implementation plays a critical role in the improved performance. In MacWhinney's

new representation, the input (for verb stems) is coded by the right-justi(cid:12)ed template

CCCVVCCCVVCCCVVCCC. The output contains two parts: a right-justi(cid:12)ed template

that is the same as the one in the input, and a coda in the form of VVCCC. The right-

justi(cid:12)ed template in the output is used to represent the past tense without including the

su(cid:14)x for the regular verbs. The su(cid:14)x of the regular past tense always stays in the coda,

which is isolated from the main, right-justi(cid:12)ed templates. For the irregular past tense, the

coda is left empty. For example, the input and output templated patterns for the past tense

of verbs in Table 3 are represented as:

INPUT

OUTPUT

(right-justified)

(right-justified)

(suffix only)

CCCVVCCCVVCCCVVCCC

CCCVVCCCVVCCCVVCCC VVCCC

___6_b__&_nd_6_n__

___6_b__&_nd_6_n__ __d__ (for abandon-abandoned)

b__E_n__6_f__I_t__

b__E_n__6_f__I_t__ I_d__ (for benefit-benefited)

________6_r__3_z__

________6_r__o_z__ _____ (for arise-arose)

_____b__I_k__6_m__

_____b__I_k__e_m__ _____ (for become-became)

Such data representation clearly facilitates learning. For the regular verbs, the output

patterns are always identical to the input patterns. In addition, the verb-ending phoneme

letters always appear at a few (cid:12)xed positions (i.e., the right most VVCCC section in the

input template) due to the right-justi(cid:12)ed, templated representation. Furthermore, the su(cid:14)x

always occupies the coda, isolated from the right-justi(cid:12)ed templates.

We performed a series of experiments to see how much improvement we could accom-

plish using the new representation over MacWhinney's recent ANN model and over the

left-justi(cid:12)ed representation discussed in Section 4.3. Our SPA (with an averaged predictive

accuracy of 89.0%) outperforms MacWhinney's recent ANN implementation (with the pre-

dictive accuracy of 80.0%) by a wide margin. In addition, the predictive accuracy is also

improved from an average of 76.3% from the left-justi(cid:12)ed representation to 82.8% of the

right-justi(cid:12)ed, isolated su(cid:14)x one. See results in Table 7.

5. General Discussion and Conclusions

Two factors contribute to the generalization ability of a learning program. The (cid:12)rst is

the data representation, and the other is the bias of the learning program. Arriving at

the right, optimal, representation is a di(cid:14)cult task. As argued by Prasada and Pinker

(1993), regular verbs should be represented in a coarse grain in terms of the verb stem

and su(cid:14)xes; while irregular verbs in a (cid:12)ner grain in terms of phonological properties.

Admittedly, SPA works uniformly at the level of phoneme letters, as ANNs do. However,

because SPA produces simple production rules that use these phoneme letters directly, those

rules can be further generalized to (cid:12)rst-order rules with new representations such as stems

and the voiced consonants which can be used across the board in other such rule-learning

modules (Ling & Marinov, 1993). This is one of the ma jor advantages over ANN models.

225

Ling

Predictive accuracy with right-justi(cid:12)ed, isolated su(cid:14)x representation

SPA

MacWhinney's ANN model

training/testing

training/testing

training/testing

500/500

1200/102

1200/102

Run 1

81.3

89.2

Run 2

84.1

90.4

Run 3

83.1

87.4

Average

82.8

89.0

80.0 (one run)

Table 7: Comparisons of testing accuracy of SPA and ANN (with right-justi(cid:12)ed, isolated

su(cid:14)x representation)

It seems quite conceivable that children acquire these high-level concepts such as stems

and voiced consonants through learning noun plurals, verb past tense, verb third-person

singular, comparative adjectives, and so on. With a large weight matrix as the result of

learning, it is hard to see how this knowledge can be further generalized in ANN models

and shared in other modules.

Even with exactly the same data representation, there exist some learning tasks that

symbolic methods such as the SPA generalize categorically better than ANNs. The con-

verse also is true. This fact re(cid:13)ects the di(cid:11)erent inductive biases of the di(cid:11)erent learning

algorithms. The Occam's Razor Principle | preferring the simplest hypothesis over more

complex ones | creates a preference bias, a preference of choosing certain hypotheses over

others in the hypothesis space. However, di(cid:11)erent learning algorithms choose di(cid:11)erent hy-

potheses because they use di(cid:11)erent measurements for simplicity. For example, among all

possible decision trees that (cid:12)t the training examples, ID3 and SPA induce simple decision

trees instead of complicated ones. Simple decision trees can be converted to small sets of

production rules. How well a learning algorithm generalizes depends upon the degree to

which the underlying regularities of the target concept (cid:12)t its bias. In other words, if the

underlying regularities can be represented compactly in the format of hypotheses produced

by the learning algorithm, the data can be generalized well, even with a small set of training

examples. Otherwise, if the underlying regularities only have a large hypothesis, but the

algorithm is looking for compact ones (as per the Occam's Razor Principle), the hypothe-

ses inferred will not be accurate. A learning algorithm that searches for hypotheses larger

than necessary (i.e., that does not use the Occam's Razor Principle) is normally \under-

constrained""; it does not know, based on the training examples only, which of the many

competitive hypotheses of the large size should be inferred.

We also can describe the bias of a learning algorithm by looking at how training examples

of di(cid:11)erent classes are separated in the n-dimensional hyperspace where n is the number

of attributes. A decision node in a decision tree forms a hyperplane as described by a

linear function such as X = a. Not only are these hyperplanes perpendicular to the axis,

they are also partial-space hyperplanes that extend only within the subregion formed by

the hyperplanes of their parents' nodes. Likewise, hidden units with a threshold function in

ANNs can be viewed as forming hyperplanes in the hyperspace. However, unlike the ones

in the decision trees, they need not be perpendicular to any axis, and they are ful l-space

226

Learning the Past Tense: Symbolic vs Connectionist Models

hyperplanes that extend through the whole space. If ID3 is applied to the concepts that (cid:12)t

ANN's bias, especially if their hyperplanes are not perpendicular to any axis, then many

zigzag hyperplanes that are perpendicular to axes would be needed to separate di(cid:11)erent

classes of the examples. Hence, a large decision tree would be needed, but this does not (cid:12)t

ID3's bias. Similarly, if ANN learning algorithms are applied to the concepts that (cid:12)t ID3's

bias, especially if their hyperplanes form many separated, partial-space regions, then many

hidden units may be needed for these regions.

Another ma jor di(cid:11)erence between ANNs and ID3 is that ANNs have a larger variation

and a weaker bias (cf. (Geman, Bienenstock, & Doursat, 1992)) than ID3. Many more

Boolean functions (e.g., linearly separable functions) can (cid:12)t a small network (e.g., one with

no hidden units) than they can a small decision tree. This is sometimes attributed to the

claimed versatility and (cid:13)exibility of ANNs; they can learn (but not necessarily predict reli-

ably well) many functions, while symbolic methods are brittle. However, it is my belief that

we humans are versatile, not because we have a learning algorithm with a large variation,

but rather because we have a set of strong-biased learning algorithms, and we can somehow

search in the bias space and add new members into the set for the new learning tasks. Sym-

bolic learning algorithms have clear semantic components and explicit representation, and

thus we can more easily construct strong-based algorithms motivated from various speci(cid:12)c

learning tasks. The adaptive default strategy in the SPA is such an example. On the other

hand, we still largely do not know how to e(cid:11)ectively strengthen the bias of ANNs for many

speci(cid:12)c tasks (such as the identity mapping, k-term DNF, etc.). Some techniques (such

as adding copy connections and weight decaying) exist, but their exact e(cid:11)ects on biasing

towards classes of functions are not clear.

From our analyses (Ling & Marinov, 1993), the underlying regularities governing the

in(cid:13)ection of the past tense of English verbs do form a small set of production rules with

phoneme letters. This is especially so for regular verbs; all the rules are either identity

rules or the su(cid:14)x-adding rules. For example, decision trees can be converted into a set of

precedence-ordered production rules with more complicated rules (rules with more condi-

tions) listed (cid:12)rst. As an example, using consecutive, left-to-right phonetic representation, a

typical su(cid:14)x-adding rule for verb stems with 4 phoneme letters (such as talk | talked) is:

If (cid:19)

= k and (cid:19)

= , then !

= t

4

5

5

That is, if the fourth input phoneme is k and the (cid:12)fth is blank (i.e., if we are at a verb

ending) then the (cid:12)fth output phoneme is t. On the other hand, the identity-mapping rules

have only one condition. A typical identity rule looks like:

If (cid:19)

= l, then !

= l

3

3

In fact, the passthrough default strategy allows all of the identity-mapping rules to be rep-

resented in a simple (cid:12)rst-order format:

If (cid:19)

= X, then !

= X

3

3

where X can be any phoneme. Clearly, the knowledge of forming regular past tenses can

thus be expressed in simple, conjunctive rules which (cid:12)t the bias of the SPA (ID3), and

therefore, the SPA has a much better generalization ability than the ANN models.

To conclude, we have demonstrated, via extensive head-to-head comparisons, that the

SPA has a more realistic and better generalization capacity than ANNs on learning the

past tense of English verbs. We have argued that symbolic decision-tree/production-rule

learning algorithms should outperform ANNs. This is because, (cid:12)rst, the domain seems to be

227

Ling

governed by a compact set of rules, and thus (cid:12)ts the bias of our symbolic learning algorithm;

second, the SPA directly manipulates on a representation better than ANNs do (i.e., the

symbolic phoneme letters vs. the distributed representation); and third, the SPA is able

to derive high-level concepts used throughout English morphology. Our results support the

view that many such high-level, rule-governed cognitive tasks should be better modeled by

symbolic, rather than connectionist, systems.

Acknowledgements

I gratefully thank Steve Pinker for his constant encouragement, and Marin Marinov, Steve

Cherwenka and Huaqing Zeng for discussions and for help in implementing the SPA. I

thank Brian MacWhinney for providing the verb data used in his simulation. Discussions

with Tom Dietterich, Dave Touretzky and Brian MacWhinney, as well as comments from

reviewers, have been very helpful. The research is conducted with support from the NSERC

Research Grant and computing facilities from our Department.

References

Cottrell, G., & Plunkett, K. (1991). Using a recurrent net to learn the past tense.

In

Proceedings of the Cognitive Science Society Conference.

Daugherty, K., & Seidenberg, M. (1993). Beyond rules and exceptions: A connectionist

modeling approach to in(cid:13)ectional morphology.

In Lima, S. (Ed.), The Reality of

Linguistic Rules. John Benjamins.

Dietterich, T., & Bakiri, G. (1991). Error-correcting output codes: A general method for

improving multiclass inductive learning programs. In AAAI-91 (Proceedings of Ninth

National Conference on Arti(cid:12)cial Intel ligence).

Dietterich, T., Hild, H., & Bakiri, G. (1990). A comparative study of ID3 and backprop-

agation for English text-to-speech mapping. In Proceedings of the 7th International

Conference on Machine Learning. Morgan Kaufmann.

Feng, C., King, R., Sutherland, A., & Henery, R. (1992). Comparison of symbolic, statis-

tical and neural network classi(cid:12)ers. Manuscript. Department of Computer Science,

University of Ottawa.

Fodor, J., & Pylyshyn, Z. (1988). Connectionism and cognitive architecture: A critical

analysis. In Pinker, S., & Mehler, J. (Eds.), Connections and Symbols, pp. 3 { 71.

Cambridge, MA: MIT Press.

Geman, S., Bienenstock, E., & Doursat, R. (1992). Neural networks and the bias/variance

dilemma. Neural Computation, 4, 1 { 58.

Lachter, J., & Bever, T. (1988). The relation between linguistic structure and associative

theories of language learning { a constructive critique of some connectionist learning

models. In Pinker, S., & Mehler, J. (Eds.), Connections and Symbols, pp. 195 { 247.

Cambridge, MA: MIT Press.

228

Learning the Past Tense: Symbolic vs Connectionist Models

Ling, X., Cherwenka, S., & Marinov, M. (1993). A symbolic model for learning the past

tenses of English verbs. In Proceedings of IJCAI-93 (Thirteenth International Con-

ference on Arti(cid:12)cial Intel ligence), pp. 1143{1149. Morgan Kaufmann Publishers.

Ling, X., & Marinov, M. (1993). Answering the connectionist challenge: a symbolic model

of learning the past tense of English verbs. Cognition, 49 (3), 235{290.

MacWhinney, B. (1990). The CHILDES Project: Tools for Analyzing Talk. Hillsdale, NJ:

Erlbaum.

MacWhinney, B. (1993). Connections and symbols: closing the gap. Cognition, 49 (3),

291{296.

MacWhinney, B., & Leinbach, J. (1991). Implementations are not conceptualizations: Re-

vising the verb model. Cognition, 40, 121 { 157.

Pinker, S. (1991). Rules of language. Science, 253, 530 { 535.

Pinker, S., & Prince, A. (1988). On language and connectionism: Analysis of a parallel

distributed processing model of language acquisition.

In Pinker, S., & Mehler, J.

(Eds.), Connections and Symbols, pp. 73 { 193. Cambridge, MA: MIT Press.

Plunkett, K., & Marchman, V. (1991). U-shaped learning and frequency e(cid:11)ects in a mul-

tilayered perceptron: Implications for child language acquisition. Cognition, 38, 43 {

102.

Prasada, S., & Pinker, S. (1993). Generalization of regular and irregular morphological

patterns. Language and Cognitive Processes, 8 (1), 1 { 56.

Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1 (1), 81 { 106.

Quinlan, J. (1993). C4.5 Programs for Machine Learning. Morgan Kaufmann: San Mateo,

CA.

Ripley, B. (1992). Statistical aspects of neural networks.

Invited lectures for SemStat

(Seminaire Europeen de Statistique, Sandb jerg, Denmark, 25-30 April 1992).

Rumelhart, D., & McClelland, J. (1986). On learning the past tenses of English verbs.

In Rumelhart, D., McClelland, J., & the PDP Research Group (Eds.), Paral lel Dis-

tributed Processing Vol 2, pp. 216 { 271. Cambridge, MA: MIT Press.

Shavlik, J., Mooney, R., & Towell, G. (1991). Symbolic and neural learning algorithms: An

experimental comparison. Machine Learning, 6 (2), 111 { 144.

Weiss, S., & Kulikowski, C. (1991). Computer Systems that Learn: classi(cid:12)cation and predic-

tion methods from statistics, neural networks, machine learning, and expert systems.

Morgan Kaufmann, San Mateo, CA.

229

","Learning the past tense of English verbs - a seemingly minor aspect of
language acquisition - has generated heated debates since 1986, and has become
a landmark task for testing the adequacy of cognitive modeling. Several
artificial neural networks (ANNs) have been implemented, and a challenge for
better symbolic models has been posed. In this paper, we present a
general-purpose Symbolic Pattern Associator (SPA) based upon the decision-tree
learning algorithm ID3. We conduct extensive head-to-head comparisons on the
generalization ability between ANN models and the SPA under different
representations. We conclude that the SPA generalizes the past tense of unseen
verbs better than ANN models by a wide margin, and we offer insights as to why
this should be the case. We also discuss a new default strategy for
decision-tree learning algorithms."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1994) 231-255

Submitted 12/93; published 2/94

Substructure Discovery Using Minimum Description

Length and Background Knowledge

Diane J. Cook

cook@cse.uta.edu

Lawrence B. Holder

holder@cse.uta.edu

Department of Computer Science Engineering

Box 19015

University of Texas at Arlington

Arlington, TX 76019 USA

Abstract

The ability to identify interesting and repetitive substructures is an essential compo-

nent to discovering knowledge in structural data. We describe a new version of our Sub-

due substructure discovery system based on the minimum description length principle.

The Subdue system discovers substructures that compress the original data and represent

structural concepts in the data. By replacing previously-discovered substructures in the

data, multiple passes of Subdue produce a hierarchical description of the structural reg-

ularities in the data. Subdue uses a computationally-bounded inexact graph match that

identi(cid:12)es similar, but not identical, instances of a substructure and (cid:12)nds an approximate

measure of closeness of two substructures when under computational constraints. In addi-

tion to the minimum description length principle, other background knowledge can be used

by Subdue to guide the search towards more appropriate substructures. Experiments in

a variety of domains demonstrate Subdue's ability to (cid:12)nd substructures capable of com-

pressing the original data and to discover structural concepts important to the domain.

1. Introduction

The large amount of data collected today is quickly overwhelming researchers' abilities to

interpret the data and discover interesting patterns within the data. In response to this

problem, a number of researchers have developed techniques for discovering concepts in

databases. These techniques work well for data expressed in a non-structural, attribute-

value representation, and address issues of data relevance, missing data, noise and uncer-

tainty, and utilization of domain knowledge. However, recent data acquisition pro jects

are collecting structural data describing the relationships among the data ob jects. Corre-

spondingly, there exists a need for techniques to analyze and discover concepts in structural

databases.

One method for discovering knowledge in structural data is the identi(cid:12)cation of com-

mon substructures within the data. The motivation for this process is to (cid:12)nd substructures

capable of compressing the data and to identify conceptually interesting substructures that

enhance the interpretation of the data. Substructure discovery is the process of identifying

concepts describing interesting and repetitive substructures within structural data. Once

discovered, the substructure concept can be used to simplify the data by replacing instances

of the substructure with a pointer to the newly discovered concept. The discovered sub-

structure concepts allow abstraction over detailed structure in the original data and provide

(cid:13)1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Cook & Holder

new, relevant attributes for interpreting the data. Iteration of the substructure discovery

and replacement process constructs a hierarchical description of the structural data in terms

of the discovered substructures. This hierarchy provides varying levels of interpretation that

can be accessed based on the goals of the data analysis.

We describe a system called Subdue (Holder, Cook, & Bunke, 1992; Holder & Cook,

1993) that discovers interesting substructures in structural data based on the minimum

description length principle. The Subdue system discovers substructures that compress

the original data and represent structural concepts in the data. By replacing previously-

discovered substructures in the data, multiple passes of Subdue produce a hierarchical de-

scription of the structural regularities in the data. Subdue uses a computationally-bounded

inexact graph match that identi(cid:12)es similar, but not identical, instances of a substructure and

(cid:12)nds an approximate measure of closeness of two substructures when under computational

constraints.

In addition to the minimum description length principle, other background

knowledge can be used by Subdue to guide the search towards more appropriate substruc-

tures.

The following sections describe the approach in detail. Section 2 describes the process of

substructure discovery and introduces needed de(cid:12)nitions. Section 3 compares the Subdue

discovery system to other work found in the literature. Section 4 introduces the minimum

description length encoding used by this approach, and Section 5 presents the inexact

graph match algorithm employed by Subdue. Section 6 describes methods of incorporating

background knowledge into the substructure discovery process. The experiments detailed

in Section 7 demonstrate Subdue's ability to (cid:12)nd substructures that compress the data and

to re-discover known concepts in a variety of domains. Section 8 details the hierarchical

discovery process. We conclude with observations and directions for future research.

2. Substructure Discovery

The substructure discovery system represents structured data as a labeled graph. Ob jects

in the data map to vertices or small subgraphs in the graph, and relationships between

ob jects map to directed or undirected edges in the graph. A substructure is a connected

subgraph within the graphical representation. This graphical representation serves as input

to the substructure discovery system. Figure 1 shows a geometric example of such an input

graph. The ob jects in the (cid:12)gure (e.g., T1, S1, R1) become labeled vertices in the graph, and

the relationships (e.g., on(T1,S1), shape(C1,circle)) become labeled edges in the graph.

The graphical representation of the substructure discovered by Subdue from this data is

also shown in Figure 1.

An instance of a substructure in an input graph is a set of vertices and edges from

the input graph that match, graph theoretically, to the graphical representation of the

substructure. For example, the instances of the substructure in Figure 1 are shown in

Figure 2.

The substructure discovery algorithm used by Subdue is a computationally-constrained

beam search. The algorithm begins with the substructure matching a single vertex in the

graph. Each iteration through the algorithm selects the best substructure and expands the

instances of the substructure by one neighboring edge in all possible ways. The new unique

generated substructures become candidates for further expansion. The algorithm searches

232

Substructure Discovery

Input Graph

Substructure

T1

S1

T2

S2

R1

T3

S3

C1

T4

S4

triangle

square

e

e

p

a

h

s

p

a

h

s

on

Figure 1: Example substructure in graph form.

Instance  1       Instance  2       Instance  3       Instance  4

T1

S1

T2

S2

T3

S3

T4

S4

Figure 2: Instances of the substructure.

for the best substructure until all possible substructures have been considered or the total

amount of computation exceeds a given limit. The evaluation of each substructure is guided

by the MDL principle and other background knowledge provided by the user.

Typically, once the description length of an expanding substructure begins to increase,

further expansion of the substructure will not yield a smaller description length. As a

result, Subdue makes use of an optional pruning mechanism that eliminates substructure

expansions from consideration when the description lengths for these expansions increases.

3. Related Work

Several approaches to substructure discovery have been developed. Winston's Arch pro-

gram (Winston, 1975) discovers substructures in order to deepen the hierarchical description

of a scene and to group ob jects into more general concepts. The Arch program searches for

two types of substructure in the blocks-world domain. The (cid:12)rst type involves a sequence

of ob jects connected by a chain of similar relations. The second type involves a set of

ob jects each having a similar relationship to some \grouping"" ob ject. The main di(cid:11)erence

between the substructure discovery procedures used by the Arch program and Subdue is

that the Arch program is designed speci(cid:12)cally for the blocks-world domain. For instance,

the sequence discovery method looks for supported-by and in-front-of relations only.

Subdue's substructure discovery method is domain independent, although the inclusion of

domain-speci(cid:12)c knowledge would improve Subdue's performance.

Motivated by the need to construct a knowledge base of chemical structures, Levinson

(Levinson, 1984) developed a system for storing labeled graphs in which individual graphs

233

Cook & Holder

are represented by the set of vertices in a universal graph. In addition, the individual graphs

are maintained in a partial ordering de(cid:12)ned by the subgraph-of relation, which improves

the performance of graph comparisons. The universal graph representation provides a

method for compressing the set of graphs stored in the knowledge base. Subgraphs of

the universal graph used by several individual graphs suggest common substructure in the

individual graphs. One di(cid:11)erence between the two approaches is that Levinson's system

is designed to incrementally process smaller individual graphs; whereas, Subdue processes

larger graphs all at once. Also, Levinson's system discovers common substructure only

as an indirect result of the universal graph construction; whereas, Subdue's main goal

is to discover and output substructure de(cid:12)nitions that reduce the minimum description

length encoding of the graph. Finally, the subgraph-of partial ordering used by Levinson's

system is not included in Subdue, but maintaining this partial ordering would improve the

performance of the graph matching procedure by pruning the number of possible matching

graphs.

Segen (Segen, 1990) describes a system for storing graphs using a probabilistic graph

model to represent subsets of the graph. Alternative models are evaluated based on a min-

imum description length measure of the information needed to represent the stored graphs

using the model.

In addition, Segen's system clusters the graphs into classes based on

minimizing the description length of the graphs according to the entire clustering. Apart

from the probabilistic representation, Segen's approach is similar to Levinson's system in

that both methods take advantage of commonalities in the graphs to assist in graph stor-

age and matching. The probabilistic graphs contain information for identifying common

substructure in the exact graphs they represent. The portion of the probabilistic graph

with high probability de(cid:12)nes a substructure that appears frequently in the exact graphs.

This notion was not emphasized in Segen's work, but provides an alternative method to

substructure discovery by clustering subgraphs of the original input graphs. As with Levin-

son's approach, graphs are processed incrementally, and substructure is found across several

graphs, not within a single graph as in Subdue.

The Labyrinth system (Thompson & Langley, 1991) extends the Cobweb incremental

conceptual clustering system (Fisher, 1987) to handle structured ob jects. Labyrinth uses

Cobweb to form hierarchical concepts of the individual ob jects in the domain based on

their primitive attributes. Concepts of structured ob jects are formed in a similar manner

using the individual ob jects as attributes. The resulting hierarchy represents a componential

model of the structured ob jects. Because Cobweb's concepts are probabilistic, Labyrinth

produces probabilistic models of the structured ob jects, but with an added hierarchical

organization. The upper-level components of the structured-ob ject hierarchy produced by

Labyrinth represent substructures common to the examples. Therefore, although not the

primary focus, Labyrinth is discovering substructure, but in a more constrained context

than the general graph representation used by Subdue.

Conklin et al. (Conklin & Glasgow, 1992) have developed the i-mem system for con-

structing an image hierarchy, similar to that of Labyrinth, used for discovering common

substructures in a set of images and for e(cid:14)cient retrieval of images similar to a given image.

Images are expressed in terms of a set of relations de(cid:12)ned by the user. Speci(cid:12)c and general

(conceptual) images are stored in the hierarchy based on a subsumption relation similar

234

Substructure Discovery

to Levinson's subgraph-of partial ordering.

Image matching utilizes a transformational

approach (similar to Subdue's inexact graph match) as a measure of image closeness.

As with the approaches of Segen and Levinson, i-mem is designed to process individual

images. Therefore, the general image concepts that appear higher in i-mem's hierarchy

will represent common substructures across several images. Subdue is designed to discover

common substructures within a single image. Subdue can mimic the individual approach

of these systems by processing a set of individual images as one disconnected graph. The

substructures found will be common to the individual images. The hierarchy also represents

a componential view of the images. This same view can be constructed by Subdue using

multiple passes over the graph after replacing portions of the input graph with substructures

discovered during previous passes.

i-mem has performed well in a simple chess domain

and molecular chemistry domains (Conklin & Glasgow, 1992). However, i-mem requires

domain-speci(cid:12)c relations for expressing images in order for the hierarchy to (cid:12)nd relevant

substructures and for image matching to be e(cid:14)cient. Again, maintaining the concepts

(images, graphs) in a partially-ordered hierarchy improves the e(cid:14)ciency of matching and

retrieval, and suggests a possible improvement to Subdue.

The CLiP system (Yoshida, Motoda, & Indurkhya, 1993) for graph-based induction is

more similar to Subdue than the previous systems. CLiP iteratively discovers patterns in

graphs by expanding and combining patterns discovered in previous iterations. Patterns

are grouped into views based on their collective ability to compress the original

input

graph. During each iteration CLiP uses existing views to contract the input graph and

then considers adding to the views new patterns consisting of two vertices and an edge from

the contracted graph. The compression of the new proposed views is estimated, and the

best views (according to a given beam width) are retained for the next iteration.

CLiP discovers substructures (patterns) di(cid:11)erently than Subdue. First, CLiP produces

a set of substructures that collectively compress the input graph; whereas, Subdue produces

only single substructures evaluated using the more principled minimum description length.

CLiP has the ability to grow substructures agglomeratively (i.e., merging two substructures

together); whereas, Subdue always produces new substructures using incremental growth

along one new edge. CLiP initially estimates the compression value of new views based on

the compression value of the parent view; whereas, Subdue performs an expensive exact

measurement of compression for each new substructure. Finally, CLiP employs an e(cid:14)cient

graph match based on graph identity, not graph isomorphism as in Subdue. Graph identity

assumes an ordering over the incident edges of a vertex and does not consider all possible

mappings when looking for occurrences of a pattern in an input graph. These di(cid:11)erences

in CLiP suggest possible enhancements to Subdue.

Research in pattern recognition has begun to investigate the use of graphs and graph

grammars as an underlying representation for structural problems (Schalko(cid:11), 1992). Many

results in grammatical inference are applicable to constrained classes of graphs (e.g., trees)

(Fu, 1982; Miclet, 1986). The approach begins with a set of sample graphs and produces a

generalized graph grammar capable of deriving the original sample graphs and many others.

The production rules of this general grammar capture regularities (substructures) in the

sample graphs. Jeltsch and Kreowski (Jeltsch & Kreowski, 1991) describe an approach that

begins with a maximally-speci(cid:12)c grammar and iteratively identi(cid:12)es common subgraphs in

the right-hand sides of the production rules. These common subgraphs are used to form

235

Cook & Holder

new, more general production rules. Although their method does not address the underlying

combinatorial nondeterminism, heuristic approaches could provide a feasible method for

extracting substructures in the form of graph grammars. Furthermore, the graph grammar

production-rule may provide a suitable representation for background knowledge during the

substructure discovery process.

4. Minimum Description Length Encoding of Graphs

The minimum description length principle (MDLP) introduced by Rissanen (Rissanen,

1989) states that the best theory to describe a set of data is that theory which minimizes

the description length of the entire data set. The MDL principle has been used for decision

tree induction (Quinlan & Rivest, 1989), image processing (Pednault, 1989; Pentland, 1989;

Leclerc, 1989), concept learning from relational data (Derthick, 1991), and learning models

of non-homogeneous engineering domains (Rao & Lu, 1992).

We demonstrate how the minimum description length principle can be used to discover

substructures in complex data. In particular, a substructure is evaluated based on how well

it can compress the entire dataset using the minimum description length. We de(cid:12)ne the

minimum description length of a graph to be the number of bits necessary to completely

describe the graph.

According to the minimum description length (MDL) principle, the theory that best

accounts for a collection of data is the one that minimizes I (S ) + I (GjS ), where S is the

discovered substructure, G is the input graph, I (S ) is the number of bits required to encode

the discovered substructure, and I (GjS ) is the number of bits required to encode the input

graph G with respect to S .

The graph connectivity can be represented by an adjacency matrix. Consider a graph

that has n vertices, which are numbered 0; 1; : : : ; n (cid:0) 1. An n (cid:2) n adjacency matrix A can

be formed with entry A[i; j ] set to 0 or 1. If A[i; j ] = 0, then there is no connection from

vertex i to vertex j . If A[i; j ] = 1, then there is at least one connection from vertex i to

vertex j . Undirected edges are recorded in only one entry of the matrix. The adjacency

matrix for the graph in Figure 3 is shown below.

2

3

x

0 1 1 0 0 0

triang le

0 0 0 0 0 0

6

7

6

7

6

7

y

0 0 0 1 1 0

6

7

6

7

square

0 0 0 0 0 0

6

7

6

7

r

0 0 0 0 0 1

4

5

rectang le

0 0 0 0 0 0

The encoding of the graph consists of the following steps. We assume that the decoder

has a table of the l

unique labels in the original graph G.

u

1. Determine the number of bits vbits needed to encode the vertex labels of the graph.

First, we need (lg v ) bits to encode the number of vertices v in the graph. Then,

encoding the labels of all v vertices requires (v lg l

) bits. We assume the vertices are

u

speci(cid:12)ed in the same order they appear in the adjacency matrix. The total number

of bits to encode the vertex labels is

v bits = lg v + v lg l

u

236

Substructure Discovery

triangle

square

rectangle

e

e

e

p

p

p

a

a

a

h

h

h

s

s

s

x

on

y

on

r

Figure 3: MDL example graph.

For the example in Figure 3, v = 6, and we assume that there are l

= 8 unique

u

labels in the original graph. The number of bits needed to encode these vertices is

lg 6 + 6 lg 8 = 20:58 bits.

2. Determine the number of bits rbits needed to encode the rows of the adjacency matrix

A. Typically, in large graphs, a single vertex has edges to only a small percentage of

the vertices in the entire graph. Therefore, a typical row in the adjacency matrix will

have much fewer than v 1s, where v is the total number of vertices in the graph. We

apply a variant of the coding scheme used by (Quinlan & Rivest, 1989) to encode bit

strings with length n consisting of k 1s and (n (cid:0) k) 0s, where k (cid:28) (n (cid:0) k). In our

case, row i (1 (cid:20) i (cid:20) v ) can be represented as a bit string of length v containing k

i

1s. If we let b = max

k

, then the i

row of the adjacency matrix can be encoded as

i

i

th

follows.

(a) Encoding the value of k

requires lg(b + 1) bits.

i

(b) Given that only k

1s occur in the row bit string of length v , only

strings

i

k

i

(cid:16)

(cid:17)

v

of 0s and 1s are possible. Since all of these strings have equal probability of

occurrence, lg

bits are needed to encode the positions of 1s in row i. The

k

i

(cid:16)

(cid:17)

v

value of v is known from the vertex encoding.

Finally, we need an additional lg(b + 1) bits to encode the number of bits needed to

specify the value of k

for each row. The total encoding length in bits for the adjacency

i

matrix is

rbits = lg(b + 1) +

lg(b + 1) + lg

v

k

i

v

X

(cid:16)

(cid:17)

i=1

v

X

(cid:16)

(cid:17)

= (v + 1) lg(b + 1)

lg

v

k

i

i=1

237

Cook & Holder

For the example in Figure 3, b = 2, and the number of bits needed to encode the

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

(cid:16)

(cid:17)

adjacency matrix is (7 lg 3)+ lg

+ lg

+ lg

+ lg

+ lg

+ lg

= 21:49

2

0

2

0

1

0

6

6

6

6

6

6

bits.

3. Determine the number of bits ebits needed to encode the edges represented by the

entries A[i; j ] = 1 of the adjacency matrix A. The number of bits needed to encode

entry A[i; j ] is (lg m) + e(i; j )[1 + lg l

], where e(i; j ) is the actual number of edges

u

between vertex i and j in the graph and m = max

e(i; j ). The (lg m) bits are needed

i;j

to encode the number of edges between vertex i and j , and [1 + lg l

] bits are needed

u

per edge to encode the edge label and whether the edge is directed or undirected. In

addition to encoding the edges, we need to encode the number of bits (lg m) needed

to specify the number of edges per entry. The total encoding of the edges is

ebits = lg m +

lg m + e(i; j )[1 + lg l

]

u

v

v

X

X

i=1

j=1

v

v

X

X

= lg m + e(1 + lg l

) +

A[i; j ] lg m

u

i=1

j=1

= e(1 + lg l

) + (K + 1) lg m

u

where e is the number of edges in the graph, and K is the number of 1s in the adjacency

matrix A. For the example in Figure 3, e = 5, K = 5, m = 1, l

= 8, and the number

u

of bits needed to encode the edges is 5(1 + lg 8) + 6 lg 1 = 20.

The total encoding of the graph takes (v bits + rbits + ebits) bits. For the example in

Figure 3, this value is 62:07 bits.

Both the input graph and discovered substructure can be encoded using the above

scheme. After a substructure is discovered, each instance of the substructure in the input

graph is replaced by a single vertex representing the entire substructure. The discovered

substructure is represented in I (S ) bits, and the graph after the substructure replacement is

represented in I (GjS ) bits. Subdue searches for the substructure S in graph G minimizing

I (S ) + I (GjS ).

5. Inexact Graph Match

Although exact structure match can be used to (cid:12)nd many interesting substructures, many

of the most interesting substructures show up in a slightly di(cid:11)erent form throughout the

data. These di(cid:11)erences may be due to noise and distortion, or may just illustrate slight

di(cid:11)erences between instances of the same general class of structures. Consider the image

shown in Figure 9. The pencil and the cube would make ideal substructures in the picture,

but an exact match algorithm may not consider these as strong substructures, because they

rarely occur in the same form and level of detail throughout the picture.

Given an input graph and a set of de(cid:12)ned substructures, we want to (cid:12)nd those subgraphs

of the input graph that most closely resemble the given substructures. Furthermore, we want

to associate a distance measure between a pair of graphs consisting of a given substructure

and a subgraph of the input graph. We adopt the approach to inexact graph match given

by Bunke and Allermann (Bunke & Allermann, 1983).

238

Substructure Discovery

g1

a

b

A

1

B

2

B

3

a

g2

b

a

5

B

A

4

b

Figure 4: Two similar graphs g

and g

.

1

2

In this inexact match approach, each distortion of a graph is assigned a cost. A distortion

is described in terms of basic transformations such as deletion, insertion, and substitution

of vertices and edges. The distortion costs can be determined by the user to bias the match

for or against particular types of distortions.

An inexact graph match between two graphs g

and g

maps g

to g

such that g

is

1

2

1

2

2

interpreted as a distorted version of g

. Formally, an inexact graph match is a mapping

1

f : N

! N

[ f(cid:21)g, where N

and N

are the sets of vertices of g

and g

, respectively. A

1

2

1

2

1

2

vertex v 2 N

that is mapped to (cid:21) (i.e., f (v ) = (cid:21)) is deleted. That is, it has no corresponding

1

vertex in g

. Given a set of particular distortion costs as discussed above, we de(cid:12)ne the cost

2

of an inexact graph match cost(f ), as the sum of the cost of the individual transformations

resulting from f , and we de(cid:12)ne matchcost(g

; g

) as the value of the least-cost function that

1

2

maps graph g

onto graph g

.

1

2

Given g

, g

, and a set of distortion costs, the actual computation of matchcost(g

; g

)

1

2

1

2

can be determined using a tree search procedure. A state in the search tree corresponds to

a partial match that maps a subset of the vertices of g

to a subset of the vertices in g

.

1

2

Initially, we start with an empty mapping at the root of the search tree. Expanding a state

corresponds to adding a pair of vertices, one from g

and one from g

, to the partial mapping

1

2

constructed so far. A (cid:12)nal state in the search tree is a match that maps all vertices of g

to

1

g

or to (cid:21). The complete search tree of the example in Figure 4 is shown in Figure 5. For

2

this example we assign a value of 1 to each distortion cost. The numbers in circles in this

(cid:12)gure represent the cost of a state. As we are eventually interested in the mapping with

minimum cost, each state in the search tree gets assigned the cost of the partial mapping

that it represents. Thus the goal state to be found by our tree search procedure is the

(cid:12)nal state with minimum cost among all (cid:12)nal states. From Figure 5 we conclude that the

minimum cost inexact graph match of g

and g

is given by the mapping f (1) = 4, f (2) = 3.

1

2

The cost of this mapping is 4.

Given graphs g

with n vertices and g

with m vertices, m (cid:21) n, the complexity of the

1

2

full inexact graph match is O(n

). Because this routine is used heavily throughout the

m+1

239

Cook & Holder

(1, 3)

1

(1, 4)

0

(1, 5)

1

(1,  )

1

(2,4) (2,5) (2, ) (2,3) (2,5) (2, ) (2,3) (2,4) (2, ) (2,3) (2,4) (2,5) (2, )

7

6

10

3

6

9

7

7

10

9

10

9

11

Figure 5: Search tree for computing matchcost(g

,g

) from Figure 4.

1

2

discovery and evaluation process, the complexity of the algorithm can signi(cid:12)cantly degrade

the performance of the system.

To improve the performance of the inexact graph match algorithm, we extend Bunke's

approach by applying a branch-and-bound search to the tree. The cost from the root of the

tree to a given node is computed as described above. Nodes are considered for pairings in

order from the most heavily connected vertex to the least connected, as this constrains the

remaining match. Because branch-and-bound search guarantees an optimal solution, the

search ends as soon as the (cid:12)rst complete mapping is found.

In addition, the user can place a limit on the number of search nodes considered by

the branch-and-bound procedure (de(cid:12)ned as a function of the size of the input graphs).

Once the number of nodes expanded in the search tree reaches the de(cid:12)ned limit, the search

resorts to hill climbing using the cost of the mapping so far as the measure for choosing the

best node at a given level. By de(cid:12)ning such a limit, signi(cid:12)cant speedup can be realized at

the expense of accuracy for the computed match cost.

Another approach to inexact graph match would be to encode the di(cid:11)erence between

two graphs using the MDL principle. Smaller encodings would indicate a lower match cost

between the two graphs. We leave this as a future research direction.

6. Guiding the Discovery Process with Background Knowledge

Although the principle of minimum description length is useful for discovering substruc-

tures that maximize compression of the data, scientists may realize more bene(cid:12)t from the

discovery of substructures that exhibit other domain-speci(cid:12)c and domain-independent char-

acteristics.

To make Subdue more powerful across a wide variety of domains, we have added the

ability to guide the discovery process with background knowledge. Although the minimum

description length principle still drives the discovery process, the background knowledge can

be used to input a bias toward certain types of substructures. This background knowledge

is encoded in the form of rules for evaluating substructures, and can represent domain-

independent or domain-dependent rules. Each time a substructure is evaluated, these input

240

Substructure Discovery

rules are used to determine the value of the substructure under consideration. Because

only the most-favored substructures are kept and expanded, these rules bias the discovery

process of the system.

Each background rule can be assigned a positive, zero, or negative weight, that biases

the procedure toward a type of substructure, eliminates the use of the rule, or biases the

procedure away from a type of substructure, respectively. The value of a substructure is

de(cid:12)ned as the description length (DL) of the input graph using the substructure multi-

plied by the weighted value of each background rule from a set of rules R applied to the

substructure.

value(s) = DL(G; s) (cid:2)

rule

(s)

(1)

r

r=1

jRj

Y

e

r

Three domain-independent heuristics that have been incorporated as rules into the Sub-

due system are compactness, connectivity, and coverage. For the de(cid:12)nitions of these rules,

we will let G represent the input graph, s represent a substructure in the graph, and I

represent the set of instances of the substructure s in G. The instance weight w of an

instance i 2 I of a substructure s is de(cid:12)ned to be

w(i; s) = 1 (cid:0)

;

(2)

matchcost(i; s)

size(i)

where size(i) = #vertices(i) + #edges(i). If the match cost is greater than the size of the

larger graph, then w(i; s) = 0. The instance weights are used in these rules to compute a

weighted average over instances of a substructure. A value of 1 is added to each formula so

that the exponential weights can be used to control the rule's signi(cid:12)cance.

The (cid:12)rst rule, compactness, is a generalization of Wertheimer's Factor of Closure, which

states that human attention is drawn to closed structures (Wertheimer, 1939). A closed

substructure has at least as many edges as vertices, whereas a non-closed substructure

has fewer edges than vertices (Prather, 1976). Thus, closed substructures have a higher

compactness value. Compactness is de(cid:12)ned as the weighted average of the ratio of the

number of edges in the substructure to the number of vertices in the substructure.

compactness(s) = 1 +

w(i; s) (cid:2)

(3)

X

1

#edges(i)

jI j

#vertices(i)

i2I

The second rule, connectivity, measures the amount of external connection in the in-

stances of the substructure. The connectivity rule is a variant of Wertheimer's Factor

of Proximity (Wertheimer, 1939), and is related to earlier numerical clustering techniques

(Zahn, 1971). These works demonstrate the human preference for \isolated"" substructures,

that is, substructures that are minimally related to adjoining structure. Connectivity mea-

sures the \isolation"" of a substructure by computing the inverse of the average number of

external connections over all the weighted instances of the substructure in the input graph.

An external connection is de(cid:12)ned here as an edge that connects a vertex in the substructure

to a vertex outside the substructure. The formula for determining the connectivity of a

substructure s with instances I in the input graph G is given below.

241

Cook & Holder

connectiv ity (s) = 1 +

w(i; s) (cid:2) num external conns(i)

(4)

""

#

(cid:0)1

X

1

jI j

i2I

The third rule, coverage, measures the fraction of structure in the input graph described

by the substructure. The coverage rule is motivated from research in inductive learning and

provides that concept descriptions describing more input examples are considered better

(Michalski & Stepp, 1983). Although MDL measures the amount of structure, the coverage

rule includes the relevance of this savings with respect to the size of the entire input graph.

Coverage is de(cid:12)ned as the number of unique vertices and edges in the instances of the

substructure divided by the total number of vertices and edges in the input graph. In this

formula, the unique structure(i) of an instance i is the number of vertices and edges in i

that have not already appeared in previous instances in the summation.

coverage(s) = 1 +

(5)

size(G)

P

i2I

w(i; s) (cid:2) unique structure(i)

Domain-dependent rules can also be used to guide the discovery process in a domain

where scientists can contribute their expertise. For example, CAD circuits generally consist

of two types of components, active and passive components. The active components are

the main driving components. Identifying the active components is the (cid:12)rst step in under-

standing the main function of the circuit. To add this knowledge to Subdue we include

a rule that assigns higher values to substructures (circuit components) representing active

components and lower values to substructures representing passive components. Since the

active components have higher scores, they are expected to be selected. The system can

then focus the attention on the active components which will be expanded to the functional

substructures.

Another method of biasing the discovery process with background knowledge is to let

background rules a(cid:11)ect the prior probabilities of possible substructures. However, choosing

the appropriate prior probabilities to express desired properties of substructures is di(cid:14)-

cult, but indicates a future direction for the inclusion of background knowledge into the

substructure discovery process.

7. Experiments

The experiments in this section evaluate Subdue's substructure discovery capability in

several domains, including chemical compound analysis, scene analysis, CAD circuit design

analysis, and analysis of an arti(cid:12)cially-generated structural database.

Two goals of our substructure discovery system are to (cid:12)nd substructures that can reduce

the amount of information needed to describe the data, and to (cid:12)nd substructures that are

considered interesting for the given database. As a result, we evaluate the Subdue system

in this section along these two criteria. First, we measure the amount of compression that

Subdue provides across a variety of databases. Second, we use the Subdue system with the

additional background knowledge rules to re-discover substructures that have been identi(cid:12)ed

as interesting by experts in each speci(cid:12)c domain. Section 7.1 describes the domains used

in these experiments, and Section 7.2 presents the experimental results.

242

Substructure Discovery

O

CH OH
2

CH

3

CH

3

C   O

O

OH

Figure 6: Cortisone.

CH

3

CH

2

H

C

C

CH

3

H

C

C

CH3

C

C

CH

2

CH

2

CH

2

CH

2

CH

2

CH

2

CH

2

CH

2

H

CH

2

C

C

CH3

H

C

C

CH

3

H

Figure 7: Natural rubber (all-cis polyisoprene).

7.1 Domains

7.1.1 Chemical Compound Analysis

Chemical compounds are rich in structure. Identi(cid:12)cation of the common and interesting

substructures can bene(cid:12)t scientists by identifying recurring components, simplying the data

description, and focusing on substructures that stand out and merit additional attention.

Chemical compounds are represented graphically by mapping individual atoms, such as

carbon and oxygen, to labeled vertices in the graph, and by mapping bonds between the

atoms onto labeled edges in the graph. Figures 6, 7, and 8 show the graphs representing

the chemical compound databases for cortisone, rubber, and a portion of a DNA molecule.

7.1.2 Scene Analysis

Images and scene descriptions provide a rich source of structure.

Images that humans

encounter, both natural and synthesized, have many structured subcomponents that draw

our attention and that help us to interpret the data or the scene.

Discovering common structures in scenes can be useful to a computer vision system.

First, automatic substructure discovery can help a system interpret an image. Instead of

working from low-level vertices and edges, Subdue can provide more abstract structured

components, resulting in a hierarchical view of the image that the machine can analyze at

many levels of detail and focus, depending on the goal of the analysis. Second, substructure

discovery that makes use of an inexact graph match can help identify ob jects in a 2D image

of a 3D scene where noise and orientation di(cid:11)erences are likely to exist. If an ob ject ap-

pears often in the scene, the inexact graph match driving the Subdue system may capture

slightly di(cid:11)erent views of the same ob ject. Although an ob ject may be di(cid:14)cult to identify

243

Cook & Holder

O

O

N

H

N

N

O

H

O

CH3

CH2

O

N

H

O

CH

2

O

adenine

O

O

P

OH

N

N

N

guanine

thymine

O

CH

2

O

O

NN

N

O

P

OH

N

N

N

H

H

O

H

O

N

N

H

O

CH

2

O

O

N

N

H

O

O

P

OH

O

CH3

O

H N

H

thymine

cytosine

HO

P O

O

N

O

CH3

CH2

O

HO

P O

O

N

N

O

N

N

CH2

O

adenine

HO

P O

O

Figure 8: Portion of a DNA molecule.

Figure 9: Scene analysis example.

244

Substructure Discovery

f

k

a

x

l

t

p

m

Figure 10: Possible vertices and labels.

l

l

l

l

l

l

l

l

l

l

l

t

l

t

t

l

l

t

t

l

l

t

t

a

l

m

l

t

t

l

a

l

l

f

a

a

l

Figure 11: Portion of graph representing scene in Figure 4.

from just one 2D picture, Subdue will match instances of similar ob jects, and the di(cid:11)er-

ences between these instances can provide additional information for identi(cid:12)cation. Third,

substructure discovery can be used to compress the image. Replacing common interesting

substructures by a single vertex simpli(cid:12)es the image description and reduces the amount of

storage necessary to represent the image.

To apply Subdue to image data, we extract edge information from the image and

construct a graph representing the scene. The graph representation consists of eight types

of vertices and two types of arcs (edge and space). The vertex labels (f , a, l, t, k, x, p, and

m) follow the Waltz labelings (Waltz, 1975) of junctions of edges in the image and represent

the types of vertices shown in Figure 10. An edge arc represents the edge of an ob ject in the

image, and a space arc links non-connecting ob jects together. The edge arcs represent an

edge in the scene that connects two vertices, and the space arcs connect the closest vertices

from two disjoint neighboring ob jects. Distance, curve, and angle information has not been

included in the graph representation, but can be added to give additional information about

the scene. Figure 11 shows the graph representation of a portion of the scene depicted in

Figure 9. In this (cid:12)gure, the edge arcs are solid and the space arcs are dashed.

245

Cook & Holder

VCC

drain

source

drain

GND

gate

gate

ext_pin

drain

gate

n_mosfet

source

connect

drain

connect

gate

n_mosfet

source

ext_pin

Figure 12: Ampli(cid:12)er circuit and graph representation.

7.1.3 CAD Circuit Analysis

In this domain, we employ Subdue to (cid:12)nd circuit components in CAD circuit data. Discov-

ery of substructures in circuit data can be a valuable tool to an engineer who is attempting to

identify common reusable parts in a circuit layout. Replacing individual components in the

circuit description by larger substructure descriptions will also simplify the representation

of the circuit.

The data for the circuit domain was obtained from National Semiconductor, and con-

sists of a set of components making up a circuit as output by the Cadence Design System.

The particular circuit used for this experiment is a portion of an analog-to-digital con-

verter. Figure 12 presents a circuit for an ampli(cid:12)er and gives the corresponding graph

representation.

7.1.4 Artificial Domain

In the (cid:12)nal domain, we arti(cid:12)cially generate graphs to evaluate Subdue's ability to discover

substructures capable of compressing the graph. Four substructures are created of varying

sizes with randomly-selected vertices and edges (see Figure 13). The name of a substructure

re(cid:13)ects the number of vertices and edges in its graph representation. Next, these substruc-

tures are embedded in larger graphs whose size is 15 times the size of the substructure.

The graphs vary across four parameters: number of possible vertex and edge labels (one

times and two times the number of labels used in the substructure), connectivity of the

substructure (1 or 2 external connections), coverage of the instances (60% and 80%), and

246

Substructure Discovery

e3

n1

n4

e2

n3

e1

n2

e1

n3

n4

e3

n2

e2

e3

n1

e2

e3

n7

e6

n5

e3

n2

e1

e4

n5

e2

n3

n1

e6

n6

e3

e5

n3

n1

n7

n4

e3

e1

e7

e9

e8

e8

n2

n3

e6

n1

Figure 13: Four arti(cid:12)cial substructures used to evaluate Subdue.

the amount of distortion in the instances (0, 1 or 2 distortions). This yields a total of 96

graphs (24 for each di(cid:11)erent substructure).

7.2 Experimental Results

7.2.1 Experiment 1: Data compression

In the (cid:12)rst experiment, we test Subdue's ability to compress a structural database. Using

a beam width of 4 and Subdue's pruning mechanism, we applied the discovery algorithm to

each of the databases mentioned above. We repeat the experiment with match thresholds

ranging from 0.0 to 1.0 in increments of 0.1. Table 1 shows the description length (DL) of the

original graph, the description length of the best substructure discovered by Subdue, and

the value of compression. Compression here is de(cid:12)ned as

. Figure 14,

DL of original graph

DL of compressed graph

shows the actual discovered substructures for the (cid:12)rst four datasets.

As can be seen from Table 1, Subdue was able to reduce the database to slightly

larger than

of its original size in the best case. The average compression value over

4

1

all of these domains (treating the arti(cid:12)cial graphs as one value) is 0.62. The results of

this experiment demonstrate that the substructure discovered by Subdue can signi(cid:12)cantly

reduce the amount of data needed to represent an input graph. We expect that compressing

the graph using combinations of substructures and hierarchies of substructures will realize

even greater compression in some databases.

247

Cook & Holder

Database

DL

Threshold

DL

Compression

original

optimal

compressed

Rubber

371.78

0.1

95.20

0.26

Cortisone

355.03

0.3

173.25

0.49

DNA

2427.93

1.0

2211.87

0.91

Pencils

1592.33

1.0

769.18

0.48

CAD { M1

4095.73

0.7

2148.8

0.52

CAD { S1SegDec

1860.14

0.7

1149.29

0.62

CAD { S1DrvBlk

12715.12

0.7

9070.21

0.71

CAD { BlankSub

8606.69

0.7

6204.74

0.72

CAD { And2

427.73

0.1

324.52

0.76

Arti(cid:12)cial (avg. over 96 graphs)

1636.25

0.0: : :1.0

1164.02

0.71

Table 1: Graph compression results.

CH

3

CH

2

C

C

H

CH
2

O

C

C

C

O

CH2
C

C

C

O

a

l

a

(a)

(b)

(c)

(d)

Figure 14: Best substructure for (a) rubber database, (b) cortisone database, (c) DNA

database, and (d) image database.

248

Substructure Discovery

Figure 15: Benzene ring discovered by Subdue.

7.2.2 Experiment 2: Re-discovery of known substructures using background

knowledge

Another way of evaluating the discovery process is to evaluate the interestingness of the

discovered substructures. The determination of this value will change from domain to

domain. As a result, in this second set of experiments we test Subdue's ability to discover

substructures that have already been labeled as important by experts in the domains under

consideration.

In the chemical compound domain, chemists frequently describe compounds in terms of

the building-block components that are heavily used. For example, in the rubber compound

database shown in Figure 7, the compound is made up of a chain of structures that are

labeled by chemists as isoprene units. Subdue's ability to re-discover this structure is

exempli(cid:12)ed in Figure 14a. This substructure, which was discovered using the MDL principle

with no extra background knowledge, represents an isoprene unit.

Although Subdue was able to re-discover isoprene units without extra background

knowledge, the substructure a(cid:11)ording the most compression will not always be the most in-

teresting or important substructure in the database. For example, in the cortisone database

the benzene ring which consists of a ring of carbons is not discovered using only the MDL

principle. However, the additional background rules can be used to increase the chance of

(cid:12)nding interesting substructures in these domains. In the case of the cortisone compound,

we know that the interesting structures exhibit a characteristic of closure. Therefore, we

give a strong weight (8.0) to the compactness background rule and use a match threshold of

0.2 to allow for deviations in the benzene ring instances. In the resulting output, Subdue

(cid:12)nds the benzene ring shown in Figure 15.

In the same way, we can use the background rules to (cid:12)nd the pencil substructure in

the image data. When the image in Figure 9 is viewed, the substructure of interest is the

pencil in its various forms. However, the substructure that a(cid:11)orded the most compression

does not make up an entire pencil. We know that the pencils have a high degree of closure

and of coverage, so the weights for these rules are set to 1.0. With these weights, Subdue

is able to (cid:12)nd the pencil substructure shown in Figure 16 for all tested match thresholds

between 0.0 and 1.0.

8. Hierarchical Concept Discovery

After a substructure is discovered, each instance of the substructure in the input graph can

be replaced by a single vertex representing the entire substructure. The discovery procedure

can then be repeated on the compressed data set, resulting in new interesting substructures.

If the newly-discovered substructures are de(cid:12)ned in terms of existing substructure concepts,

the substructure de(cid:12)nitions form a hierarchy of substructure concepts.

249

Cook & Holder

l

l

a

a

l

Figure 16: Pencil substructure discovered by Subdue.

Hierarchical concept discovery also adds the capability to improve Subdue's perfor-

mance. When Subdue is applied to a large input graph, the complexity of the algorithm

prevents consideration of larger substructures. Using hierarchical concept discovery, Sub-

due can (cid:12)rst discover those smaller substructures which best compress the data. Applying

the compression reduces the graph to a more manageable size, increasing the chance that

Subdue will (cid:12)nd the larger substructures on the subsequent passes through the database.

Once Subdue selects a substructure, all vertices that comprise the exact instances of

the substructure are replaced in the graph by a single vertex representing the discovered

substructure. Edges connecting vertices outside the instance to vertices inside the instance

now connect to the new vertex. Edges internal to the instance are removed. The discovery

process is then applied to the compressed data. If a hierarchical description of concepts is

particularly desired, heavier weight can be given to substructures which utilize previously

discovered substructures. The increased weight re(cid:13)ects increased attention to this substruc-

ture. Figure 17 illustrates the compressed rubber compound graph using the substructure

shown in Figure 14a.

To demonstrate the ability of Subdue to (cid:12)nd a hierarchy of substructures, we let the sys-

tem make multiple passes through a database that represents a portion of a DNA molecule.

Figure 8 shows a portion of two chains of a double helix, using three pairs of bases which

are held together by hydrogen bonds. Figure 18 shows the substructures found by Subdue

after each of three passes through the data. Note that, on the third pass, Subdue linked

together the instances of the substructure in the second pass to (cid:12)nd the chains of the double

helix.

Although replacing portions of the input graph with the discovered substructures com-

presses the data and provides a basis for discovering hierarchical concepts in the data, the

substructure replacement procedure becomes more complicated when concepts with inexact

instances are discovered. When inexact instances of a discovered concept are replaced by

a single vertex in the data, all distortions of the graph (di(cid:11)erences between the instance

graph and the substructure de(cid:12)nition) must be attached as annotations to the vertex label.

250

Substructure Discovery

Highest-valued substructure

S  =
1

CH

3

CH

2

C

C

H

CH

2

Compressed graph using discovered substructure

G  =

S
1

S
1

S

1

S
1

S

1

CH

3

H

=

C

C

CH

3

H

C

C

CH3

C

C

CH
2

CH

2

CH
2

CH

2

CH

2

CH
2

CH
2

CH
2

CH
2

H

CH

2

C

C

CH3

H

C

C

CH3

H

Figure 17: Compressed graph for rubber compound data.

251

Cook & Holder

Highest-valued substructure
after First Pass

Highest-valued substructure
after Second Pass

S

1

C

S  =2S  =2

O

P

=

S  =
1

CH

2

O

O

O

CH

2

O

O

P

Highest-valued substructure
after Third Pass

S  =3

O

O

O

S

2

OH

=

OH

OH

S

2

S

2

O

O

CH

2

O

O

O

P

OH

O

CH

2

O

O

O

P

OH

O

CH

2

O

O

O

P

OH

O

Figure 18: Hierarchical discovery in DNA data.

252

Substructure Discovery

9. Conclusions

Extracting knowledge from structural databases requires the identi(cid:12)cation of repetitive sub-

structures in the data. Substructure discovery identi(cid:12)es interesting and repetitive structure

in structural data. The substructures represent concepts found in the data and a means of

reducing the complexity of the representation by abstracting over instances of the substruc-

ture. We have shown how the minimum description length (MDL) principle can be used to

perform substructure discovery in a variety of domains. The substructure discovery process

can also be guided by background knowledge. The use of an inexact graph match allows

deviation in the instances of a substructure. Once a substructure is discovered, instances

of the substructure can be replaced by the concept de(cid:12)nition, a(cid:11)ording compression of the

data description and providing a basis for discovering hierarchically-de(cid:12)ned structures.

Future work will combine structural discovery with discovery of concepts using a linear-

based representation such as AutoClass (Cheeseman, Kelly, Self, Stutz, Taylor, & Freeman,

1988).

In particular, we will use Subdue to compress the data fed to AutoClass, and

let Subdue evaluate the interesting structures in the classes generated by AutoClass. In

addition, we will be developing a parallel implementation of the AutoClass / Subdue

system that will enable application of substructure discovery to larger structural databases.

Acknowledgements

This pro ject is supported by NASA grant NAS5-32337. The authors would like to thank

Mike Shay at National Semiconductor for providing the circuit data. We would also like

to thank Surnjani Djoko and Tom Lai for their help with this pro ject. Thanks also to the

reviewers for their numerous insightful comments.

References

Bunke, H., & Allermann, G. (1983). Inexact graph matching for structural pattern recog-

nition. Pattern Recognition Letters, 1 (4), 245{253.

Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., & Freeman, D. (1988). Autoclass:

A bayesian classi(cid:12)cation system. In Proceedings of the Fifth International Workshop

on Machine Learning, pp. 54{64.

Conklin, D., & Glasgow, J. (1992). Spatial analogy and subsumption. In Proceedings of the

Ninth International Machine Learning Workshop, pp. 111{116.

Derthick, M. (1991). A minimal encoding approach to feature discovery. In Proceedings of

the Ninth National Conference on Arti(cid:12)cial Intel ligence, pp. 565{571.

Fisher, D. H. (1987). Knowledge acquisition via incremental conceptual clustering. Machine

Learning, 2 (2), 139{172.

Fu, K. S. (1982). Syntactic Pattern Recognition and Applications. Prentice-Hall.

Holder, L. B., Cook, D. J., & Bunke, H. (1992). Fuzzy substructure discovery. In Proceedings

of the Ninth International Machine Learning Conference, pp. 218{223.

253

Cook & Holder

Holder, L. B., & Cook, D. J. (1993). Discovery of inexact concepts from structural data.

IEEE Transactions on Know ledge and Data Engineering, 5 (6), 992{994.

Jeltsch, E., & Kreowski, H. J. (1991). Grammatical inference based on hyperedge replace-

ment. In Fourth International Workshop on Graph Grammars and Their Application

to Computer Science, pp. 461{474.

Leclerc, Y. G. (1989). Constructing simple stable descriptions for image partitioning. In-

ternational journal of Computer Vision, 3 (1), 73{102.

Levinson, R. (1984). A self-organizing retrieval system for graphs. In Proceedings of the

Second National Conference on Arti(cid:12)cial Intel ligence, pp. 203{206.

Michalski, R. S., & Stepp, R. E. (1983). Learning from observation: Conceptual clustering.

In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), Machine Learning:

An Arti(cid:12)cial Intel ligence Approach, Vol. I, pp. 331{363. Tioga Publishing Company.

Miclet, L. (1986). Structural Methods in Pattern Recognition. Chapman and Hall.

Pednault, E. P. D. (1989). Some experiments in applying inductive inference principles

to surfa ce reconstruction. In Proceedings of the International Joint Conference on

Arti(cid:12)cial Intel ligence, pp. 1603{1609.

Pentland, A. (1989). Part segmentation for ob ject recognition. Neural Computation, 1,

82{91.

Prather, R. (1976). Discrete Mathemetical Structures for Computer Science. Houghton

Mi(cid:14)n Company.

Quinlan, J. R., & Rivest, R. L. (1989). Inferring decision trees using the minimum descrip-

tion length principle. Information and Computation, 80, 227{248.

Rao, R. B., & Lu, S. C. (1992). Learning engineering models with the minimum descrip-

tion length principle. In Proceedings of the Tenth National Conference on Arti(cid:12)cial

Intel ligence, pp. 717{722.

Rissanen, J. (1989). Stochastic Complexity in Statistical Inquiry. World Scienti(cid:12)c Publishing

Company.

Schalko(cid:11), R. J. (1992). Pattern Recognition: Statistical, Structural and Neural Approaches.

John Wiley & Sons.

Segen, J. (1990). Graph clustering and model learning by data compression. In Proceedings

of the Seventh International Machine Learning Workshop, pp. 93{101.

Thompson, K., & Langley, P. (1991). Concept formation in structured domains. In Fisher,

D. H., & Pazzani, M. (Eds.), Concept Formation: Know ledge and Experience in Un-

supervised Learning, chap. 5. Morgan Kaufmann Publishers, Inc.

Waltz, D. (1975). Understanding line drawings of scenes with shadows. In Winston, P. H.

(Ed.), The Psychology of Computer Vision. McGraw-Hill.

254

Substructure Discovery

Wertheimer, M. (1939). Laws of organization in perceptual forms. In Ellis, W. D. (Ed.), A

Sourcebook of Gestalt Psychology, pp. 331{363. Harcourt, Brace and Company.

Winston, P. H. (1975). Learning structural descriptions from examples. In Winston, P. H.

(Ed.), The Psychology of Computer Vision, pp. 157{210. McGraw-Hill.

Yoshida, K., Motoda, H., & Indurkhya, N. (1993). Unifying learning methods by colored

digraphs.

In Proceedings of the Learning and Know ledge Acquisition Workshop at

IJCAI-93.

Zahn, C. T. (1971). Graph-theoretical methods for detecting and describing gestalt clusters.

IEEE Transactions on Computers, 20 (1), 68{86.

255

","The ability to identify interesting and repetitive substructures is an
essential component to discovering knowledge in structural data. We describe a
new version of our SUBDUE substructure discovery system based on the minimum
description length principle. The SUBDUE system discovers substructures that
compress the original data and represent structural concepts in the data. By
replacing previously-discovered substructures in the data, multiple passes of
SUBDUE produce a hierarchical description of the structural regularities in the
data. SUBDUE uses a computationally-bounded inexact graph match that identifies
similar, but not identical, instances of a substructure and finds an
approximate measure of closeness of two substructures when under computational
constraints. In addition to the minimum description length principle, other
background knowledge can be used by SUBDUE to guide the search towards more
appropriate substructures. Experiments in a variety of domains demonstrate
SUBDUE's ability to find substructures capable of compressing the original data
and to discover structural concepts important to the domain. Description of
Online Appendix: This is a compressed tar file containing the SUBDUE discovery
system, written in C. The program accepts as input databases represented in
graph form, and will output discovered substructures with their corresponding
value."
"Journal of Artiﬁcial Intelligence Research 1 (1994) 159-208

Submitted 8/93; published 2/94

Bias-Driven Revision of Logical Domain Theories

Moshe Koppel
Ronen Feldman
Department of Mathematics and Computer Science, Bar-Ilan University,
Ramat-Gan, Israel

KOPPEL@BIMACS.CS.BIU.AC.IL

FELDMAN@BIMACS.CS.BIU.AC.IL

Alberto Maria Segre
Department of Computer Science, Cornell University,
Ithaca, NY 14853, USA

SEGRE@CS.CORNELL.EDU

Abstract

The  theory  revision  problem  is  the  problem  of  how best  to  go  about  revising  a  deﬁcient
domain theory using information contained in examples that expose inaccuracies. In this paper we
present  our  approach  to  the  theory  revision  problem  for  propositional  domain  theories. The
approach described here, called PTR, uses probabilities associated with domain theory elements to
numerically track the ‘‘ﬂow’’ of proof through the theory. This allows us to measure the precise
role of a clause or literal in allowing or preventing a (desired or undesired) derivation for a given
example.  This information is used to efﬁciently locate and repair ﬂawed elements of the theory.
PTR  is  proved to converge to a theory  which  correctly  classiﬁes  all  examples,  and  shown
experimentally to be fast and accurate even for deep theories.

1.  Introduction

One of the main problems in building expert systems is that models elicited from experts tend to
be  only  approximately  correct. Although  such  hand-coded  models  might  make a good  ﬁrst
approximation to the real world, they typically contain inaccuracies that are exposed when a fact
is  asserted  that  does  not  agree  with  empirical  observation.  The theory  revision  problem is  the
problem  of  how best  to  go  about  revising  a  knowledge  base  on  the  basis  of  a  collection  of
examples,  some  of  which  expose  inaccuracies  in  the  original  knowledge  base.  Of  course,  there
may be many possible revisions that sufﬁciently account for all of the observed examples; ideally,
one  would  ﬁnd  a  revised  knowledge  base  which  is  both  consistent  with  the  examples  and  as
faithful as possible to the original knowledge base.

Consider, for  example,  the  following  simple  propositional  domain  theory, T

. This  theory,
although  ﬂawed  and  incomplete,  is  meant  to  recognize  situations  where  an  investor  should  buy
stock in a soft drink company.

buy-stock ‹
product-liability ‹
increased-demand ‹
increased-demand ‹

increased-demand (cid:217) ¬product-liability

popular-product (cid:217) unsafe-packaging

popular-product (cid:217) established-market
new-market (cid:217) superior-ﬂavor.

The theory T
essentially states that buying stock in this company is a good idea if demand for its
product is expected to increase and the company is not expected to face product liability lawsuits.
In this theory, product liability lawsuits may result if the product is popular (and therefore may
present  an  attractive  target  for  sabotage)  and  if  the  packaging  is  not  tamper-proof.  Increased
product demand results if the product is popular and enjoys a large market share, or if there are

© 1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

KOPPEL, FELDMAN, & SEGRE

new market  opportunities  and  the  product  boasts  a  superior  ﬂavor.  Using  the  closed  world
assumption, buy-stock is derivable given that the set of true observable propositions is precisely,
say,

{popular-product, established-market, celebrity-endorsement}, or
{popular-product, established-market, colorful-label}

but not if they are, say,

{unsafe-packaging, new-market}, or
{popular-product, unsafe-packaging, established-market}.

Suppose  now that  we  are  told  for  various  examples  whether buy-stock should  be  derivable.

For example, suppose we are told that if the set of true observable propositions is:

(1)

(2)

(3)

(4)

(5)

{popular-product, unsafe-packaging, established-market} then buy-stock is false,

{unsafe-packaging, new-market} then buy-stock is true,

{popular-product, established-market, celebrity-endorsement} then buy-stock is true,

{popular-product, established-market, superior-ﬂavor} then buy-stock is false,

{popular-product, established-market, ecologically-correct} then buy-stock is false, and

{new-market, celebrity-endorsement} then buy-stock is true.
(6)
Observe that examples 2, 4, 5 and 6 are misclassiﬁed by the current theory T
. Assuming that the
explicitly given information regarding the examples is correct, the question is how to revise the
theory so that all of the examples will be correctly classiﬁed.

1.1.  Two Paradigms

One approach to this problem consists of enumerating partial proofs of the various examples in
order to ﬁnd a minimal set of domain theory elements (i.e., literals or clauses) the repair of which
will  satisfy  all  the  examples  (EITHER, Ourston  &  Mooney, in press).  One problem  with  this
approach is that proof enumeration even for a single example is potentially exponential in the size
of the theory. Another problem with this approach is that it is unable to handle negated internal
literals, and is restricted to situations where each example must belong to one and only one class.
These  problems  suggest  that  it  would  be  worthwhile  to  circumvent  proof  enumeration  by
employing incremental numerical schemes for focusing blame on speciﬁc elements.

A completely  different  approach  to  the  revision  problem  is  based  on  the  use  of  neural
networks (KBANN, Towell & Shavlik, 1993). The idea is to transform the original domain theory
into  network  form,  assigning  weights  in  the  graph  according  to  some  pre-established  scheme.
The  connection  weights  are  then  adjusted  in  accordance  with  the  observed  examples  using
standard  neural-network  backpropagation  techniques. The  resulting  network  is  then  translated
back  into  clausal  form. The  main  disadvantage  of  this  method  that  it  lacks representational
transparency; the  neural  network  representation  does  not  preserve the  structure  of  the  original
knowledge base while revising it. As a result, a great deal of structural information may be lost
translating  back  and  forth  between  representations. Moreover,  such  translation  imposes  the
limitations  of  both  representations;  for  example,  since  neural  networks  are  typically  slow to
converge, the method is practical for only very shallow domain theories. Finally, revised domain
theories  obtained  via  translation  from  neural  networks  tend  to  be  signiﬁcantly  larger  than  their
corresponding original domain theories.

160

BIAS DRIVEN REVISION

Other approaches to theory revision which are much less closely related to the approach we
will  espouse  here  are  RTLS  (Ginsberg, 1990), KR-FOCL  (Pazzani  &  Brunk, 1991),  and
ODYSSEUS (Wilkins, 1988).

1.2.  Probabilistic Theory Revision

Probabilistic  Theory  Revision  (PTR)  is  a  new approach  to  theory  revision  which  combines  the
best  features  of  the  two approaches  discussed  above. The  starting  point  for  PTR  is  the
observation  that  any method  for  choosing  among  several  possible  revisions  is  based  on  some
implicit bias, namely the a priori probability that each element (clause or literal) of the domain
theory requires revision.

In PTR this bias is made explicit right from the start. That is, each element in the theory is
assigned some a priori probability that it is not ﬂawed.  These probabilities might be assigned by
an expert or simply chosen by default.

The  mere  existence  of  such  probabilities  solves  two central  problems  at  once. First,  these
probabilities very naturally deﬁne the ‘‘best’’ (i.e., most probable) revision out of a given set of
possible  revisions.  Thus, our  objective  is well-deﬁned;  there  is  no  need  to  impose  artiﬁcial
syntactic or semantic criteria for identifying the optimal revision.  Second, these probabilities can
be  adjusted  in  response  to  newly-obtained  information. Thus  they provide  a  framework  for
incremental revision of the ﬂawed domain theory.

Brieﬂy, then,  PTR  is  an  algorithm  which  uses  a  set  of  provided  examples  to  incrementally
adjust probabilities associated with the elements of a possibly-ﬂawed domain theory in order to
ﬁnd the ‘‘most probable’’ set of revisions to the theory which will bring it into accord with the
examples.1 Like KBANN,  PTR  incrementally  adjusts  weights  associated  with  domain  theory
elements;  like EITHER,  all  stages  of  PTR  are  carried  out  within  the  symbolic  logic  framework
and the obtained theories are not probabilistic.

As a result PTR has the following features:

(1) 

(2) 

(3) 

it can handle a broad range of theories including those with negated internal literals and
multiple roots.

it is linear in the size of the theory times the number of given examples.

it produces  relatively  small,  accurate  theories  that  retain  much  of  the  structure  of  the
original theory.

(4) 

it can exploit additional user-provided bias.

In the next section of this paper we formally deﬁne the theory revision problem and discuss
issues of data representation. We  lay the foundations for any future approach to theory revision
by  introducing  very  sharply  deﬁned  terminology  and  notation.  In  Section  3  we  propose  an
efﬁcient  algorithm  for  ﬁnding  ﬂawed  elements  of  a  theory, and  in  Section  4  we  show how  to
revise these elements. Section 5 describes how these two components are combined to form the
PTR algorithm. In Section 5, we also discuss the termination and convergence properties of PTR
and walk through a simple example of PTR in action. In Section 6 we experimentally evaluate
PTR and compare it to other theory revision algorithms. In Section 7, we sum up our results and

1 In the following section we will make precise the notion of ‘‘most probable set of revisions.’’

161

KOPPEL, FELDMAN, & SEGRE

indicate directions for further research.

The  formal  presentation  of  the  work  described  here  is,  unfortunately, necessarily  dense.  To
aid  the  more  casual  reader, we hav e moved all  formal  proofs  to  three  separate  appendices.  In
particular, in the  third  appendix  we  prove  that,  under  appropriate  conditions,  PTR  converges.
Reading  of  these  appendices  can  safely  be  postponed  until  after  the  rest  of  the  paper  has  been
read.  In addition,  we  provide  in  Appendix  D,  a  ‘‘quick  reference  guide’’ to the  notation  used
throughout  the  paper. We would  suggest  that  a  more  casual  reader  might  prefer  to  focus  on
Section  2,  followed  by  a  cursory  reading  of  Sections  3  and  4,  and  a  more  thorough  reading  of
Section 5.

2.  Representing the Problem
A propositional domain theory, denoted G
Bi
where Ci is a clause label, Hi is a proposition (called the head of Ci) and Bi is a set of positive
and  negative  literals  (called  the body of Ci).  As  usual,  the  clause Ci: Hi ‹
Bi represents  the
assertion that the proposition Hi is implied by the conjunction of literals in Bi. The domain theory
is simply the conjunction of its clauses. It may be convenient to think of this as a propositional
logic program without facts (but with negation allowed).

, is a stratiﬁed set of clauses of the form Ci: Hi ‹

A proposition  which  does  not  appear  in  the  head  of  any clause  is  said  to  be observable. A
proposition  which  appears  in  the  head  of  some  clause  but  does  not  appear  in  the  body  of  any
clause is called a root. An example, E, is a truth assignment to all observable propositions. It is
convenient to think of E as a set of true observable propositions.

Let G
be  a  domain  theory  with  roots r1, . . . , rn. For  an  example, E, we deﬁne  the  vector
G (E) = Æ
i(E) = 0 if
1(E), . . . , G n(E) æ where G
E |–/ G ri. Intuitively, G (E) tells us which of the conclusions r1, . . . , rn can be drawn by the expert
system when given the truth assignment E.

i(E) = 1 if E |– G ri (using  resolution)  and G

Let the target domain theory, Q

of interest. In other words, Q
is called an exemplar of the domain: if Q
of ri, while  if Q
theory revision, we know Q

, be some domain theory which accurately models the domain
(E) æ ,
i(E) = 1 then the exemplar is said to be an IN exemplar
i(E) = 0 then  the  exemplar  is  said  to  be  an OUT  exemplar of ri. Typically, in

represents the correct domain theory. An ordered pair, Æ E, Q

.
Let G be some possibly incorrect theory for a domain which is in turn correctly modeled by
(E).
. Thus, a misclassiﬁed IN exemplar for ri, or false
i(E) = 0, while a misclassiﬁed OUT exemplar for ri, or
i(E) = 1.2 Typically, in theory  revision  we  know

the target theory Q
Such exemplars are said to be misclassiﬁed by G
negative for ri, will have Q
false  positive  for ri, will  have Q

. Any inaccuracies in G will be reﬂected by exemplars for which G (E) „

i(E) = 0 but G

i(E) = 1 but G

(E) without knowing Q

(E) without knowing Q

.

Consider, for example, the domain theory, T , and example set introduced in Section 1. The
theory T has  only  a  single  root, buy-stock. The  observable  propositions  mentioned  in  the
examples  are popular-product, unsafe-packaging, established-market, new-market, celebrity-

2 We prefer the new terminology ‘‘IN/OUT’’ to the more standard ‘‘positive/negative’’ because the lat-
ter is often used to refer to the classiﬁcation of the example by the given theory, while we use ‘‘IN/OUT’’ to
refer speciﬁcally to the actual classiﬁcation of the example.

162

G
Q
Q
BIAS DRIVEN REVISION

superior-ﬂavor,

endorsement,
E = {unsafe-packaging, new-market} we  have T (E) = Æ
1(E) æ = Æ 1 æ .
that
misclassiﬁed IN exemplar of the root buy-stock.

(E) = Æ

Thus,

and

ecologically-correct.

For 

example
1(E) æ = Æ 0 æ . Nev ertheless,  we  are  told
a

the 

is 

E = Æ {unsafe-packaging, new-market}, Æ 1 æ

Now,  giv en misclassiﬁed exemplars, there are four re vision operators available for use with

propositional domain theories:

(1) 

(2) 

(3) 

add a literal to an existing clause,

delete an existing clause,

add a new clause, and

delete a literal from an existing clause.

(4) 
For neg ation-free domain theories, the ﬁrst two operations result in specializing G
, since they may
allow some  IN  exemplars  to  become  OUT  exemplars.  The latter  two operations  result  in
generalizing G
, since they may allow some OUT exemplars to become IN exemplars.3

We  say  that  a  set  of  revisions  to G

operators are applied, all the exemplars are correctly classiﬁed by the revised domain theory G
Note  that  we  are  not  implying  that G
Æ E, Q
(E) = Q
any theory revision system, then, is to ﬁnd the ‘‘best’’ revision set for G
given a set of exemplars.

is adequate for  a  set  of  exemplars  if,  after  the  revision
.
, but  rather  that  for  every  exemplar
(E). Thus, there may be more than one adequate revision set. The goal of
, which is adequate for a

is  identical  to Q

(E) æ

, G

2.1.  Domain Theories as Graphs

In order to deﬁne the problem even more precisely and to set the stage for its solution, we will
show how to represent a domain theory in the form of a weighted digraph. We begin by deﬁning a
more general version of the standard AND–OR proof tree, which collapses the distinction between
AND nodes and OR nodes.

For any  set  of  propositions {P1, . . . , Pn}, let  NAND({P1, . . . , Pn}) be a Boolean  formula
which is false if and only if {P1, . . . , Pn} are all true. Any domain theory G can be translated into
an equivalent domain theory ˆG consisting of NAND equations as follows:

.

(1) 

(2) 

, the equation ˆCi = NAND(Bi) is in ˆG

For each clause Ci: Hi ‹
Bi
the equation P = NAND(C P) is in
For each non-observable proposition P appearing in G
ˆG
, where C P = { ˆCi Hi = P}, i.e., the set consisting of the label of each clause in G whose
head is P.
For each negative literal ¬P appearing in G

, the equation ¬P = NAND({P}) is in ˆG

(3) 
contains  no  equations  other  than  these. Observe that  the  literals  of ˆG

ˆG
together  with  the  new literals  { ˆCi} which  correspond  to  the  clauses  of G
equivalent to G
observable propositions of G

are  the  literals  of G
. Most  important, ˆG
is
in the sense that for each literal l in G and any assignment E of truth values to the

, E |– ˆG l if and only if E |– G l.

.

3 In the event that negative  literals appear in the domain theory, the consequences of applying these

operators are slightly less obvious.  This will be made precise in the second part of this section.

163

T
Q
Q
æ
¢
¢
¢
˛
G
KOPPEL, FELDMAN, & SEGRE

Consider, for example, the domain theory T of Section 1. The set of NAND equations ˆT
buy-stock = NAND({C1}),
C1 = NAND({increased-demand, ¬product-liability}),
¬product-liability = NAND({product-liability}),
increased-demand = NAND({C3, C4}),
product-liability = NAND({C2}),
C2 = NAND({popular-product, unsafe-packaging}),
C3 = NAND({popular-product, established-market}), and
C4 = NAND({new-market, superior-ﬂavor}).

is

Observe that buy-stock is  true  in ˆT
which buy-stock is true in T .

for  precisely  those  truth  assignments  to  the  observables  for

to obtain a useful graph representation of G

We  now use ˆG
refer to the left side of ˆG
i. In other words, h( ˆG
ˆG

i and let b( ˆG
i) = NAND(b( ˆG

i)
i) refer to the set of literals which appear on the right side of
i)).

. For an equation ˆG

, let h( ˆG

i in ˆG

for a domain theory G

Deﬁnition: A dt-graph D
correspond to the literals of ˆG
x = h( ˆG
of ordered pairs { Æ x, y æ
r we add an edge, er , leading into r (from some artiﬁcial node).

consists of a set of nodes which
and a set of directed edges corresponding to the set
ˆG }. In addition, for each root

i), y ˛ b( ˆG

i), ˆG

i

In  other  words, D
graph representation of T

is shown in Figure 1.

G consists  of  edges  from  each  literal  in ˆG

to  each  of  its  antecedents. The  dt-

Let ne be the node to which the edge e leads and let ne be the node from which it comes. If
ne is a clause, then we say that e is a clause edge; if ne is a root, then we say that e is a root edge;
if ne is a literal and ne is a clause, then we say that e is a literal edge; if ne is a proposition and ne
is its negation, then we say that e is a negation edge.

The dt-graph D

is very much like an AND–OR graph for G

. It has, however, a very signiﬁcant
advantage  over AND–OR graphs  because  it  collapses  the  distinction  between  clause  edges  and
literal edges which is central to the AND–OR graph representation. In fact, even neg ation edges
(which do not appear at all in the AND–OR representation) are not distinguished from literal edges
and clause edges in the dt-graph representation.

In terms of the dt-graph D

G , there are two basic revision operators — deleting edges or adding
edges.  What are the effects of adding or deleting edges from D
G ? If the length of every path from
a root r to a node n is even (odd) then n is said to be an even (odd) node for ri. If ne is even (odd)
for ri, then e is said to be even (odd) for ri. (Of course it is possible that the depth of an edge is
neither even nor odd.) Deleting an even edge for ri specializes the deﬁnitions of ri in the sense
(E) æ ; likewise,
i(E) for all exemplars Æ E, Q
i(E) £
that if D
adding an even edge for ri generalizes the deﬁnition of ri in the sense that if D
is the result of
adding the edge to D
i(E).  Analogously, deleting an odd edge for ri generalizes
the deﬁnition of ri, while adding an odd edge for ri specializes the deﬁnition of ri. (Deleting or
adding an edge which is neither odd nor even for ri might result in a new deﬁnition of ri which is
neither strictly more general nor strictly more speciﬁc.)

is the result of the deletion, then G

i(E) ‡

then G

G . Then an even edge in D

To  understand this intuitively, ﬁrst consider the case in which there are no negation edges in
, so that deleting is specialization and adding
is  generalization. An  odd  edge  in D
so  that
deleting is generalization and adding a specialization. Now,  if an odd number of negation edges

represents  a  literal  in  the  body  of  a  clause  in G

represents a clause in G

164

G
˛
G
G
¢
¢
G
G
¢
G
¢
G
D
G
G
BIAS DRIVEN REVISION

buy-stock

C1

increased-demand

¬product-liability

C4

C3

product-liability

new-market

established-market

C2

superior-ﬂavor

popular-product

unsafe-packaging

Figure 1: The dt-graph, D

T , of the theory T

.

are present on the path from ri to an edge then the role of the edge is reversed.

G , w æ where D

2.2.  Weighted Graphs
A weighted dt-graph is an ordered pair Æ
is a dt-graph w and is an assignment
of  values  in  (0, 1]  to  each  node  and  edge  in D
G . For  an  edge e, w(e) is meant  to  represent  the
user’s degree  of  conﬁdence  that  the  edge e need  not  be  deleted  to  obtain  the  correct  domain
theory. For a node n, w(n) is the user’s degree of conﬁdence that no edge leading from the node
n need be added in order to obtain the correct domain theory. Thus, for example, the assignment
w(n) = 1 means  that  it  is  certain  that  no  edge  need  be  added  to  the  node n and  the  assignment
w(e) means that it is certain that e should not be deleted. Observe that if the node n is labeled by
a neg ative literal or an observable proposition then w(n) = 1 by deﬁnition, since graphs obtained
by adding edges to such nodes do not correspond to any domain theory. Likewise, if e is a root-
edge or a negation-edge, then w(e) = 1.

165

D
G
KOPPEL, FELDMAN, & SEGRE

For practical reasons, we conﬂate the weight w(e) of an edge e and the weight, w(ne), of the
node ne, into a single value, p(e) = w(e) · w(ne), associated with the edge e. The value p(e) is
the user’s conﬁdence that e need not be repaired, either by deletion or by dilution via addition of
child edges.

There are many ways that these values can be assigned. Ideally, they can be provided by the
expert  such  that  they actually  reﬂect  the  expert’s degree  of  conﬁdence  in  each  element  of  the
theory. Howev er, even in the absence of such information, values can be assigned by default; for
example,  all  elements  can  be  assigned  equal  value.  A more  sophisticated  method  of  assigning
values is to assign higher values to elements which have  greater ‘‘semantic impact’’ (e.g., those
closer  to  the  roots). The  details  of  one  such  method  are  given in Appendix  A. It  is  also,  of
course, possible for the expert to assign some weights and for the rest to be assigned according to
some default scheme. For example, in the weighted dt-graph, Æ
T , p æ , shown in Figure 2, some
edges  have  been  assigned  weight  near  1  and  others  have  been  assigned  weights  according  to  a
simple default scheme.

The semantics of the values associated with the edges can be made clear by considering the
case in which it is known that the correct dt-graph is a subset of the given dt-graph, D
. Consider a
probability function on the space of all subgraphs of D
. The weight of an edge is simply the sum
of the probabilities of the subgraphs in which the edge appears. Thus the weight of an edge is the
probability that the edge does indeed appear in the target dt-graph. We  easily extend this to the
case where the target dt-graph is not necessarily a subgraph of the given one.4

Conversely, giv en only the probabilities associated with edges and assuming that the deletion
of  different  edges  are  independent  events,  we  can  compute  the  probability  of  a  subgraph, D
.
Since p(e) is the probability that e is not deleted and 1 - p(e) is the probability that e is deleted, it
follows that
) =
p(D

1 - p(e).

p(e) ·

e ˛
Letting S = D
) =
p(D

e ˛

¢ , we rewrite this as
p(e) ·

1 - p(e).

e ˛

- S

e ˛ S

We use this formula as a basis for assigning a value to each dt-graph D

obtainable from D via
revision of the set of edges S, even in the case where edge-independence does not hold and even
in the case in which D
) =

is not a subset of D

. We simply deﬁne

1 - p(e).

p(e) ·

w(D

e ˛

- S
(In the event that D
maximized.)  Note that  where  independence  holds  and D

and D

e ˛ S

are such that S is not uniquely deﬁned, choose S such that w(D

is  subgraph  of

) is
, we hav e

4 In order to avoid confusion it should be emphasized that the meaning of the weights associated with
edges is completely different than that associated with edges of Pearl’s Bayesian networks (1988). For us,
these weights represent a meta-domain-theory concept: the probability that this edge appears in some un-
known  target  domain  theory. For  Pearl  they represent  conditional  probabilities  within  a  probabilistic  do-
main theory. Thus, the updating method we are about to introduce is totally unrelated to that of Pearl.

166

D
¢
¢
D
¢
P
D
-
D
¢
P
-
D
¢
D
P
P
¢
¢
¢
D
P
P
¢
¢
¢
D
BIAS DRIVEN REVISION

.999

buy-stock

.99

C1

1.0

.95

increased-demand

¬product-liabilty

.9

.9

C3

product-liability

.8

.8

.9

C2

.8

.99

popular-product

unsafe-packaging

.9

.8

C4

.8

new-market

superior-ﬂavor

established-market

Figure 2: The weighted dt-graph, Æ

T , p æ .

w(D

) = p(D

).

2.3.  Objectives of Theory Revision

Now we can formally deﬁne the proper objective of a theory revision algorithm:

Given a weighted dt-graph Æ

, p æ and a set of exemplars Z

correctly classiﬁes every exemplar in Z and w(D

, ﬁnd a dt-graph D
) is maximal over all such dt-graphs.

such that

Restating this in the terminology of information theory, we deﬁne the radicality of a dt-graph D
, p æ as
relative to an initial weighted dt-graph K = Æ
log(1 - p(e))

log( p(e)) +

RadK (D

) =

e ˛

- S

e ˛ S

where S is  the  set  of  edges  of D which  need  to  be  revised  in  order  to  obtain D
weighted dt-graph K

. Thus  given a
, we wish to ﬁnd the least radical dt-graph relative

and a set of exemplars Z

167

D
¢
¢
D
¢
D
¢
¢
¢
D
¢
D
S
-
S
-
¢
KOPPEL, FELDMAN, & SEGRE

to K which correctly classiﬁes the set of exemplars Z

.

Note that radicality is a straightforward measure of the quality of a revision set which neatly
balances syntactic and semantic considerations. It has been often noted that minimizing syntactic
change  alone  can  lead  to  counter-intuitive  results  by  giving  preference  to  changes  near  the  root
which radically alter the semantics of the theory. On the other hand, regardless of the distribution
of examples, minimizing semantic change alone results in simply appending to the domain theory
the correct classiﬁcations of the given misclassiﬁed examples without affecting the classiﬁcation
of any other examples.

Minimizing radicality automatically takes into account both these criteria. Thus, for example,
by  assigning  higher  initial  weights  to  edges  with  greater  semantic  impact  (as  in  our  default
scheme  of  Appendix  A),  the  syntactic  advantage  of  revising  close  to  the  root  is  offset  by  the
higher cost of such revisions.  For example, suppose we are given the theory T of the introduction
and the single misclassiﬁed exemplar

Æ {unsafe-packaging, new-market}, Æ 1 æ

.

There  are  several  possible  revisions  which  would  bring T
could, for example, add a new clause

buy-stock ‹

unsafe-packaging (cid:217) new-market,

into  accord  with  the  exemplar. We

delete superior-ﬂavor from clause C4, delete popular-product and established-market from clause
C3, or delete increased-demand from clause C1.  Given the weights of Figure 2, the deletion of
superior-ﬂavor from clause C4 is clearly the least radical revision.

Observe that  in  the  special  case  where  all  edges  are  assigned  identical  initial  weights,
regardless of their semantic strength, minimization of radicality does indeed reduce to a form of
minimization  of  syntactic  change.  We wish  to  point  out,  however,  that  even in this  case  our
deﬁnition  of  ‘‘syntactic  change’’ differs  from  some  previous  deﬁnitions  (Wogulis  &
Pazzani, 1993). Whereas  those  deﬁnitions  count  the  number  of  deleted  and  added  edges,  we
count the number of edges deleted or added to. To understand why this is preferable, consider the
case in which some internal literal, which happens to have a large deﬁnition, is omitted from one
of  the  clause  in  the  target  theory. Methods  which  count  the  number  of  added  edges  will  be
strongly  biased  against  restoring  this  literal,  prefering  instead  to  make several different  repairs
which  collectively  involve fewer  edges  than  to  make a single repair  involving  more  edges.
Nevertheless, given the assumption that the probabilities of the various edges in the given theory
being mistaken are equal, it is far more intuitive to repair only at a single edge, as PTR does. (We
agree, though, that once an edge has been chosen for repair, the chosen repair should be minimal
over all equally effective repairs.)

3.  Finding Flawed Elements

PTR  is  an  algorithm  which  ﬁnds  an  adequate  set  of  revisions  of  approximately  minimum
radicality. It achieves this by locating ﬂawed edges and then repairing them. In this section we
give the algorithm for locating ﬂawed edges; in the next section we show how to repair them.

The  underlying  principle  of  locating  ﬂawed  edges  is  to  process  exemplars  one  at  a  time,  in
each  case  updating  the  weights  associated  with  edges  in  accordance  with  the  information
contained in the exemplars.  We measure the ‘‘ﬂow’’ of a proof (or refutation) through the edges
of the graph. The more an edge contributes to the correct classiﬁcation of an example, the more
its weight is raised; the more it contributes to the misclassiﬁcation of the example, the more its

168

æ
BIAS DRIVEN REVISION

weight is lowered. If the weight of an edge drops below a prespeciﬁed revision threshold s , it is
revised.

The  core  of  the  algorithm  is  the  method  of  updating  the  weights. Recall  that  the  weight
represents the probability that an edge appears in the target domain theory. The most natural way
to update these weights, then, is to replace the probability that an edge need not be revised with
the conditional probability that it need not be revised given the classiﬁcation of an exemplar. As
we shall see later, the computation of conditional probabilities ensures many desirable properties
of updating which ad hoc methods are liable to miss.

3.1.  Processing a Single Exemplar

One  of  the  most  important  results  of  this  paper  is  that under  certain  conditions  the  conditional
probabilities of all the edges in the graph can be computed in a single bottom-up-then-top-down
sweep  through  the  dt-graph. We shall  employ this  method  of  computation  even when  those
conditions do not hold. In this way, updating is performed in highly efﬁcient fashion while, at the
same time, retaining the relevant desirable properties of conditional probabilities.

More  precisely, the  algorithm  proceeds  as  follows.  We think  of  the  nodes  of D

G which
represent  observable  propositions  as  input  nodes,  and  we  think  of  the  values  assigned  by  an
example E to each observable proposition as inputs. Recall that the assignment of weights to the
edges  is  associated  with  an  implicit  assignment  of  probabilities  to  various  dt-graphs  obtainable
via revision of D
G . For some of these dt-graphs, the root ri is provable from the example E, while
for others it is not. We  wish to make a bottom-up pass through K = Æ
in order to compute
(or at least approximate) for each root ri, the probability that the target domain theory is such that
ri is  true  for  the  example E. The  obtained  probability  can  then  be  compared  with  the  desired
result, Q
i(E), and the resulting difference can be used as a basis for adjusting the weights, w(e),
for each edge e.

G , p æ

Let

E(P) =

1
0

if P is true in E
if P is false in E.

is true if the literal of ˆG which labels it is true. Now, a node passes the
We say that a node n ˛
value ‘‘true’’ up the graph if it is either true or deleted, i.e., if it is not both undeleted and false.
the  value
Thus, 
uE (e) = 1 -
(1 - E(P))] is the probability of the value ‘‘true’’ being passed up the graph
from e.5

the  observable  proposition P,

for  an  edge
[ p(e) ·

that ne

such 

is 

e

Now,  recalling that a node in D

is
independent of the truth of any of its brothers, then for any edge e, the probability of ‘‘true’’ being
passed up the graph is

represents a NAND operation, if the truth of a node in D

5 Note that, in principle, the updating can be performed exactly the same way even if 0 < E(P) < 1.
Thus, the algorithm extends naturally to the case in which there is uncertainty regarding the truth-value of
some of the observable propositions.

169

D
(cid:236)
(cid:237)
(cid:238)
D
G
G
G
KOPPEL, FELDMAN, & SEGRE

uE (e) = 1 - p(e)

uE (s).

s ˛

children(e)

We call uE (e) the ﬂow of E through e.

We hav e deﬁned the ﬂow uE (e) such that, under appropriate independence conditions, for any
node ne, uE (e) is in fact the probability that ne is true given Æ
G , w æ and E. (For a formal proof
of this, see Appendix B.) In particular, for a root ri, the ﬂow uE (eri ) is, even in the absence of the
independence  conditions,  a  good  approximation  of  the  probability  that  the  target  theory  is  such
that ri is true given Æ

G , w æ and E.

In  the  second  stage  of  the  updating  algorithm,  we  propagate  the  difference  between  each
computed  value uE (eri ) (which  lies  somewhere  between  0  and  1)  and  its  target  value Q
i(E)
(which  is  either  0  or  1)  top-down  through D
in  a  process  similar  to  backpropagation  in  neural
networks.  As we proceed, we compute a new value vE (e) as well as an updated value for p(e),
for every edge e in D
G . The new value vE (e) represents an updating of uE (e) where the correct
classiﬁcation, Q

(E), of the example E has been taken into account.

Thus,  we  begin  by  setting  each  value vE (ri) to reﬂect  the  correct  classiﬁcation  of  the

example.  Let e > 0 be some very small constant6 and let

vE (eri ) =

e
1 -

e

if Q
if Q

i(E) = 0
i(E) = 1.

Now we proceed  top  down  through D
G . In each  case  we
compute vE (e) on the basis of uE (e), that is, on the basis of how much of the proof (or refutation)
of E ﬂows through the edge e. The precise formula is

G , computing vE (e) for  each  edge  in D

vE (e) = 1 -

(1 - uE (e)) ·

vE ( f (e))
uE ( f (e))

where f (e) is that parent of e for which

Appendix B why this formula works.

1 - max[vE ( f (e)), uE ( f (e))]
min[vE ( f (e)), uE ( f (e))]

is greatest. We  show in

Finally, we compute pnew(e), the new values of p(e), using the current value of p(e) and the

values of vE (e) and uE (e) just computed:

pnew(e) = 1 -

(1 - p(e)) ·

vE (e)
uE (e)

.

If the deletion of different edges are independent events and Q

is known to be a subgraph of
, giv en the  exemplar
(see proof in Appendix B). Figure 3 gives the pseudo code for processing a single

, then pnew(e) is the  conditional  probability  that  the  edge e appears  in Q
Æ E, Q
(E) æ
exemplar.

6 Strictly speaking, for the computation of conditional probabilities, we need to use e = 0. However, in
order to ensure convergence of the algorithm in all cases, we choose e > 0 (see Appendix C). In the experi-
ments reported in Section 6, we use the value e = . 01.

170

P
D
D
G
(cid:236)
(cid:237)
(cid:238)
(cid:239)
(cid:239)
(cid:239)
(cid:239)
(cid:239)
(cid:239)
G
BIAS DRIVEN REVISION

, p æ : weighted dt-graph, E: exemplar): array of real;

Leaves(D );

function BottomUp( Æ
begin
S (cid:220)
; V (cid:220)
for e ˛ Leaves(D ) do
begin
if e ˛ E then u(e) (cid:220)
else u(e) (cid:220)
S (cid:220) Merge(S, Parents(e, D ));
end
while S „
begin
e (cid:220) PopSuitableParent(S, V ); V (cid:220) AddElement(e, V );
u(e) (cid:220)

1 - p(e);

u(d));

( p(e)

do

1 -

1;

d ˛ Children(e,D )
S (cid:220) Merge(S, Parents(e, D ));
end
return u;
end
function TopDown( Æ

, p æ : weighted dt-graph, u: array of real,

E: exemplar,

e : real): weighted dt-graph;

e ;

; V (cid:220) Roots(D );
˛ Roots(D ) do

i(E) = 1 then v(ri) (cid:220)
1 -

begin
S (cid:220)
for ri
begin
if G
else v(ri) (cid:220)
S (cid:220) Merge(S, Children(ri, D ));
end
while S „
begin
e (cid:220) PopSuitableChild(S, V ); V (cid:220) AddElement(e, V ); f (cid:220) MostChangedParent(e, D );
v(e) (cid:220)

(1 - u(e)) ·

do

e ;

1 -

;

v( f )
u( f )
v(e)
u(e)

1 -

(1 - p(e)) ·
p(e) (cid:220)
;
S (cid:220) Merge(S, Children(e, D ));
end
return Æ
end

, p æ ;

Figure  3:  Pseudo  code  for  processing  a  single  exemplar. The  functions BottomUp and TopDown
sweep the dt-graph. BottomUp returns an array on edges representing proof ﬂow, while TopDown
returns  an  updated  weighted  dt-graph. We  are  assuming  the  dt-graph  datastructure  has  been  de-
ﬁned and initialized appropriately. Functions Children, Parents, Roots, and Leaves return sets of
edges corresponding to the corresponding graph relation on the dt-graph. Function Merge and Ad-
dElement operate  on  sets,  and  functions PopSuitableParent and PopSuitableChild return  an  ele-
ment of its ﬁrst argument whose children or parents, respectively, are all already elements of its
second  argument  while  simultaneously  deleting  the  element  from  the  ﬁrst  set,  thus  guaranteeing
the appropriate graph traversal strategy.

171

D
˘
˘
P
D
˘
˘
D
KOPPEL, FELDMAN, & SEGRE

Consider the application of this updating algorithm to the weighted dt-graph of Figure 2. We
are  given the  exemplar
, i.e.,  the  example  in  which
unsafe-packaging and new-market are  true  (and  all  other  observables  are  false)  should  yield  a
derivation  of  the  root buy-stock. The  weighted  dt-graph  obtained  by  applying  the  algorithm  is
shown in Figure 4.

Æ {unsafe-packaging, new-market}, Æ 1 æ

This example illustrates some important general properties of the method.

(1)

Given an IN exemplar, the weight of an odd edge cannot decrease and the weight of an
even edge cannot increase. (The analogous property holds for an OUT exemplar.) In the
case  where  no  negation  edge  appears  in D
G , this  corresponds  to  the  fact  that  a  clause
cannot help prevent a proof, and literals in the body of a clause cannot help complete a

.998

buy-stock

.999

C1

1.0

.94

increased-demand

¬product-liability

.98

.91

1.0

C4

C3

product-liability

.8

.15

.69

.69

new-market

superior-ﬂavor

established-market

.88

C2

.96

.99

popular-product

unsafe-packaging

Figure  4:  The  weighted  dt-graph  of  Figure  2 
Æ {unsafe-packaging, new-market}, Æ 1 æ

.

after  processing 

the 

exemplar

172

æ
æ
BIAS DRIVEN REVISION

proof.  Note in  particular  that  the  weights  of  the  edges  corresponding  to  the  literals
popular-product and established-market in  clause C3 dropped  by  the  same  amount,
reﬂecting the identical roles played by them in this example.  However, the weight of the
edge corresponding to the literal superior-ﬂavor in clause C4 drops a great deal more than
both those edges, reﬂecting the fact that the deletion of superior-ﬂavor alone would allow
a proof  of buy-stock, while  the  deletion  of  either popular-product alone  or established-
market alone would not allow a proof of buy-stock.

An edge with initial weight 1 is immutable; its weight remains 1 forever. Thus although an
edge with weight 1, such as that corresponding to the literal increased-demand in clause
C1, may contribute to the prevention of a desired proof, its weight is not diminished since
we are told that there is no possibility of that literal being ﬂawed.

If the processed exemplar can only be correctly classiﬁed if a particular edge e is revised,
then  the  updated  probability  of e will  approach 0 and  e will  be  immediately  revised.7
Thus,  for  example,  were  the  initial  weights  of  the  edge  corresponding  to established-
market and popular-product in C3 to approach 1, the weight of the edge corresponding to
superior-ﬂavor in C4 would  approach  0. Since  we  use  weights  only  as  a  temporary
device  for  locating  ﬂawed  elements,  this  property  renders  our  updating  method  more
appropriate  for  our  purposes  then  standard  backpropagation  techniques  which  adjust
weights gradually to ensure convergence.

The computational complexity of processing a single exemplar is linear in the size of the
theory G
. Thus,  the  updating  algorithm  is  quite  efﬁcient  when  compared  to  revision
techniques  which  rely  on  enumerating  all  proofs  for  a  root. Note  further  that  the
computation  required  to  update  a  weight  is  identical  for  every  edge  of D
regardless  of
edge type. Thus, PTR is well suited for mapping onto ﬁne-grained SIMD machines.

(2)

(3)

(4)

3.2.  Processing Multiple Exemplars

As stated above, the updating method is applied iteratively to one example at a time (in random
order) until some edge drops below the revision threshold, s . If after a complete cycle no edge
has  dropped  below the  revision  threshold,  the  examples  are  reordered  (randomly)  and  the
updating is continued.8

For example, consider the weighted dt-graph of Figure 2. After processing the exemplars
Æ {unsafe-packaging, new-market}, Æ 1 æ
Æ {popular-product, established-market, superior-ﬂavor}, Æ 0 æ
, and
Æ {popular-product, established-market, celebrity-endorsement}, Æ 0 æ

,

we obtain the dt-graph shown in Figure 5. If our threshold is, say, s = . 1, then we have to revise
the edge corresponding to the clause C3. This reﬂects the fact that the clause C3 has contributed

7 If we were to choose e = 0 in the deﬁnition of vE (er ), then the updated probability would equal 0.
8 Of course, by processing the examples one at a time we abandon any pretense that the algorithm is
Bayesian.  In this respect, we are proceeding in the spirit of connectionist learning algorithms in which it is
assumed that the sequential processing of examples in random order, as if they were actually independent,
approximates the collective effect of the examples.

173

G
æ
æ
æ
KOPPEL, FELDMAN, & SEGRE

.999...

buy-stock

.998

C1

1.0

.95

increased-demand

¬product-liability

.98

.02

1.0

C4

C3

product-liability

.99

.15

.69

.69

new-market

established-market

superior-ﬂavor

.89

C2

.96

.88

popular-product

unsafe-packaging

Figure 5: The weighted dt-graph of Figure 2 after processing exemplars

Æ {unsafe-packaging, new-market}, Æ 1 æ
Æ {popular-product, established-market, superior-ﬂavor}, Æ 0 æ
, and
Æ {popular-product, established-market, celebrity-endorsement}, Æ 0 æ

,

.

The clause C3 has dropped below the threshold.

substantially to the misclassiﬁcation of the second and third examples from the list above while
not contributing substantially to the correct classiﬁcation of the ﬁrst.

4.  Revising a Flawed Edge

Once an edge has been selected for revision, we must decide how to revise it. Recall that p(e)
represents the product of w(e) and w(ne). Thus, the drop in p(e) indicates either that e needs to
be deleted or that, less dramatically, a subtree needs to be appended to the node ne. Thus, we need
to  determine  whether  to  delete  an  edge  completely  or  to  simply  weaken  it  by  adding  children;
intuitively, adding  edges  to  a  clause  node  weakens  the  clause  by  adding  conditions  to  its  body,

174

æ
æ
æ
BIAS DRIVEN REVISION

while adding edges to a proposition node weakens the proposition’s refutation power by adding
clauses to its deﬁnition. Further, if we decide to add children, then we need to determine which
children to add.

4.1.  Finding Relevant Exemplars

The ﬁrst stage in making such a determination consists of establishing, for each exemplar, the role
of  the  edge  in  enabling  or  preventing  a  derivation  of  a  root. More  speciﬁcally, for  an  IN
exemplar, Æ E, Q
(E) æ , of some root, r, an edge e might play a positive role by facilitating a proof
of r, or play a destructive role by preventing a proof of r, or may simply be irrelevant to a proof
of r.

Once  the  sets  of  exemplars  for  which e plays  a  positive  role  or  a  destructive  role  are
determined,  it  is  possible  to  append  to e an  appropriate  subtree  which  effectively  redeﬁnes  the
role  of e such  that  it  is  used  only  for  those  exemplars  for  which  it  plays  a  positive  role.9 How,
then, can we measure the role of e in allowing or preventing a proof of r from E?

At ﬁrst glance, it would appear that it is sufﬁcient to compare the graph D with the graph D e
which results from deleting e from D
. If E |– D r and E |–/ D e r (or vice versa) then it is clear that e
(E) æ . But, this
is ‘‘responsible’’ for r being provable or not provable given the exemplar Æ E, Q
criterion is too rigid. In the case of an OUT exemplar, even if it is the case that E |–/ D e r, it is still
necessary to modify e in the event that e allowed an additional proof of r from E. And, in the
case of an IN exemplar, even if it is the case that E |– D r it is still necessary not to modify e in
such a way as to further prevent a proof of r from E, since ultimately some proof is needed.

Fortunately, the weights assigned to the edges allow us the ﬂexibility to not merely determine
whether or not there is a proof of r from E given D or D e but also to measure numerically the ﬂow
of E through r both with and without e. This is just what is needed to design a simple heuristic
which captures the degree to which e contributes to a proof of r from E.

Let K = Æ

, p æ be the weighted dt-graph which is being revised.  Let K

is identical with p, except that p¢ (e) = 1. Let K
that p¢ (e) = 0; that is, K

e is obtained from K by deleting the edge e.

æ where p¢

e = Æ

, p¢

e = Æ

æ where p¢
, p¢
is identical with p, except

Ri( Æ E, Q

(E) æ , e, K

Then deﬁne for each root ri
º 1 -
º 1 -
(E) æ , e, K
) < 1/2 we say that e is destructive for E and ri.

if Ri( Æ E, Q
(E) æ , e, K

) > 2, we say 

E (eri )

E (eri )

i(E)

i(E)

- u

- u

that

) =

e

.

e

e

Then 
Ri( Æ E, Q

is needed for E and ri

and 

if

9 PTR is not strictly incremental in the sense that when an edge is revised its role in proving or refut-
ing each exemplar  is  checked.  If strict  incrementality  is  a  desideratum,  PTR  can  be  slightly  modiﬁed  so
that an edge is revised on the basis of only those exemplars which have already been processed. Moreover,
it is generally not necessary to check all exemplars for relevance. For example, if e is an odd edge and E is
a correctly  classiﬁed  IN  exemplar, then e can  be  neither  needed  for E (since  odd  edges  can  only  make
derivations more difﬁcult) nor destructive for E (since E is correctly classiﬁed despite e).

175

D
D
D
Ø
Q
ø
ß
K
Ø
Q
ø
ß
K
KOPPEL, FELDMAN, & SEGRE

e

Intuitively, this means, for example, that the edge e is needed for an IN exemplar, E, of ri, if
most  of  the  derivation  of ri from E passes  through  the  edge e. We hav e simply  given formal
deﬁnition  to  the  notion  that  ‘‘most’’ of the  derivation  passes  through e, namely, that  the  ﬂow,
E (eri ), of E through ri with e.
E (eri ), of E through ri without e is less than half of the ﬂow, u
u
For neg ation-free  theories,  this  corresponds  to  the  case  where  the  edge e represents  a  clause
which is critical for the derivation of ri from E. The intuition for destructive edges and for OUT
exemplars is analogous. Figure 6 gives the pseudo code for computing the needed and destructive
sets for a given edge e and exemplar set Z

.

e

In  order  to  understand  this  better, let  us  now return  to  our  example  dt-graph  in  the  state  in
which  we  left  it  in  Figure  5. The  edge  corresponding  to  the  clause C3 has  dropped  below the
threshold.  Now let  us  check  for  which  exemplars  that  edge  is  needed  and  for  which  it  is
destructive. Computing R( Æ E, Q

) for each example E we obtain the following:

(E) æ , C3, H

R( Æ {popular-product, unsafe-packaging, established-market}, Æ 0 æ
R( Æ {unsafe-packaging, new-market}, Æ 1 æ
R( Æ {popular-product, established-market, celebrity-endorsement}, Æ 1 æ
R( Æ {popular-product, established-market, superior-ﬂavor}, Æ 0 æ
, C3, H
R( Æ {popular-product, established-market, ecologically-correct}, Æ 0 æ
R( Æ {new-market, celebrity-endorsement}, Æ 1 æ

) = 1. 0

) = 1. 0

, C3, H

, C3, H

, C3, H

) = 0. 8

) = 136. 1

, C3, H
) = 0. 1

, C3, H

) = 0. 1

, p æ : weighted dt-graph , Z

: exemplar set, e: edge): tuple of set;

1; u (cid:220) BottomUp(D
0; u (cid:220) BottomUp(D
u

, p, E); u
, p, E); u

e

e

E

E

u(ri); p (cid:220)
u(ri); p (cid:220)

psaved ;
psaved ;

e

e

;

if G

i(E) = 1 then Ri

E

u

E

;
;

function Relevance( Æ
begin
N (cid:220)
D (cid:220)
psaved
for E ˛
for ri
p(e) (cid:220)
p(e) (cid:220)

(cid:220) Copy( p);
do

˛ Roots(D ) do

e

e

;

E

E

1 - u
else Ri
1 - u
if Ri > 2 then N (cid:220) N ¨
then D (cid:220) D ¨
if Ri <
end
end
return Æ N , D æ ;
end

1
2

{E};

{E};

Figure 6: Pseudo code for computing the relevant sets (i.e., the needed and destructive sets) for a
given edge e and exemplar set Z
. The general idea is to compare proof ‘‘ﬂow’’ (computed using
function BottomUp) both with and without the edge in question for each exemplar in the exemplar
set. Note that the original weights are saved and later restored at the end of the computation.

176

K
K
æ
æ
æ
æ
æ
æ
D
˘
˘
Z
K
(cid:220)
K
(cid:220)
(cid:220)
K
K
(cid:220)
K
K
BIAS DRIVEN REVISION

The high value of
R( Æ {popular-product, established-market, celebrity-endorsement}, Æ 1 æ

, C3, H

)

reﬂects the fact that without the clause C3, there is scant hope of a derivation of buy-stock for this
example. (Of course, in principle, both new-market and superior-ﬂavor might still be deleted from
the  body  of  clause C4,  thus  obviating  the  need  for C3,  but  the  high  weight  associated  with  the
literal new-market in C4 indicates that this is unlikely.)  The low values of
, C3, H

R( Æ {popular-product, established-market, superior-ﬂavor}, Æ 0 æ
R( Æ {popular-product, established-market, ecologically-correct}, Æ 0 æ

, C3, H

) and

)

reﬂect  the  fact  that  eliminating  the  clause C3 would  greatly  diminish  the  currently  undesirably
high  ﬂow through buy-stock (i.e.,  probability  of  a  derivation  of buy-stock) from  each  of  these
examples.

An interesting case to examine is that of
Æ {popular-product, unsafe-packaging, established-market}, Æ 0 æ

.

It is true that the elimination of C3 is helpful in preventing an unwanted derivation of buy-stock
because it prevents a derivation of increased-demand which is necessary for buy-stock in clause
C1.  Nevertheless, R correctly  reﬂects  the  fact  that  the  clause C3 is not destructive  for  this
exemplar  since  even in the  presence  of C3, buy-stock is  not  derivable  due  to  the  failure  of  the
literal ¬product-liability.

4.2.  Appending a Subtree

Let N be the set of examples for which e is needed for some root and let D be the set of examples
for which e is destructive  for some root (and not needed for any other root). Having found the
sets N and D, how do we repair e?

At  this  point,  if  the  set D is  non-empty  and  the  set N is  empty, we simply  delete  the  edge
from D
G . We justify  this  deletion  by  noting  that  no  exemplars  require e, so deletion  will  not
compromise the performance of the theory. On the other hand, if N is not empty, we apply some
inductive algorithm10 to produce a disjunctive normal form (DNF) logical expression constructed
from  observable  propositions  which  is  true  for  each  exemplar  in D but no exemplar  in N . We
reformulate this DNF expression as a conjunction of clauses by taking a single new literal l as the
head  of  each  clause,  and  using  each  conjunct  in  the  DNF  expression  as  the  body  of  one  of  the
clauses. This set of clauses is converted into dt-graph D n with l as its root. We then suture D n to e
by adding to D

G a new node t, an edge from e to t, and another edge from t to the root, l, of G n.

In  order  to  understand  why this  works,  ﬁrst  note  the  important  fact  that  (like every  other
subroutine of PTR), this method is essentially identical whether the edge, e, being repaired is a
clause edge, literal edge or negation edge. However, when translating back from dt-graph form to
domain theory form, the new node t will be interpreted differently depending on whether ne is a
clause or a literal. If ne is a literal, then t is interpreted as the clause ne ‹
l. If ne is a clause,

10 Any standard algorithm for constructing decision trees from positive and negative examples can be
used.  Our implementation  of  PTR  uses  ID3  (Quinlan, 1986).  The use  of  an  inductive  component  to  add
new substructure is due to Ourston and Mooney (Ourston & Mooney, in press).

177

æ
æ
æ
æ
KOPPEL, FELDMAN, & SEGRE

then t is interpreted as the negative literal ¬l.11

Now it is plain that those exemplars for which e is destructive will use the graph rooted at t to
overcome the effect of e. If ne is a literal which undesirably excludes E, then E will get by ne by
satisfying the clause t; if ne is a clause which undesirably allows E, then E will be stopped by the

, p æ : weighted dt-graph , Z

: set of exemplars, e: edge, l : real): weighted dt-graph;

function Revise( Æ
begin

then

Relevance( Æ

Æ N , D æ
if D „
begin
if N = ˘
else
begin
p(e) (cid:220)
l ;
l (cid:220) NewLiteral();

then p(e) (cid:220)

, p æ , Z

, e);

0;

ID3

= DTGraph(l, DNF-ID3(D, N ));
t (cid:220) NewNode(); D
, t);
if Clause?(ne) then Label(t) (cid:220) ¬l;
else Label(t) (cid:220) NewClause();

AddNode(D

AddEdge(D
AddEdge(D

, Æ ne, t æ ); p( Æ ne, t æ ) (cid:220)
, Æ t, Root(D

ID3) æ ); p( Æ t, Root(D

l ;

ID3; for e ˛

ID3 do p(e) (cid:220)

1;

ID3) æ ) (cid:220)

1;

end

end
return Æ
end

, p æ ;

Figure 7: Pseudo code for performing a revision. The function Revise takes a dt-graph, a set of ex-
emplars Z
, an edge to be revised e, and a parameter l as inputs and produces a revised dt-graph as
output.  The  function DNF-ID3 is  an  inductive  learning  algorithm  that  produces  a  DNF  formula
that accepts elements of D but not of N , while the function DTGraph produces a dt-graph with the
given root from the given DNF expression as described in the text.  For the sake of expository sim-
plicity, we hav e not shown the special cases in which ne is a leaf or e is a negation edge, as dis-
cussed in Footnote 11.

11 Of course, if we were willing to sacriﬁce some elegance, we could allow separate sub-routines for
the clause case and the literal case. This would allow us to make the dt-graphs to be sutured considerably
more compact. In particular, if ne is a literal we could suture the children of l in D
n directly to ne. If ne is a
clause, we could use the inductive algorithm to ﬁnd a DNF expression which excludes examples in D and
includes those in N (rather than the other way around as we now do it).  Translating this expression to a dt-
graph D with root l, we could suture D
G by simply adding an edge from the clause ne to the root l.
Moreover, if D
l1, . . . , lm then we can simply suture each of the leaf-nodes
l1, . . . , lm directly  to ne. Note  that  if ne is  a  leaf  or  a  negative  literal,  it  is  inappropriate  to  append  child
edges to ne. In such cases, we simply replace ne with a new literal l¢ and append to l¢ both D
n and the graph
of the clause l¢

n to D
n represents a single clause l ‹

ne.

178

D
(cid:220)
D
˘
D
(cid:220)
D
(cid:220)
D
(cid:220)
D
(cid:220)
D
¨
D
D
D
‹
BIAS DRIVEN REVISION

new literal t = ¬l.

Whenever a graph D n is sutured into D

G , we must assign weights to the edges of D n. Unlike
the original domain theory, howev er, the new substructure is really just an artifact of the inductive
algorithm  used  and  the  current  relevant  exemplar  set. For this  reason,  it  is  almost  certainly
inadvisable  to  try  to  revise  it  as  new exemplars  are  encountered. Instead,  we  would  prefer  that
this  new structure  be  removed and  replaced  with  a  more  appropriate  new construct  should  the
need arise. To ensure replacement instead of revision, we assign unit certainty factors to all edges
of the substructure. Since the internal edges of the new structure have  weights equal to 1, they
to the substructure root edge Æ ne, t æ ,
will never be revised.  Finally, we assign a default weight l
that connects the new component to the existing D
G and we reset the weight of the revised edge,
e, to the  same  value l . Figure  7  gives the  pseudo  code  for  performing  the  revision  step  just
described.

Consider our example from above. We are repairing the clause C3. We hav e already found

that the set D consists of the examples

{popular-product, established-market, superior-ﬂavor} and
{popular-product, established-market, ecologically-correct}

while the set N consists of the single example

{popular-product, established-market, celebrity-endorsement}.

Using  ID3  to  ﬁnd  a  formula  which  excludes N and  includes D, we obtain { ¬celebrity-
endorsement} which translates into the single clause, {l ‹ ¬celebrity-endorsement}. Translating
into dt-graph form and suturing (and simplifying using the technique of Footnote 11), we obtain
the dt-graph shown in Figure 8.

Observe now  that  the  domain  theory T

represented  by  this  dt-graph  correctly  classiﬁes  the

examples

{popular-product, established-market, superior-ﬂavor} and
{popular-product, established-market, ecologically-correct}

which were misclassiﬁed by the original domain theory T

.

5.  The PTR Algorithm

In this section we give the details of the control algorithm which puts the pieces of the previous
two sections together and determines termination.

5.1.  Control

The PTR algorithm is shown in Figure 9. We can brieﬂy summarize its operation as follows:

(1) 

PTR process  exemplars  in  random  order, updating  weights  and  performing  revisions
when necessary.

(2)  Whenever a revision is made, the domain theory which corresponds to the newly revised

graph is checked against all exemplars.

(3) 

PTR terminates if:
(i) All exemplars are correctly classiﬁed, or
(ii) Every edge in the newly revised graph has weight 1.

179

¢
KOPPEL, FELDMAN, & SEGRE

.999...

buy-stock

.998

C1

1.0

.95

increased-demand

¬product-liability

.98

.70

1.0

C4

C3

product-liability

.99

.15

.69

.70

.69

new-market

established-market

superior-ﬂavor

celebrity-endorsement

.89

C2

.96

.88

popular-product

unsafe-packaging

Figure  8:  The  weighted  dt-graph  of  Figure  2  after  revising  the  clause C3 (the  graph  has  been
slightly simpliﬁed in accordance with the remark in Footnote 11).

(4) 

(5) 

If, after  a  revision  is  made,  PTR  does  not  terminate,  then  it  continues  processing
exemplars in random order.

if, after  a  complete  cycle  of  exemplars  has  been  processed,  there  remain  misclassiﬁed
exemplars, then we
(i)
(ii)

Increment the revision threshold s so that s = min[s + d s , 1], and
Increment the value l assigned to a revised edge and to the root edge of an added
component, so that l = min[l + d l , 1].

(6)  Now we begin anew, processing the exemplars in (new) random order.

It  is  easy  to  see  that  PTR  is  guaranteed  to  terminate. The  argument  is  as  follows.  Within

max

1
d s

,

1
d l

cycles, both s and l will reach 1. At this point, every edge with weight less than

180

Ø
Œ
º
ø
œ
ß
BIAS DRIVEN REVISION

1 will be revised and will either be deleted or have its weight reset to l = 1. Moreover, any edges
added during revision will also be assigned certainty factor l = 1. Thus all edges will have weight
1 and the algorithm terminates by the termination criterion (ii).

Now,  we wish  to  show that  PTR  not  only  terminates,  but  that  it  terminates  with  every
exemplar correctly classiﬁed. That is, we wish to show that, in fact, termination criterion (ii) can
never be satisﬁed  unless  termination  criterion  (i)  is  satisﬁed  as  well. We  call  this  property
In Appendix  C  we  prove  that,  under  certain  very  general  conditions,  PTR  is
convergence.
guaranteed to converge.

5.2.  A Complete Example

Let us now review the example which we have been considering throughout this paper.

We begin with the ﬂawed domain theory and set of exemplars introduced in Section 1.
C1: buy-stock ‹
C2: product-liability ‹
C3: increased-demand ‹
C4: increased-demand ‹

popular-product (cid:217) established-market
new-market (cid:217) superior-ﬂavor.

increased-demand (cid:217) ¬product-liability

popular-product (cid:217) unsafe-packaging

T , p æ of  Figure  2,  assigning
We  translate  the  domain  theory  into  the  weighted  dt-graph Æ
weights via a combination of user-provided information and default values. For example, the user
has indicated that their conﬁdence in the ﬁrst literal (increased-demand) in the body of clause C1
is greater than their conﬁdence in the second literal ( ¬product-liability).

function PTR( Æ

Æ l

, p æ : weighted dt-graph,
0, s

0, d l , d s , e æ : ﬁve tuple of real): weighted dt-graph;

: set of exemplars,

such that G (E) „

(E) do

for E ˛ RandomlyPermute(Z ) do

u (cid:220) BottomUp( Æ

, p æ , E);

0;

begin
l (cid:220)
l
s
s
0;
while $ E ˛
begin

begin

, p æ
if $ e ˛
if ""
e ˛
end

l (cid:220) max[l + d l , 1];
(cid:220) max[s + d s , 1];
s
end

TopDown( Æ
such that p(e) £
s
, p(e) = 1 or "" E ˛

, p æ , u, E, e );
then Æ

, p æ
, G (E) = Q

Revise( Æ

, p æ , Z
(E) then return Æ

, e , l );
, p æ ;

end

Figure  9:  The  PTR  control  algorithm.  Input  to  the  algorithm  consists  of  a  weighted  dt-graph
0, d l , d s , and e . The algorithm
.

produces a revised weighted dt-graph whose implicit theory correctly classiﬁes all exemplars in Z

, and ﬁve real-valued parameters l

, p æ , a set of exemplars Z

0, s

181

D
D
Z
(cid:220)
Z
Q
D
Æ
D
(cid:220)
D
D
D
(cid:220)
D
D
Z
D
Æ
D
KOPPEL, FELDMAN, & SEGRE

We  set  the  revision  threshold s

initially  to  .7  and  their  respective
to . 03. We now  start updating the weights of the edges by processing the

to  .1,  the  reset  value l

increments d s and d l
exemplars in some random order.

We ﬁrst process the exemplar
Æ {unsafe-packaging, new-market}, Æ 1 æ

.

First, the leaves of the dt-graph are labeled according to their presence or absence in the exemplar.
Second, uE (e) values  (proof  ﬂow)  are  computed  for  all  edges  of  the  dt-graph  in  bottom  up
fashion. Next, vE (eri ) values are set to reﬂect the vector of correct classiﬁcations for the example
(E). New values for vE (e) are computed in top down fashion for each edge in the dt-graph. As
these  values  are  computed,  new values  for p(e) are  also  computed.  Processing  of  this  ﬁrst
exemplar produces the updated dt-graph shown in Figure 3.

Processing  of  exemplars  continues  until  either  an  edge  weight  falls  below s

(indicating  a
ﬂawed domain theory element has been located), a cycle (processing of all known exemplars) is
completed,  or  the  PTR  termination  conditions  are  met.  For  our  example,  after  processing  the
additional exemplars

Æ {popular-product, established-market, superior-ﬂavor}, Æ 0 æ
Æ {popular-product, established-market, ecologically-correct}, Æ 0 æ

and

the weight of the edge corresponding to clause C3 drops below s
this edge needs to be revised.

(see Figure 5), indicating that

We  proceed with the revision by using the heuristic in Section 4.2 in order to determine for
which set of exemplars the edge in question is needed and for which it is destructive. The edge
corresponding to the clause C3 is needed for

{ Æ {popular-product, established-market, celebrity-endorsement}, Æ 1 æ

æ }

and is destructive for

{ Æ {popular-product, established-market, ecologically-correct}, Æ 0 æ
Æ {popular-product, established-market, superior-ﬂavor}, Æ 0 æ

æ }.

,

Since  the  set  for  which  the  edge  is  needed  is  not  empty, PTR  chooses  to  append  a  subtree
weakening clause C3 rather than simply deleting the clause outright. Using these sets as input to
ID3, we determine that the fact celebrity-endorsement suitably discriminates between the needed
and destructive sets.  We then repair the graph to obtain the weighted dt-graph shown in Figure 8.
This graph corresponds to the theory in which the literal celebrity-endorsement has been added to
the body of C3.

We  now check the newly-obtained theory embodied in the dt-graph of Figure 8 (i.e., ignoring
weights)  against  all  the  exemplars  and  determine  that  there  are  still  misclassiﬁed  exemplars,
namely

Æ {unsafe-packaging, new-market}, Æ 1 æ
Æ {new-market, celebrity-endorsement}, Æ 1 æ

and

.

Thus, we continue processing the remaining exemplars in the original (random) order.

After processing the exemplars
Æ {popular-product, unsafe-packaging, established-market}, Æ 0 æ
,
Æ {popular-product, established-market, celebrity-endorsement}, Æ 1 æ
Æ {new-market, celebrity-endorsement}, Æ 1 æ

,

, and

182

æ
Q
æ
æ
æ
æ
æ
æ
æ
æ
BIAS DRIVEN REVISION

the weight of the edge corresponding to the literal superior-ﬂavor in clause C4 drops below the
revision threshold s . We then determine that this edge is not needed for any exemplar and thus
the edge is simply deleted.

At this point, no misclassiﬁed exemplars remain. The ﬁnal domain theory is:
C1: buy-stock ‹
C2: product-liability ‹
C3: increased-demand ‹
C4: increased-demand ‹

popular-product (cid:217) established-market (cid:217) celebrity-endorsement
new-market.

increased-demand (cid:217) ¬product-liability

popular-product (cid:217) unsafe-packaging

This theory correctly classiﬁes all known exemplars and PTR terminates.

6.  Experimental Evaluation

In  this  section  we  will  examine  experimental  evidence  that  illustrates  several  fundamental
hypotheses concerning PTR. We wish to show that:

(1) 

(2) 

theories produced by PTR are of high quality in three respects: they are of low radicality,
they are  of  reasonable  size,  and  they provide  accurate  information  regarding  exemplars
other than those used in the training.

PTR converges  rapidly  —  that  is,  it  requires  few cycles  to  ﬁnd  an  adequate  set  of
revisions.

(3)  well-chosen initial  weights  provided  by  a  domain  expert  can  signiﬁcantly  improve  the

performance of PTR.
More precisely, giv en a theory G

obtained by using PTR to revise a theory G on the basis of a

set of training examplars, we will test these hypotheses as follows.

Radicality. Our claim is that RadK (G

) is typically close to minimal over all theories which
, is known, we measure

. If this  value  is  less  than  1,  then  PTR  can  be  said  to  have  done  even ‘‘better’’ than

correctly classify all the examples.  For cases where the target theory, Q
RadK (G
RadK (Q
ﬁnding  the  target  theory  in  the  sense  that  it  was  able  to  correctly  classify  all  training  examples
using less radical revisions than those required to restore the target theory. If the value is greater
than 1, then PTR can be said to have ‘‘over-revised’’ the theory.

)
)

Cross-validation. We perform  one  hundred  repetitions  of  cross-validation  using  nested  sets
of training examples.  It should be noted that our actual objective  is to minimize radicality, and
that  often  there  are  theories  that  are  less  radical  than  the  target  theory  which  also  satisfy  all
training  examples.  Thus, while  cross-validation  gives some  indication  that  theory  revision  is
being successfully performed, it is not a primary objective of theory revision.

Theory  size. We count  the  number  of  clauses  and  literals  in  the  revised  theory  merely  to
demonstrate that theories obtained using PTR are comprehensible. Of course, the precise size of
the theory obtained by PTR is largely an artifact of the choice of inductive component.

Complexity. Processing a complete cycle of exemplars is O(n · d) where n is the number of
edges in the graph and d is the number of exemplars.  Likewise repairing an edge is O(n · d). We
will  measure  the  number  of  cycles  and  the  number  of  repairs  made  until  convergence.  (Recall

that the number of cycles until convergence is in any event bounded by max

show that, in practice, the number of cycles is small even if d s = d l = 0.

1
d s

,

1
d l

. We will

183

¢
¢
¢
Ø
Œ
º
ø
œ
ß
KOPPEL, FELDMAN, & SEGRE

leads to faster and more accurate results. For cases in which the target theory, Q
be the set of edges of D

Utility  of  Bias. We wish  to  show that  user-provided  guidance  in  choosing  initial  weights
, is known, let S
. Deﬁne
1
pb (e) such  that  for  each e ˛ S, 1 - pb (e) = (1 - p(e))
b .
That is, each edge which needs to be revised to obtain the intended theory has its initial weight
diminished and each edge which need not be revised to obtain the intended theory has its weight
increased.  Let K

G which need to be revised in order to restore the target theory Q
1
b and  for  each e ˛ / S, pb (e) = ( p(e))

æ . Then, for each b ,

b = Æ

G , pb

RadK

b (Q

) = -

log(

(1 - p(e))

1
b

e ˛ S

( p(e))

e ˛ / S

1

b ) = 1
b

RadK (Q

).

Here,  we  compare  the  results  of  cross-validation  and  number-of-cycles  experiments  for b = 2
with their unbiased counterparts (i.e., b = 1).

6.1.  Comparison with other Methods

In  order  to  put  our  results  in  perspective  we compare  them  with  results  obtained  by  other
methods.12
(1) 

ID3 (Quinlan, 1986) is  the  inductive  component  we  use  in  PTR. Thus  using  ID3  is
equivalent to learning directly from the examples without using the initial ﬂawed domain
theory. By comparing results obtained using ID3 with those obtained using PTR we can
gauge the usefulness of the given theory.

(2) 

EITHER (Ourston & Mooney, in press) uses enumeration of partial proofs in order to ﬁnd
a minimal  set  of  literals,  the  repair  of  which  will  satisfy  all  the  exemplars.  Repairs are
then  made  using  an  inductive  component.  EITHER is  exponential  in  the  size  of  the
It  also  cannot  handle
theory.
theories with multiple roots unless those roots are mutually exclusive.

It cannot  handle  theories  with  negated  internal  literals.

(3)  KBANN (Towell & Shavlik, 1993) translates a symbolic domain theory into a neural net,
uses  backpropagation  to  adjust  the  weights  of  the  net’s edges,  and  then  translates  back
from  net  form  to  partially  symbolic  form. Some  of  the  rules  in  the  theory  output  by
KBANN might be numerical, i.e., not strictly symbolic.

(4)  RAPTURE (Mahoney &  Mooney, 1993)  uses  a  variant  of  backpropagation  to  adjust
certainty factors in a probabilistic domain theory. If necessary, it can also add a clause to
a root.  All the rules produced by RAPTURE are numerical. Like EITHER, RAPTURE
cannot handle negated internal literals or multiple roots which are not mutually exclusive.

Observe that,  relative  to the  other  methods  considered  here,  PTR  is  liberal  in  terms  of  the
theories it can handle, in that (like KBANN, but unlike EITHER and RAPTURE) it can handle
negated literals and non-mutually exclusive multiple roots; it is also strict in terms of the theories
it yields in that (like EITHER, but unlike KBANN and RAPTURE) it produces strictly symbolic
theories.

12 There are other interesting theory revision algorithms, such as RTLS (Ginsberg, 1990), for which no

comparable data is available.

184

D
P
·
P
BIAS DRIVEN REVISION

We  hav e noted that both KBANN and RAPTURE output ‘‘numerical’’ rules.  In the case of
KBANN,  a  numerical  rule  is  one  which  ﬁres  if  the  sum  of  weights  associated  with  satisﬁed
antecedents exceeds a threshold. In the case of RAPTURE, the rules are probabilistic rules using
certainty factors along the lines of MYCIN (Buchanan & Shortliffe, 1984). One might ask, then,
to  what  extent  are  results  obtained  by  theory  revision  algorithms  which  output  numerical  rules
merely artifacts of the use of such numerical rules? In other words, can we separate the effects of
using numerical rules from the effects of learning?

To  make this  more  concrete,  consider  the  following  simple  method  for  transforming  a
symbolic domain theory into a probabilistic domain theory and then reclassifying examples using
the obtained probabilistic theory. Suppose we are given some possibly-ﬂawed domain theory G
.
Suppose further that we are not given the classiﬁcation of even a single example.  Assign a weight
p(e) to each edge of D
G according to the default scheme of Appendix A. Now, using the bottom-
up subroutine of the updating algorithm, compute uE (er ) for each test example E. (Recall that
uE (er ) is a measure of how close to a derivation of r from E there is, given the weighted dt-graph
G , p æ .)  Now,  for some chosen ‘‘cutoff’’ value 0 £ n £ 100, if E0 is such that uE0(er ) lies in
is true for E0; otherwise conclude

the upper n% of the set of values {uE (er )} then conclude that G
that G

is false for E0.

This  method,  which  for  the  purpose  of  discussion  we  call  PTR*,  does  not  use  any training
examples at all. Thus if the results of theory revision systems that employ numerical rules can be
matched  by  PTR*  — which performs  no  learning — then  it  is  clear  that  the  results  are  merely
artifacts of the use of numerical rules.

6.2.  Results on the PROMOTER Theory

We  ﬁrst consider the PROMOTER theory from molecular biology (Murphy & Aha, 1992), which
is  of  interest  solely  because  it  has  been  extensively  studied  in  the  theory  revision  literature
(Towell & Shavlik, 1993), thus enabling explicit performance comparison with other algorithms.
The PROMOTER theory is a ﬂawed theory intended to recognize promoters in DNA nucleotides.
The theory recognized none of a set of 106 examples as promoters despite the fact that precisely
half of them are indeed promoters.13

Unfortunately,

the  PROMOTER  theory  (like many others  used  in  the  theory  revision
literature) is trivial in that it is very shallow.  Moreover, it is atypical of ﬂawed domains in that it
is overly speciﬁc but not overly general. Given the shortcomings of the PROMOTER theory, we
will  also  test  PTR  on  a  synthetically-generated  theory  in  which  errors  have  been  artiﬁcially
introduced.  These synthetic  theories  are  signiﬁcantly  deeper  than  those  used  to  test  previous
methods.  Moreover,  the  fact  that  the  intended  theory  is  known  will  enable  us  to  perform
experiments involving radicality and bias.

13 In our experiments, we use the default initial weights assigned by the scheme of Appendix A. In ad-
dition, the clause whose head is the proposition contact is treated as a deﬁnition not subject to revision but
only deletion as a whole.

185

Æ
D
KOPPEL, FELDMAN, & SEGRE

6.2.1.  Cross-validation

In  Figure  10  we  compare  the  results  of  cross-validation  for  PROMOTER.  We distinguish
between  methods  which  use  numerical  rules  (top  plot)  and  those  which  are  purely  symbolic
(bottom plot).

The lower plot in Figure 10 highlights the fact that, using the value n = 50, PTR* achieves
better  accuracy, using  no  training  examples, than  any of the  methods  considered  here  achieve
using 90 training examples.  In particular, computing uE (er ) for each example, we obtain that of
the  53  highest-ranking  examples  50  are  indeed  promoters  (and,  therefore,  of  the  53  lowest-
ranking examples 50 are indeed non-promoters). Thus, PTR* achieves 94. 3% accuracy.  (In fact,
all  of  the  47  highest-ranking  examples  are  promoters  and  all  of  the  47  lowest-ranking  are  not
promoters.  Thus, a more  conservative  version  of  PTR*  which  classiﬁes  the,  say, 40%  highest-
ranking  examples  as  IN  and  the  40%  lowest-ranking  as  OUT, would  indeed  achieve  100%
accuracy over the examples for which it ventured a prediction.)

This merely shows that the original PROMOTER theory is very accurate provided that it is
given a numerical interpretation. Thus we conclude that the success of RAPTURE and KBANN
for this domain is not a consequence of learning from examples but rather an artifact of the use of
numerical rules.

As for the three methods — EITHER, PTR and ID3 — which yield symbolic rules, we see in
the  top  plot  of  Figure  10  that,  as  reported  in  (Ourston  &  Mooney, in press;  Towell  &
Shavlik, 1993), the  methods  which  exploit  the  given ﬂawed  theory  do  indeed  achieve  better
results on PROMOTER than ID3, which does not exploit the theory. Moreover, as the size of the
training set grows, the performance of PTR is increasingly better than that of EITHER.14

Finally, we wish to point out an interesting fact about the example set. There is a set of 13
out  of  the  106  examples  which  each  contain  information  substantially  different  than  that  in  the
rest  of  the  examples.  Experiments show that  using  ten-fold  cross-validation  on  the  93  ‘‘good’’
examples yields 99. 2% accuracy, while training on all 93 of these examples and testing on the 13
‘‘bad’’ examples yields below 40% accuracy.

6.2.2.  Theory size

The  size  of  the  output  theory  is  an  important  measure  of  the  comprehensibility  of  the  output
theory. Ideally, the  size  of  the  theory  should  not  grow too  rapidly  as  the  number  of  training
examples  is  increased,  as  larger  theories  are  necessarily  harder  to  interpret. This  observation
holds  both  for  the  number  of  clauses  in  the  theory  as  well  as  for  the  average  number  of
antecedents in each of those clauses.

Theory  sizes  for  the  theories  produced  by  PTR  are  shown  in  Figure  11. The  most  striking
aspect  of  these  numbers  is  that  all  measures  of  theory  size  are  relatively  stable  with  respect  to
training  set  size. Naturally, the  exact  values  are  to  a  large  degree  an  artifact  of  the  inductive
learning  component  used. In  contrast,  for  EITHER,  theory  size  increases  with  training  set  size

14 Those  readers  familiar  with  the  PROMOTER  theory  should  note  that  the  improvement  over EI-
THER is a consequence of PTR repairing one ﬂaw at a time and using a sharper relevance criterion. This
results in PTR always deleting the extraneous conformation literal, while EITHER occasionallly fails to do
so, particularly as the number of training exmaple increases.

186

BIAS DRIVEN REVISION

ID3
EITHER
PTR

20

40

60

80

100

# Training Exemplars

RAPTURE
KBANN
PTR*

20

40

60

80

100

# Training Exemplars

60

% Misclassiﬁed

50

40

30

20

10

0

0

60

% Misclassiﬁed

50

40

30

20

10

0

0

Figure  10:  PROMOTER:  Error  rates  using  nested  training  sets  for  purely  symbolic  theories  (top
plot) and numeric theories (bottom plot). Results for EITHER, RAPTURE, and KBANN are taken
from (Mahoney & Mooney, 1993), while results for ID3 and PTR were generated using similar ex-
perimental procedures. Recall that PTR* is a non-learning numerical rule system; the PTR* line is
extended horizontally for clarity.

187

KOPPEL, FELDMAN, & SEGRE

Training 
Set Size

Mean
Clauses in
Output 

Mean 
Literals in
Output

Mean
Mean
Revisions to
Exemplars to
Convergence  Convergence

Original
Theory

20 
40 
60 
80 
100 

14 

11
11
11
11
12

83

39 
36 
35 
32 
36 

10.7
15.2
18.2
22.1
22.0

88
140
186
232
236

Figure 11: PROMOTER: Results. Numbers reported for each training set size are average values
over one hundred trials (ten trials for each of ten example partitions).

(Ourston, 1991). For example,  for  20  training  examples  the  output  theory  size  (clauses  plus
literals) is 78, while for 80 training examples, the output theory size is 106.

Unfortunately, making  direct  comparisons  with  KBANN  or  RAPTURE  is  difﬁcult.  In  the
case  of  KBANN  and  RAPTURE,  which  allow numerical  rules,  comparison  is  impossible  given
the  differences  in  the  underlying  representation  languages. Nevertheless,  it  is  clear  that,  as
expected,  KBANN  produces  signiﬁcantly  larger  theories  than  PTR. For example,  using  90
training examples from the PROMOTER theory, KBANN produces numerical theories with, on
av erage,  10  clauses  and  102  literals  (Towell  &  Shavlik, 1993). These  numbers  would  grow
substantially if the theory were converted into strictly symbolic terms. RAPTURE, on the other
hand,  does  not  change  the  theory  size,  but,  like KBANN,  yields  numerical  rules  (Mahoney &
Mooney, 1993).

6.2.3.  Complexity

EITHER  is  exponential  in  the  size  of  the  theory  and  the  number  of  training  examples.  For
KBANN,  each  cycle  of  the  training-by-backpropagation  subroutine  is O(d · n) (where d is  the
size of the network and n is the number of exemplars), and the number of such cycles typically
numbers in the hundreds even for shallow nets.

Like backpropagation, the cost of processing an example with PTR is linear in the size of the
theory. In contrast, however, PTR typically converges after processing only a tiny fraction of the
number  of  examples  required  by  standard  backpropagation  techniques. Figure  11  shows  the
av erage number of exemplars (not cycles!) processed by PTR until convergence as a function of
training set size. The only other cost incurred by PTR is that of revising the theory. Each such
revision in O(d · n). The average number of revisions to convergence is also shown in Figure 11.

6.3.  Results on Synthetic Theories

The  character  of  the  PROMOTER  theory  make it less  than  ideal  for  testing  theory  revision
algorithms.  We wish to consider theories which (i) are deeper, which (ii) make substantial use of
negated internal literals and which (iii) are overly general as well as overly speciﬁc. As opposed
to shallow theories which can generally be easily repaired at the leaf level, deeper theories often

188

BIAS DRIVEN REVISION

require repairs at internal levels of the theory. Therefore, a theory revision algorithm which may
perform well on shallow theories will not necessarily scale up well to larger theories. Moreover,
as  theory  size  increases,  the  computational  complexity  of  an  algorithm  might  preclude  its
application altogether. We wish to show that PTR scales well to larger, deeper theories.

Since  deeper, propositional,  real-world  theories  are  scarce,  we  have  generated  them
synthetically. As an added bonus, we now know the target theory so we can perform controlled
In (Feldman, 1993) the  aggregate  results  of  experiments
experiments  on  bias  and  radicality.
performed  on  a  collection  of  synthetic  theories  are  reported.
In  order  to  avoid  the  dubious
practice of averaging results over different theories and in order to highlight signiﬁcant features of
a particular application of PTR, we consider here one synthetic theory typical of those studied in
(Feldman, 1993).

E, F
p0, ¬G, p1, p2, p3

r ‹
A, B
r ‹ C, ¬D 
A ‹
A ‹
B ‹ ¬ p0
B ‹
p1, ¬H 
B ‹
p4, ¬ p11
C ‹
I , J
C ‹
p2, ¬K 
C ‹ ¬ p8, ¬ p9
D ‹
p10, ¬ p12, L
D ‹
p3, ¬ p9, ¬M 
E ‹
N , p5, p6
E ‹ ¬O, ¬ p7, ¬ p8
F ‹
p4
F ‹ Q, ¬R 
G ‹
S, ¬ p3, p8
G ‹ ¬ p10, p12
H ‹ U, V
H ‹
I ‹ W
I ‹
p6
J ‹
X, p5
J ‹
Y
K ‹
P, ¬ p5, p9
K ‹ ¬ p6, p9

p1, p2; p3, p4

p3, p4, p6
p10, ¬ p12
p2, p3

T , p1
p2, p12, p16
Z , ¬ p17
p18, ¬ p19

L ‹
L ‹
M ‹
M ‹
N ‹ ¬ p0, p1
N ‹
N ‹
Z ‹
Z ‹ ¬ p2, p3, p17, p18, p20
O ‹ ¬ p3, p4, p5, p11, ¬ p12
O ‹ ¬ p13, p18
Y ‹
p4, p5 p6
P ‹ ¬ p6, p7, p8
X ‹
p7, p9
Q ‹
p0, p4
Q ‹
p3, ¬ p13, p14, p15
W ‹
p10, p11
W ‹
p3, p9
R ‹
p12, ¬ p13, p14
V ‹ ¬ p14, p15
S ‹
U ‹
U ‹
T ‹
T ‹ ¬ p7, p8, p9, ¬ p16, ¬ p17, ¬ p18

p3, p6, ¬ p14, p15, p16
p11, p12
p13, p14, ¬ p15, ¬ p16, ¬ p17
p7

Figure 12: The synthetic domain theory Q used for the experiments of Section 6.

189

KOPPEL, FELDMAN, & SEGRE

The theory Q

is shown is Figure 12. Observe that Q

includes four levels of clauses and has
many neg ated  internal  nodes. It  is  thus  substantially  deeper  than  theories  considered  before  in
testing  theory  revision  algorithms. We  artiﬁcially  introduce,  in  succession,  15  errors  into  the
theory Q
. The errors are shown in Figure 13. For each of these theories, we use the default initial
weights assigned by the scheme of Appendix A.

Let G

i (Q

), of Q

i be  the  theory  obtained  after  introducing  the  ﬁrst i of  these  errors. In  Figure  14  we
i for i = 3, 6, 9, 12, 15,
relative to each of the ﬂawed theories, G
show the radicality, RadG
as well as the number of examples misclassiﬁed by each of those theories. Note that, in general,
the  number  of  misclassiﬁed  examples  cannot  necessarily  be  assumed  to  increase  monotonically
with  the  number  of  errors  introduced  since  introducing  an  error  may  either  generalize  or
specialize  the  theory. For  example,  the  fourth  error  introduced  is  ‘‘undone’’ by the  ﬁfth  error.
Nevertheless,  it  is  the  case  that  for  this  particular  set  of  errors,  each  successive  theory  is  more
radical and misclassiﬁes a larger number of examples with respect to Q

.

To  measure radicality and accuracy,  we choose 200 exemplars which are classiﬁed according
to Q
i (i = 3, 6, 9, 12, 15), we withhold 100 test examples and train on nested sets
. Now for each G
of 20, 40, 60, 80 and 100 training examples.  We choose ten such partitions and run ten trials for
each partition.

In Figure 15, we graph the average value of

RadG
RadG

)
)

, where G

is the theory produced by

PTR.  As can be seen, this value is consistently below 1. This indicates that the revisions found

i (G
i (Q

p4, ¬ p11

p4, ¬ p6, ¬ p11

p8, ¬ p15

Added clause A ‹ ¬ p6
Added clause S ‹ ¬ p5
Added clause A ‹
Added literal ¬ p6 to clause B ‹
Deleted clause B ‹
Added clause D ‹ ¬ p14
Added clause G ‹ ¬ p12, p8
Added literal p2 to clause A ‹
Added clause L ‹

1
2
3
4
5
6
7
8
9
10  Added clause M ‹ ¬ p13, ¬ p7
11  Deleted clause Q ‹
12  Deleted clause L ‹
13  Added clause J ‹
14  Deleted literal p4 from clause F ‹
15  Deleted literal p1 from clause B ‹

p16

p3, ¬ p13, p14, p15
p2, p12, p16
p11

E, F

p4
p1, ¬H

Figure 13: The errors introduced into the synthetic theory Q
thetic theories G

i. Note that the ﬁfth randomly-generated error obviates the fourth.

in order to produce the ﬂawed syn-

190

¢
¢
BIAS DRIVEN REVISION

Number of Errors

G 3

3

G 6

6

G 9

 9

G 12

G 15

12 

15

Rad(Q

)

7.32 

17.53

22.66 

27.15 

33.60

Misclassiﬁed IN
Misclassiﬁed OUT

0
50 

26
45

34
45 

34
46 

27
64

Initial Accuracy

75% 

64.5% 60.5% 

60% 

54.5%

Figure 14: Descriptive statistics for the ﬂawed synthetic theories G

i (i = 3, 6, 9, 12, 15).

1

Normalized
Radicality

0.8

0.6

0.4

0.2

0

G 15
G 12
G 9
G 6
G 3

20

40

60

80

100

# Training Exemplars

Figure 15: The normalized radicality,

, for the output theories G

produced by PTR from

i (i = 3, 6, 9, 12, 15). Error bars reﬂect 1 standard error.

RadG
RadG

i (G
i (Q

)
)

by  PTR  are  less  radical  than  what  is  needed  to  restore  the  original Q
. Thus  by  the  criterion  of
success that PTR set for itself, minimizing radicality, PTR does better than restoring Q
. As is to
be expected, the larger the training set the closer this value is to 1. Also note that as the number
of errors introduced increases, the saving in radicality achieved by PTR increases as well, since a
larger number of opportunities are created for more parsimonious revision.  More precisely, the

191

¢
¢
G
KOPPEL, FELDMAN, & SEGRE

av erage number of revisions made by PTR to G 3, G 6, G 9, G 12, and G 15 with a 100 element training
set are 1.4, 4.1, 7.6, 8.3, and 10.4, respectively.

An example will show how PTR achieves this. Note from Figure 13 that the errors introduced

in G 3 are the additions of the rules:

A ‹ ¬ p6
S ‹ ¬ p5
S ‹

p8, ¬ p15.

In most cases, PTR quickly locates the extraneous clause A ‹ ¬ p6, and discovers that deleting it
results  in  the  correct  classiﬁcation  of  all  exemplars  in  the  training  set.  In  fact,  this  change  also
results in the correct classiﬁcation of all test examples as well. The other two added rules do not
affect  the  classiﬁcation  of  any training  examples,  and  therefore  are  not  deleted  or  repaired  by
PTR. Thus the radicality of the changes made by PTR is lower than that required for restoring the
original theory. In a minority of cases, PTR ﬁrst deletes the clause B ‹ ¬ p0 and only then deletes
the clause A ‹
p6. Since the literal B is higher in the tree than the literal S, the radicality of these
changes is marginally higher that that required to restore the original theory.

In Figure 16, we graph the accuracy of G

on the test set. As expected, accuracy degenerates
somewhat as the number of errors is increased. Nevertheless, even for G 15, PTR yields theories
which generalize accurately.

Figure  17  shows  the  average  number  of  exemplars  required  for  convergence.  As expected,
the fewer errors in the theory, the fewer exemplars PTR requires for convergence. Moreover,  the

60

% Misclassiﬁed

50

40

30

20

10

0

0

G 15
G 12
G 9
G 6
G 3

20

40

60

80

100

# Training Exemplars

Figure 16: Error rates for the output theories produced by PTR from G

i (i = 3, 6, 9, 12, 15).

192

¢
BIAS DRIVEN REVISION

G 15
G 12

G 9G 6G 3

20

40

60

300

Exemplars to
Convergence

250

200

150

100

50

0

0

80

100

# Training Exemplars

Figure 17: Number of exemplars processed until convergence for G

i (i = 3, 6, 9, 12, 15).

number of exemplars processed grows less than linearly with the training set size. In fact, in no
case was the average number of examples processed greater than 4 times the training set size. In
comparison, backpropagation typically requires hundreds of cycles when it converges.

Next we wish show the effects of positive  bias, i.e., to show that user-provided guidance in
the choice of initial weights can improve speed of convergence and accuracy in cross-validation.
For each  of  the  ﬂawed  theories G 3 and G 15, we compare  the  performance  of  PTR  using  default
initial  weights  and  biased  initial  weights  (b = 2).  In Figure  18,  we  show how  cross-validation
accuracy increases when bias is introduced. In Figure 19, we show how the number of examples
which need to be processed until convergence decreases when bias is introduced.

Returning  to  the  example  above, we see  that  the  introduction  of  bias  allows  PTR  to
immediately  ﬁnd  the  ﬂawed  clause A ‹
p6 and  to  delete  it  straight  away.  In fact,  PTR  never
requires the processing of more than 8 exemplars to do so. Thus, in this case, the introduction of
bias  both  speeds  up  the  revision  process  and  results  in  the  consistent  choice  of  the  optimal
revision.

Moreover,  it has  also  been  shown  in  (Feldman, 1993)  that  PTR  is  robust  with  respect  to
random perturbations in the initial weights. In particular, in tests on thirty different synthetically-
generated  theories,  introducing  small  random  perturbations  to  each  edge  of  a  dt-graph  before
training resulted in less than 2% of test examples being classiﬁed differently than when training
was performed using the original initial weights.

193

KOPPEL, FELDMAN, & SEGRE

G 15
G 3
G 15 + bias
G 3 + bias

20

40

60

80

100

# Training Exemplars

60

% Misclassiﬁed

50

40

30

20

10

0

0

Figure 18: Error rates for the output theories produced by PTR from G
favorably-biased initial weights.

i (i = 3, 6, 9, 12, 15), using

6.4.  Summary

Repairing internal literals and clauses is as natural for PTR as repairing leaves.  Moreover,  PTR
converges rapidly. As a result, PTR scales up to deep theories without difﬁculty. Even for very
badly  ﬂawed  theories,  PTR  quickly  ﬁnds  repairs  which  correctly  classify  all  known  exemplars.
These repairs are typically less radical than restoring the original theory and are close enough to
the original theory to generalize accurately to test examples.

Moreover,  although PTR is robust with respect to initial weights, user guidance in choosing
these weights can signiﬁcantly improve both speed of convergence and cross-validation accuracy.

7.  Conclusions

In  this  paper, we hav e presented  our  approach,  called  PTR,  to  the  theory  revision  problem  for
propositional theories. Our approach uses probabilities associated with domain theory elements
to numerically track the ‘‘ﬂow’’ of proof through the theory, allowing us to efﬁciently locate and
repair ﬂawed elements of the theory. We prove  that PTR converges to a theory which correctly
classiﬁes  all  examples,  and  show experimentally  that  PTR  is  fast  and  accurate  even for  deep
theories.

There are several ways in which PTR can be extended.

First-order  theories. The  updating  method  at  the  core  of  PTR  assumes  that  provided
exemplars  unambiguously  assign  truth  values  to  each  observable  proposition.
In  ﬁrst-order
theory  revision  the  truth  of  an  observable  predicate  typically  depends  on  variable  assignments.

194

BIAS DRIVEN REVISION

G 15
G 15 + bias
G 3G 3 + bias

20

40

60

300

Exemplars to
Convergence

250

200

150

100

50

0

0

80

100

# Training Exemplars

Figure  19:  Number  of  exemplars  processed  until  convergence  using  favorably-biased  initial
weights.

Thus, in order to apply PTR to ﬁrst-order theory revision it is necessary to determine ‘‘optimal’’
variable assignments on the basis of which probabilities can be updated. One method for doing so
is discussed in (Feldman, 1993).

Inductive bias. PTR uses bias to locate ﬂawed elements of a theory. Another type of bias can
be used to determine which revision to make.  For example, it might be known that a particular
clause might be missing a literal in its body but should under no circumstances be deleted, or that
only  certain  types  of  literals  can  be  added  to  the  clause  but  not  others. Likewise,  it  might  be
known that a particular literal is replaceable but not deletable, etc. It has been shown (Feldman et
al., 1993)  that  by  modifying  the  inductive  component  of  PTR  to  account  for  such  bias,  both
convergence speed and cross-validation accuracy are substantially improved.

Noisy  exemplars. We hav e assumed  that  it  is  only  the  domain  theory  which  is  in  need  of
revision, but that the exemplars are all correctly classiﬁed. Often this is not the case. Thus, it is
necessary  to  modify  PTR  to  take into  account  the  possibility  of  reclassifying  exemplars  on  the
basis  of  the  theory  rather  than  vice-versa.  The PTR*  algorithm  (Section  6)  suggests  that
misclassed  exemplars  can  sometimes  be  detected  before  processing. Brieﬂy, the  idea  is  that  an
example which allows multiple proofs of some root is almost certainly IN for that root regardless
of the classiﬁcation we have been told. Thus, if uE (er ) is high, then E is probably IN regardless
of  what  we  are  told;  analogously, if uE (er ) is low. A modiﬁed  version  of  PTR  based  on  this
observation has already been successfully implemented (Koppel et al., 1993).

In  conclusion,  we  believe  the  PTR  system  marks  an  important  contribution  to  the  domain

theory revision problem. More speciﬁcally, the primary innovations reported here are:

195

KOPPEL, FELDMAN, & SEGRE

(1)  By assigning  bias  in  the  form  of  the  probability  that  an  element  of  a  domain  theory  is

ﬂawed, we can clearly deﬁne the objective of a theory revision algorithm.

(2)  By reformulating a domain theory as a weighted dt-graph, we can numerically trace the

ﬂow of a proof or refutation through the various elements of a domain theory.

(3) 

Proof ﬂow can be used to efﬁciently update the probability that an element is ﬂawed on
the basis of an exemplar.

(4)  By updating  probabilities  on  the  basis  of  exemplars,  we  can  efﬁciently  locate  ﬂawed

elements of a theory.

(5)  By using proof ﬂow, we can determine precisely on the basis of which exemplars to revise

a ﬂawed element of the theory.

Acknowledgments

The authors wish to thank Hillel Walters of Bar-Ilan University for his signiﬁcant contributions to
the  content  of  this  paper. The  authors  also  wish  to  thank  the  JAIR  reviewers  for  their
exceptionally prompt and helpful remarks. Support for this research was provided in part by the
Ofﬁce of Naval Research through grant N00014-90-J-1542 (AMS, RF) and the Air Force Ofﬁce
of Scientiﬁc Research under contract F30602-93-C-0018 (AMS).

196

BIAS DRIVEN REVISION

Appendix A: Assigning Initial Weights

In  this  appendix  we  give  one  method  for  assigning  initial  weights  to  the  elements  of  a  domain
theory. The  method  is  based  on  the  topology  of  the  domain  theory  and  assumes  that  no  user-
provided  information  regarding  the  likelihood  of  errors  is  available.  If such  information  is
available, then it can be used to override the values determined by this method.

The method works as follows.  First, for each edge e in D

G we deﬁne the ‘‘semantic impact’’
(e) is meant to signify the proportion of examples whose classiﬁcation is directly

(e). M

of e, M
affected by the presence of e in D

G .

One  straightforward  way  of  formally  deﬁning M
G , I æ

I be  the  pair
such  that I assigns  all  root  and  negation  edges  the  weight  1  and  all  other  edges  the

(e) is the  following.  Let K

1
2

. Let I (e) be identical to I except that e and all its ancestor edges have been assigned

weight
the weight 1. Let E be the example such that for each observable proposition P in G
, E(P) is the
a priori probability that P is true in a randomly selected example.15 In particular, for the typical
case in which observable propositions are Boolean and all example are equiprobable, E(P) = 1
2
E can be thought of as the ‘‘average’’ example.  Then, if no edge of D
G has more than one parent-
edge, we formally deﬁne the semantic signiﬁcance, M

(e), of an edge e in D

G as follows:

.

(e) = u

E

I (e)

(er ) - u

e

E

I (e)

(er ).

That is, M

(e) is the difference of the ﬂow of E through the root r, with and without the edge e.

(e) can be efﬁciently computed by ﬁrst computing u
G , and then computing M

E (e) for every edge e in a
(e) for every edge e in a single top-down

I

Note that M

single bottom-up traversal of D
traversal of D

G , as follows:
For a root edge r, M

(1) 

(2) 

(r) = 1 - u

I

E (r).
( f (e)) · 2(1 - u

I

E (e))

For all other edges, M

(e) = M

uK
G has more than one parent-edge then we deﬁne M

E (e)

I

If some edge in D
using this method of computation, where in place of M

, where f (e) is the parent edge of e.

(e) for an edge by

( f (e)) we use

max
f

( f (e))

ß .

Finally, for a set, R, of edges in G, we deﬁne M

(R) =

(e).16

e ˛ R

Now,  having  computed M

(e) we compute  the  initial  weight  assignment  to e, p(e),  in  the

following way. Choose some large C.17 For each e in D

G deﬁne:

15 Although we have deﬁned an example as a {0, 1} truth assignment to each observable proposition,
we have  already noted in Footnote 4 that we can just as easily process examples which assign to observ-
ables any value in the interval [0, 1].

16 Observe that the number of examples reclassiﬁed as a result of edge-deletion is, in fact, superaddi-

tive, a fact not reﬂected by this last deﬁnition.

17 We hav e not tested how to choose C ‘‘optimally.’’ In the experiments reported in Section 6, the val-

ue C = 106 was used.

197

Æ
D
M
K
K
K
K
K
Ø
º
M
ø
S
M
KOPPEL, FELDMAN, & SEGRE

(e)

.

(e) + 1

p(e) = C
CM
Now, reg ardless of how M
(e) is
the following: for such an initial assignment, p, if two sets of edges Æ
G , p æ are of equal total
strength  then  as  revision  sets  they are  of equal  radicality. This  means  that  all  revision  sets  of
equal strength are a priori equally probable.

(e) is deﬁned, the virtue of this method of computing p(e) from M

For a set of edges of D

G , deﬁne

S(e) =

1 if e ˛ S
0 if e ˛ / S

Then the above can be formalized as follows:

Theorem A1: If R and S are sets of elements of G
follows that Rad(R) = Rad(S).
Proof  of  Theorem  A1: Let R and S be  sets  of  edges  such  that M
Recall that

such that M

(R) = M

(S) then it

(R) = M

(S).

Rad(S) = -

log

Then

exp(- Rad(S))
exp(- Rad(R))

[1 - p(e)]S(e) ·

[ p(e)]1- S(e)(cid:246)

ł .

[1 - p(e)]S(e) · p(e)1- S(e)
[1 - p(e)]R(e) · p(e)1- R(e)

R(e)- S(e)

º p(e)1 - p(e)

R(e)- S(e)

(e)ø

º C

e ˛

=

=

=

e ˛

e ˛

e ˛

= C

(R)-

(S) = 1.

It follows immediately that Rad(R) = Rad(S).
A simple  consequence  which  illustrates  the  intuitiveness  of  this  theorem  is  the  following:
suppose  we  have  two  possible  revisions  of D
, each  of  which  entails  deleting  a  simple  literal.
Suppose further that one literal, l1, is deep in the tree and the other, l2, is higher in the tree so that
the  radicality  of

(l1).  Then, using  default  initial  weights  as  assigned  above,

(l2) = 4 ·

deleting l2 is 4 times as great as the radicality of deleting l1.

198

M
D
(cid:236)
(cid:237)
(cid:238)
(cid:230)
Ł
D
P
D
P
D
P
Ø
ø
ß
D
P
Ø
M
ß
M
M
M
M
BIAS DRIVEN REVISION

Appendix B: Updated Weights as Conditional Probabilities

In  this  appendix  we  prove  that  under  certain  limiting  conditions,  the  algorithm  computes  the
conditional probabilities of the edges given the classiﬁcation of the example.

Our ﬁrst assumption for the purpose of this appendix is that the correct dt-graph D

G . This means that for every node n in D

G , p(e) = w(e)).  A pair Æ

is known
G , w(n) = 1 (and,
G , w æ with this property is said to

to be a subgraph of the given dt-graph D
consequently, for every edge e in D
be deletion-only.

Although  we  informally  deﬁned  probabilities  directly  on  edges,  for  the  purposes  of  this
G . That is,

appendix we formally deﬁne our probability function on the space of all subgraphs of D
Q = D
the elementary events are of the form D
¢ }.
is simply

G . Then the probability that e ˛

¢ where D

Q = D

{ p(D

)|e ˛

¢ ˝ G

We  say  that  a  deletion-only, weighted  dt-graph Æ

G , p æ

is edge-independent if  for  any

,

¢ ˝ G

p(D

Q = D

) =

p(e) ·

1 - p(e).

e ˛
Finally, we say that D
any dt-graph which is connected and tree-like has only one root.

is tree-like if no edge e ˛

e ˛ / D

G has more than one parent-edge. Observe that

We will prove results for deletion-only, edge-independent, tree-like weighted dt-graphs.18
First we introduce some more terminology. Recall that every node in D

and that by deﬁnition, this literal is true if not all of its children in D

is labeled by one of
ˆG are true.
. A literal l in
to be true, given the set of equations ˆG
and the example E, if l appears in
and E. (This follows from the deﬁnition of NAND.)  Thus we say that an

represents the sets of NAND equations, ˆG
¢ ˝

the literals in ˆG
Recall also that the dt-graph D
ˆG
ˆG
edge e in D

forces its parent in ˆG
and is false given ˆG

if e ˛

ˆG

is used by E in D
If e is not used by E in D
Note that, given the probabilities of the elementary events D

|– E ¬ne.
E (e). Note that N

¢ we write N

and ˆG

E (er ) if and only if G

(E) = 1.

¢ = D

, the probability p(N

theory Q

E (e))
is  simply

that 

the  edge

e

is  not  used  by E in 

the 

target  domain 

p(D

¢ = D

¢ ˝ G

Q )|N

E (e)

. Where there is no ambiguity we will use N E (e) to refer to N

E (e).

G , w æ

is a deletion-only, edge-independent, tree-like weighted

Theorem B1: If Æ
dt-graph, then for every edge e in D
Proof  of  Theorem  B1: We use  induction  on  the  distance  of ne from  its  deepest
descendant.  If ne is  an  observable  proposition P then e is  used  by E in Q
precisely if e ˛
and P is false in E. Thus the probability that e is not used by E
[1 - E(P)] = uE (e).
in Q

G , uE (e) = p(N E (e)).

is [1 - p(e)] ·

18 Empirical results show that our algorithm yields reasonable approximations of the conditional prob-

abilities even when these conditions do not hold.

199

Q
D
G
G
¢
˝
D
D
Q
G
S
G
¢
D
G
D
G
G
¢
D
G
¢
P
G
¢
P
G
D
G
G
¢
˝
D
G
¢
¢
¢
G
G
¢
D
G
¢
¢
G
G
¢
G
¢
¢
G
Q
Q
G
S
(cid:236)
(cid:237)
(cid:238)
G
G
¢
(cid:252)
(cid:253)
(cid:254)
Q
D
Q
KOPPEL, FELDMAN, & SEGRE

If ne is  not  a  observable  proposition  then ˆQ

|– E ¬ne precisely  if  all  its

, that is, if all its children are unused in ˆQ

. But then

children in ˆQ

are true in ˆQ
p(N E (e)) = p(e) · p(Q

|– E ¬ne)

(edge independence)

(induction hypothesis)

p(N E (s))

s ˛

children(e)

uE (s)

s ˛

children(e)

= p(e) ·

= p(e) ·

= uE (e).

This justiﬁes the bottom-up part of the algorithm. In order to justify the top-down part we need
one more deﬁnition.
Let p(e| Æ E, Q
(E) æ . Then

(E) æ ) be the  probability  that e ˛

and  the  exemplar

given Æ

G , p æ

Æ E, Q

p(e| Æ E, Q

(E) æ ) = G

¢ ˝ G

{ p(D

Q = D

)|e ˛

, Q

(E) = G

¢ (E)}

{ p(D

Q = D

)|Q

(E) = G

¢ (E)}

.

¢ ˝ G

Now we hav e

Theorem B2: If Æ
ev ery edge e in D

G , w æ

is deletion-only, edge-independent and tree-like, then for

G , pnew(e) = p(e| Æ E, Q

(E) æ ).

In order to prove the theorem we need several lemmas:

Lemma B1: For every example E and every edge e in D

p( ¬N E (e)) = p( ¬N E (e), N E ( f (e))) = p( ¬N E (e)|N E ( f (e))) · p(N E ( f (e))).

This follows immediately from the fact that if an edge, e, is used, then its parent-edge, f (e), is not
used.

Lemma B2: For every example E and every edge e in D

G ,
(E) æ ) = p(N E (e)|N E ( f (e))).

p(N E (E)|N E ( f (e)), Æ E, Q

(E) æ

This  lemma  states  that N E (e) and Æ E, Q
are  conditionally  independent  given N E ( f (e))
(E) æ
(Pearl, 1988). That  is,  once N E ( f (e))  is  known,
adds  no  information  regarding
N E (e). This is immediate from the fact that p( Æ E, Q
|N E ( f (e))) can be expressed in terms of
the  probabilities  associated  with  non-descendants  of f (e),  while p(N E (e))  can  be  expressed  in
terms of the probabilities associated with descendants of r(e).
Lemma B3: For every example E and every edge e in D

Æ E, Q
(E) æ

G ,

vE (e) = p(N E (e)| Æ E, Q

(E) æ ).

Proof of Lemma B3: The proof is by induction on the depth of the edge, e. For
the root edge, er , we hav e

200

P
P
D
Q
D
S
G
¢
D
G
¢
G
S
G
¢
D
G
BIAS DRIVEN REVISION

vE (er ) = Q

(E) = p(Q

(E) = 1| Æ E, Q

(E) æ ) = p(N E (er )| Æ E, Q

(E) æ ).

Assuming  that  the  theorem  is  known  for f (e),  we  show that  it  holds  for e as
follows:

1 - vE (e) = Ø

º 1 - uE (e)

vE ( f (e))
uE ( f (e))

= p( ¬N E (e)) ·

vE ( f (e))
p(N E ( f (e))

= p(N E (e)| Æ E, Q

(E) æ ) ·

p( ¬N E (e))
p(N E ( f (e))

= p(N E (e)| Æ E, Q

(E) æ ) · p( ¬N E (e)|N E ( f (e))

= p(N E (e)| Æ E, Q

(E) æ )

· p( ¬N E (e)|N E ( f (e)), Æ E, Q

(E) æ )

= p( ¬N E (e), N E ( f (e))| Æ E, Q

(E) æ )

= p( ¬N E (e)| Æ E, Q

(E) æ )

= 1 - p(N E (e)| Æ E, Q

(E) æ ).

(deﬁnition of v)

(Theorem B1)

(induction hypothesis)

(Lemma B1)

(Lemma B2)

(Bayes rule)

(Lemma B1)

Let ¬e be short for the event e ˛ / D

. Then we have

Lemma B4: For every example E and every edge e in D

G ,
p( ¬e) = p( ¬e, ¬N E (e)) = p( ¬e|N E (e)) · p(N E (e)).

This lemma, which is analogous to Lemma B1, follows from the fact that if e is deleted, then e is
unused.

Lemma B5: For every example E and every edge e in D

G ,

p( ¬e| ¬N E (e), Æ E, Q

(E) æ ) = p( ¬e| ¬N E (e)).

This lemma, which is analogous to Lemma B2, states that ¬e and Æ E, Q
independent  given ¬N E (e).  That is,  once ¬N E (e) is known,
regarding the probability of ¬e. This is immediate from the fact that p( Æ E, Q
be expressed in terms of the probabilities of edges other than e.

Æ E, Q

(E) æ are conditionally
(E) æ adds  no  information
| ¬N E (e)) can

(E) æ

We now hav e all the pieces to prove Theorem B2.

Proof of Theorem B2:
1 - pnew(e) = Ø

º 1 - p(e)

= p( ¬e) ·

vE (e)
uE (e)

vE (e)
p(N E (e))

201

(deﬁnition of p

)

new

(Theorem B1)

ø
ß
Q
ø
ß
KOPPEL, FELDMAN, & SEGRE

= p(N E (e)| Æ E, Q

(E) æ ) ·

p( ¬e)
p(N E (e))

= p(N E (e)| Æ E, Q

(E) æ ) · p( ¬e|N E (e))

(Lemma B3)

(Lemma B4)

= p(N E (e)| Æ E, Q

(E) æ ) · p( ¬e|N E (e), Æ E, Q

(E) æ

(Lemma B5)

= p( ¬e, N E (e)| Æ E, Q

(E) æ )

= p( ¬e| Æ E, Q

(E) æ )

= 1 - p(e| Æ E, Q

(E) æ ).

(Bayes rule)

(Lemma B4)

202

BIAS DRIVEN REVISION

Appendix C: Proof of Convergence

We  hav e seen in Section 5 that PTR always terminates. We  wish to show that when it does, all
exemplars  are  classiﬁed  correctly. We will  prove  this  for  domain  theories  which  satisfy  certain
conditions which will be made precise below.  The general idea of the proof is the following: by
deﬁnition, the algorithm terminates either when all exemplars are correctly classiﬁed or when all
edges have weight 1. Thus, it is only necessary to show that it is not possible to reach a state in
which  all  edges  have  weight  1  and  some  exemplar  is  misclassiﬁed. We  will  prove  that  such  a
state  fails  to  possess  the  property  of  ‘‘consistency’’ which  is  assumed  to  hold  for  the  initial
weighted dt-graph K

, and which is preserved at all times by the algorithm.

Deﬁnition  (Consistency): The  weighted  dt-graph K = Æ
exemplar Æ E, Q

if, for every root ri in D

, either:

(E) æ
i(E) = 1 and u
i(E) = 0 and u

(i)
(ii)

E (ri) > 0, or
E (ri) < 1.

, p æ

is consistent with

Recall that an edge e is deﬁned to be even if it is of even depth along every path from a root and
odd if is of odd depth along every path from a root. A domain theory is said to be unambiguous if
ev ery edge is either odd or even.  Note that negation-free domain theories are unambiguous. We
will prove our main theorem for unambiguous, single-root domain theories.

Recall that the only operations performed by PTR are:

(1) 

(2) 

(3) 

(4) 

(5) 

updating weights,

deleting ev en edges,

deleting odd edges,

adding a subtree beneath an even edge, and

adding a subtree beneath an odd edge.

We  shall  show that  each  of  these  operations  is  performed  in  such  a  way  as  to  preserve
consistency.

Theorem  C1  (Consistency): If K = Æ
weighted  dt-graph  which  is  consistent  with  the  exemplar
¢ =
is also a single-rooted, unambiguous dt-graph which is consistent with E.

is obtained from K via a single operation performed by PTR, then K

is  a  single-rooted,  unambiguous
and

Æ E, Q

(E) æ

¢ , p¢

, p æ

Before we prove this theorem we show that it easily implies convergence of the algorithm.

Theorem  C2  (Convergence): Giv en a single-rooted,  unambiguous  weighted  dt-
graph K
is consistent with every exemplar in

and a set of exemplars Z

such that K
, PTR terminates and produces a dt-graph D

¢ which classiﬁes every exemplar in Z

correctly.

¢ . Assume,  contrary  to  the  theorem,  that  some  exemplar

Proof  of  Theorem  C2: If PTR  terminates  prior  to  each  edge  being  assigned  the
weight 1, then by deﬁnition, all exemplars are correctly classiﬁed. Suppose then
such that p¢ (e) = 1 for every
that PTR produces a weighted dt-graph K
¢ =
e ˛
is
misclassiﬁed  by K
for  the  root r. Without  loss  of  generality, assume  that
is an IN exemplar of r. Since p¢ (e) = 1 for every edge, this means that
(E) æ
Æ E, Q
E (er ) = 0.  But  this  is  impossible  since  the  consistency of K
u
implies  that
E (er ) > 0  and thus it follows from Theorem C1 that for any K
uK
obtainable form

Æ E, Q

(E) æ

¢ , p¢

203

D
Q
K
Q
K
D
K
Æ
D
æ
¢
Z
Æ
D
æ
D
¢
K
¢
¢
KOPPEL, FELDMAN, & SEGRE

, u

E (er ) > 0. This contradicts the assumption that E is misclassiﬁed by K

.

Let  us  now turn  to  the  proof  of  Theorem  C1. We  will  use  the  following  four  lemmas,  slight
variants of which are proved in (Feldman, 1993).

is obtained from K = Æ

, p æ via updating of weights,

such that 0 < p(e) < 1, we hav e 0 < p¢ (e) < 1.19

, p æ be a weighted dt-graph such that 0 < u

E (er ) < 1  and
such  that  0 < p(e) < 1, we hav e

. Then  if  for  every  edge e in D

, p æ be a weighted dt-graph such that u

E (er ) > 0  and let

. The, if for every edge e in D

, it holds that either:

¢ =

, p¢

Lemma C1: If K
then for every edge e ˛
Lemma C2: Let K = Æ
let K
0 < p¢ (e) < 1, it follows that 0 < u
Lemma C3: Let K = Æ
¢ =

, p¢

¢ =

¢ , p¢
p¢ (e) = p(e), or
(i)
(ii) depth(e) is odd and u
(iii) depth(e) is even and u

E (er ) < 1.

E (e) > 0, or
E (e) < 1

then u

E (e) > 0.

An analogous lemma holds where the roles of ‘‘> 0’’ and ‘‘< 1’’ are reversed.

e

, then u

, then u

Lemma C4: If e is even edge in K
e is an odd edge in K
We  can  now prove  consistency (Theorem  C1). We  assume,  without  loss  of  generality, that
is  an  IN  exemplar  of  the  root r and  prove  that  for  each  one  of  the  ﬁve operations
and

is obtained by that operation from K

Æ E, Q
(updating and four revision operators) of PTR, that if K
E (er ) > 0, then u
u

E (er ) ‡ u
E (r).

E (r).  In addition, if

E (er ) £ u

E (er ) ‡ u

E (er ) £ u

E (er ) > 0.

(E) æ

e

e

e

is obtained from K via updating of weights.

, if 0 < p(e) < 1  then 0 < p¢ (e) < 1. But then

Proof  of  Theorem  C1: The  proof  consists  of  ﬁve separate  cases,  each
corresponding to one of the operations performed by PTR.
Case 1: K
By Lemma C1, for every edge e in D
E (er ) > 0 then u
by Lemma C2, if u
Case 2: K

is obtained from K via deletion of an even edge, e.
E (er ) ‡ u
is obtained from K via deletion of an odd edge, e.

From Lemma C4(i), we have u
Case 3: K
The  edge e is  deleted  only  if  it  is  not  needed  for  any exemplar. Suppose  that,
contrary to the theorem, there is an IN exemplar Æ E, Q
E (er ) > 0
but u

E (er ) > 0.

E (er ) > 0.

such that u

E (er ) = 0. Then

(E) æ

e

19 Recall that in the updating algorithm we deﬁned

vE (eri ) =

e
1 -

e

if Q
if Q

i(E) = 0
i(E) = 1

.

The somewhat annoying presence of e > 0 is necessary for the proof of Lemma C1.

204

K
K
¢
¢
Æ
D
æ
D
D
D
K
Æ
D
æ
K
¢
D
K
K
Æ
D
æ
K
¢
K
¢
K
¢
K
K
K
K
K
K
¢
K
K
¢
¢
K
K
¢
¢
K
K
¢
K
K
¢
(cid:236)
(cid:237)
(cid:238)
BIAS DRIVEN REVISION

R( Æ E, Q

(E) æ , e, K

) = u
u

e

E (er )
E (er )

e

e

E (er )
= u
uK
E (er )

= u

e

E (er )
0

> 2.

is obtained from K via appending a subtree beneath an even edge, e.

But  then e is  needed  for E, contradicting  the  fact  that e is  not  needed  for  any
exemplar.
Case 4: K
If p¢ (e) < 1, then the result is immediate from Lemma C2. Otherwise, let f be the
, beneath e. Then K
root  edge  of  the  subtree D a which  is  appended  to D
| f = K
e.
(E) æ such
Suppose that, contrary to the theorem, there is some IN exemplar Æ E, Q
that
C4(ii),
E (er ) = u
e
u

E (er ) > 0  
E (er ) £ u
|e

Lemma 

Then 

by 

u

R( Æ E, Q

(E) æ , e, K

E (er ) = 0. 
but
u
E (er ) = 0. But then,
) = u
u

E (er )
E (er )

e

e

0
E (er )

e

u

= 0.

. But then, by the construction of D a, u

E (e) = 0 < 1. The result follows immediately from Lemma C3.

Thus e is destructive for E in K
Thus, u
Case 5: K
e.
Suppose that, contrary to the theorem, some IN exemplar Æ E, Q
but u

via appending a subtree to K

is obtained from K

E (er ) = 0. Since K

e, it follows that

beneath the odd edge,

(E) æ , u

E (er ) > 0

E ( f ) = 1.

R( Æ E, Q

(E) æ , e, K

e = K
) = u
u

e

E (er )
E (er )

e

= u
u

e

E (er )
E (er )

e

.

Now, using Lemma C4(ii) on both numerator and denominator, we hav e

u

u

e

E (er )
E (er )

e

‡ u

E (er )u

E (er ) = ¥

> 2.

Thus, e is needed for E in K
D a. Then,  by  the  construction  of D a, it follows  that u
E (e) > 0. The result is immediate from Lemma C3.
u

. Now, let f be the root edge of the appended subtree,
E ( f ) < 1  and,  therefore

205

K
K
K
¢
K
¢
¢
K
K
¢
K
K
¢
K
¢
K
K
£
K
K
¢
K
¢
¢
K
K
¢
¢
K
K
K
K
¢
K
K
¢
K
K
¢
K
¢
K
¢
KOPPEL, FELDMAN, & SEGRE

This completes the proof of the theorem.
It  is  instructive  to note  why the  proof  of  Theorem  C1  fails  if D

is  not  restricted  to
unambiguous single-rooted dt-graphs. In case 4 of the proof of Theorem C1, we use the fact that
if  an  edge e is  destructive  for  an  exemplar Æ E, Q
then  the  revision  algorithm  used  to
construct the subgraph, D a, appended to e will be such that u
E ( f ) = 1. However,  this fact does
not hold in the case where e is simultaneously needed and destructive. This can occur if e is a
descendant of two roots where E is IN for one root and OUT for another root. It can also occur
when one path from e to the root r is of even length and another path is of odd length.

(E) æ

206

K
¢
BIAS DRIVEN REVISION

Appendix D: Guide to Notation

Ci
Hi
Bi
E

i(E)

i(E)
Æ E, Q
ˆG

ne
ne
p(e)

(E) æ

K = Æ

, p æ

e

e
uE (e)
vE (e)

Ri( Æ E, Q

(E) æ , e, K

)

s

l

d s
d l
RadK (G

)

A domain theory consisting of a set of clauses of the form Ci: Hi ‹
A clause label.

Bi.

A clause head; it consists of a single positive literal.

A clause body; it consists of a conjunction of positive or neg ative literals.

An example; it is a set of observable propositions.

The classiﬁcation of the example E for the ith root according to domain
theory G
The correct classiﬁcation of the example E for the ith root.

.

An exemplar, a classiﬁed example.
The set of NAND clauses equivalent to G
The dt-graph representation of G
The node to which the edge e leads.

.

.

The node from which the edge e comes.

The  weight  of  the  edge e; it represents  the  probability  that  the  edge e
needs to be deleted or that edges need to be appended to the node ne.
A weighted dt-graph.
Same as K but with the weight of the edge e equal to 1.
Same as K but with the edge e deleted.
The ‘‘ﬂow’’ of proof from the example E through the edge e.

The  adjusted  ﬂow of proof  through e taking  into  account  the  correct
classiﬁcation of the example E.
The extent (ranging from 0 to ¥
) to which the edge e in the weighted dt-
graph K
contributes to the correct classiﬁcation of the example E for the
ith root. If Ri is less/more than 1, then e is harmful/helpful; if Ri = 1 then
e is irrelevant.
The revision threshold; if p(e) < s

then e is revised.

The  weight  assigned  to  a  revised  edge  and  to  the  root  of  an  appended
component.

The revision threshold increment.

The revised edge weight increment.
The radicality of the changes required to K
theory G

.

in order to obtain a revised

207

G
G
Q
D
G
D
K
K
¢
¢
KOPPEL, FELDMAN, & SEGRE

References

Buchanan, B. & Shortliffe, E.H. (1984). Rule-Based Expert Systems: The MYCIN Experiments of
the Stanford Heuristic Programming Project. Reading, MA: Addison Wesley.

Feldman,  R.  (1993). Probabilistic  Revision  of  Logical  Domain  Theories.
Thesis, Department of Computer Science, Cornell University.

Ithaca,  NY: Ph.D.

Feldman, R., Koppel, M. & Segre, A.M. (August 1993). The Relevance of Bias in the Revision
of  Approximate  Domain  Theories. Working  Notes  of  the  1993  IJCAI  Workshop  on  Machine
Learning  and  Knowledge Acquisition:  Common  Issues,  Contrasting  Methods,  and  Integrated
Approaches, 44-60.

Ginsberg, A. (July 1990). Theory Reduction, Theory Revision, and Retranslation. Proceedings
of the National Conference on Artiﬁcial Intelligence, 777-782.

Koppel,  M.,  Feldman,  R.  &  Segre,  A.M.  (December  1993). Theory  Revision  Using  Noisy
Exemplars. Proceedings of the Tenth Israeli Symposium on Artiﬁcial Intelligence and Computer
Vision, 96-107.

Mahoney,  J. & Mooney,  R. (1993).  Combining Connectionist and Symbolic Learning to Reﬁne
Certainty-Factor Rule-Bases. Connection Science, 5, 339-364.

Murphy,  P.M. & Aha, D.W. (1992). UCI Repository of Machine Learning Databases [Machine-
readable  data  repository].
Irvine,  CA:  Department  of  Information  and  Computer  Science,
University of California at Irvine.

Ourston,  D.  (August  1991). Using  Explanation-Based  and  Empirical  Methods  in  Theory
Revision. Austin, TX: Ph.D. Thesis, University of Texas at Austin.

Ourston, D. & Mooney,  R. (in press). Theory Reﬁnement Combining Analytical and Empirical
Methods. Artiﬁcial Intelligence.

Pazzani,  M.  &  Brunk,  C.  (June  1991). Detecting  and  Correcting  Errors  in  Rule-Based  Expert
Systems: An Integration of Empirical and Explanation-Based Learning. Knowledge Acquisition,
3(2), 157-173.

Pearl,  J.  (1988). Probabilistic  Reasoning  in  Intelligent  Systems. San  Mateo,  CA:  Morgan
Kaufmann.

Quinlan, J.R. (1986). Induction of Decision Trees. Machine Learning, 1(1), 81-106.

To well, G.G. & Shavlik, J.W. (October 1993). Extracting Reﬁned Rules From Knowledge-Based
Neural Networks. Machine Learning, 13(1), 71-102.

Wilkins,  D.C.  (July  1988). Knowledge  Base  Reﬁnement  Using  Apprenticeship  Learning
Techniques. Proceedings of the National Conference on Artiﬁcial Intelligence, 646-653.

Wogulis,  J.  &  Pazzani,  M.J.  (August  1993). A Methodology  for  Evaluating  Theory  Revision
Systems: Results with Audrey II. Proceedings of the Thirteenth International Joint Conference
on Artiﬁcial Intelligence, 1128-1134.

208

","The theory revision problem is the problem of how best to go about revising a
deficient domain theory using information contained in examples that expose
inaccuracies. In this paper we present our approach to the theory revision
problem for propositional domain theories. The approach described here, called
PTR, uses probabilities associated with domain theory elements to numerically
track the ``flow'' of proof through the theory. This allows us to measure the
precise role of a clause or literal in allowing or preventing a (desired or
undesired) derivation for a given example. This information is used to
efficiently locate and repair flawed elements of the theory. PTR is proved to
converge to a theory which correctly classifies all examples, and shown
experimentally to be fast and accurate even for deep theories."
"Journal of Arti(cid:12)cial Intelligence Research 1 (1994) 257-275

Submitted 11/93; published 3/94

Exploring the Decision Forest: An Empirical Investigation

of Occam's Razor in Decision Tree Induction

Patrick M. Murphy

pmurphy@ics.uci.edu

Michael J. Pazzani

pazzani@ics.uci.edu

Department of Information & Computer Science

University of California, Irvine, CA 92717

Abstract

We report on a series of experiments in which all decision trees consistent with the

training data are constructed. These experiments were run to gain an understanding of the

properties of the set of consistent decision trees and the factors that a(cid:11)ect the accuracy

of individual trees.

In particular, we investigated the relationship between the size of a

decision tree consistent with some training data and the accuracy of the tree on test data.

The experiments were performed on a massively parallel Maspar computer. The results of

the experiments on several arti(cid:12)cial and two real world problems indicate that, for many

of the problems investigated, smaller consistent decision trees are on average less accurate

than the average accuracy of slightly larger trees.

1. Introduction

The top-down induction of decision trees is an approach to machine learning that has been

used on a variety of real world tasks. Decision trees are well-suited for such tasks since they

scale fairly well with the number of training examples and the number of features, and can

represent complex concepts in a representation that is fairly easy for people to understand.

Decision tree induction algorithms (Breiman, Friedman, Olshen, & Stone, 1984; Quin-

lan, 1986; Fayyad & Irani, 1992) typically operate by choosing a feature that partitions the

training data according to some evaluation function (e.g., the purity of the resulting par-

titions). Partitions are then further partitioned recursively until some stopping criterion is

reached (e.g., the partitions contain training examples of a single class). Nearly all decision

tree induction algorithms create a single decision tree based upon local information of how

well a feature partitions the training data. However, this decision tree is only one of a set of

decision trees consistent with the training data. In this paper, we experimentally examine

the properties of the set of consistent decision trees. We will call the set of decision trees

that are consistent with the training data a decision forest.

Our experiments were run on several arti(cid:12)cial concepts for which we know the correct

answer and two naturally occurring databases from real world tasks available from the UCI

Machine Learning Repository (Murphy & Aha, 1994) in which the correct answer is not

known. The goal of the experiments were to gain insight into how factors such as the

size of a consistent decision tree are related to the error rate on classifying unseen test

instances. Decision tree learners, as well as most other learners, attempt to produce the

(cid:13)1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.

c

Murphy & Pazzani

smallest consistent hypothesis.

Occam's razor is often used to justify this bias. Here,

1

we experimentally evaluate this bias towards simplicity by investigating the relationship

between the size of a consistent decision tree and its accuracy.

If the average error of

decision trees with N test nodes is less than the average error of decision trees of size N + i

(for i > 0), an appropriate bias for a learner attempting to minimize average error would

be to return the smallest decision tree it can (cid:12)nd within its resource constraints.

In this paper, we restrict our attention to decision trees that are consistent with the

training data and ignore issues such as pruning which trade o(cid:11) consistency with the training

data and the simplicity of the hypothesis. For the purposes of this paper, a consistent

decision tree is one that correctly classi(cid:12)es every training example.

We also place two

2

additional constraints on decision trees. First, no discriminator can pass all instances down

a single branch. This insures that the test made by the decision tree partitions the training

data. Second, if all of the training instances at a node are of the same class, no additional

discriminations are made. In this case, a leaf is formed with class label speci(cid:12)ed by the class

of the instances at the leaf. These two constraints are added to insure that the decision

trees analyzed in the experiments correspond to those that could be formed by top down

decision tree induction algorithms. In this paper, we will not investigate problems that have

continuous-valued features or missing feature values.

In Section 2 (and the appendix), we will report on some initial exploratory experiments

in which the smallest consistent decision trees tend to be less accurate than the average

accuracy of those slightly larger. Section 3 provides results of additional experiments that

address this issue. Section 4 addresses the implication of our (cid:12)ndings to the policy a learner

should take in deciding which of the many consistent hypotheses it should prefer. Section

5 relates this work to previous empirical and theoretical research.

2. Initial Experiments

We will investigate the relationship between various tree characteristics and error. In par-

ticular, we will look at node cardinality (i.e., the number of internal nodes in a tree) and

leaf cardinality (i.e., the total number of leaves in a tree).

It should be noted that even when using a powerful massively parallel computer, the

choice of problems is severely constrained by the computational complexity of the task.

The number of trees of any node cardinality that might be generated is O(d

) where d is

c

the number of discriminators and c is the node cardinality. Even on a massively parallel

computer, this precluded the use of problems with many features or any continuous-valued

features.

The (cid:12)rst experiment considered learning from training data in which there are 5 boolean

features. The concept learned was X Y Z _ AB . This concept was chosen because it was of

moderate complexity, requiring a decision tree with at least 8 nodes to represent correctly.

With 5 boolean features, the smallest concept (e.g., True) would require 0 test nodes and

the largest (e.g., parity) would require 31.

1. We say \attempt to produce the smallest consistent hypothesis"" because most systems use some form of

limited look-ahead or greedy search. As a result, the smallest consistent tree is rarely found.

2. The arti(cid:12)cial and natural problems we study here have consistent training sets.

258

Exploring the Decision Forest

We ran 100 trials, creating a training set by randomly choosing without replacement

20 of the 32 possible training examples and using the remaining 12 examples as the test

set. For each trial, every consistent decision tree was created, and we computed the average

error rate made by trees with the same node cardinality. Figure 1 plots the mean and 95%

con(cid:12)dence interval of these average errors as a function of the node cardinality. Figure 1

also plots the number of trials on which at least one decision tree of a given node cardinality

is consistent with the training data.

r
o
r
r
E

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

2

4

100

80

60

40

20

0

s
l
a
i
r
T

f
o

r
e
b
m
u
N

Error
Trials

14

16

18

20

8

6
12
Node Cardinality

10

Figure 1. The average error of 100 trials as a function of node cardinality and the number

of trials for each node cardinality.

From node cardinality 7 to node cardinality 16, there is a monotonic increase in error

with increasing node cardinality. For the range from 2 to 3 nodes, the error is varied;

however there is little evidence for these error values because they are based on only 2 and

1 trials, respectively. For the range of node cardinalities between 4 and 7, average error is

de(cid:12)nitely not a monotonically increasing function of node cardinality. As seen in the curve,

5 node trees are on the average more accurate than 4 node trees, and 7 node trees are on

the average more accurate than trees with 6 nodes. This last result is somewhat surprising

since one gets the impression from reading the machine learning literature (Muggleton,

Srinivasan, & Bain, 1992) that the smaller hypothesis (i.e., the one that provides the most

compression of the data (Rissanen, 1978)) is likely to be more accurate. We will explore

this issue in further detail in Section 3. Appendix 1 presents data showing that this result

is not unique to this particular concept. A (cid:12)nal, interesting (cid:12)nding that we will not explore

further in this paper is that for very large node cardinalities, error begins to decrease as the

node cardinality increases.

Table 1 lists the average number of consistent trees for each node cardinality and the

average number of correct trees (i.e., those trees consistent with the training data that make

no errors on the unseen test examples). There are no correct trees with fewer than 8 nodes,

since at least 8 nodes are required to represent this concept. Clearly, since there are many

trees consistent with the training data, a learner needs some policy to decide which tree to

return. We will return to this issue in Section 4.

259

 
 
 
Murphy & Pazzani

Nodes

Number of

Number of

Consistent Trees Correct Trees

2

2.0

0.0

3

4.0

0.0

4

3.3

0.0

5

12.3

0.0

6

27.6

0.0

7

117.1

0.0

8

377.0

17.8

9

879.4

37.8

10

1799.9

50.2

11

3097.8

41.6

12

4383.0

95.4

13

5068.9

66.6

14

4828.3

37.7

15

3631.5

31.3

16

1910.6

14.8

17

854.4

4.0

18

308.6

3.6

19

113.8

0.0

Table 1. The average number of trees consistent with 20 training examples of the X Y Z _AB

concept.

3. Further Experimentation

For most of the problems studied, we found that on average, the smallest decision trees

consistent with the training data had more error on unseen examples than slightly larger

trees. We ran additional experiments to make sure that this result is not an artifact of the

experimental methodology that we used, as reported in the next sections.

3.1 Representative Train/Test Partitions

One possible explanation for the (cid:12)nding of the previous section is that the smaller decision

trees are formed from unrepresentative samples. For example, there are 11 positive and 21

negative examples of the concept X Y Z _ AB . If all or most of the examples in the training

set are negative, a very small tree may be learned which would probably do very poorly on

the mostly positive test set. To insure that the results are not caused by unrepresentative

training sets, we eliminated all training data that was not reasonably representative. In

particular, since there is a

probability that a training instance is positive, a representative

32

11

training set of size 20 would have about 7 positive instances. Since one standard deviation

q

11

11

would be

20 (cid:3)

(cid:3) (1 (cid:0)

), we eliminated from analysis those training sets with greater

32

32

than 8 or fewer than 5 positive instances. Similarly, there is a 0.5 probability that each

binary feature takes on a true value, so we eliminated from analysis any training data which

has any feature that is true in greater than 13 or fewer than 7 instances. Figure 2 is based

260

Exploring the Decision Forest

on the 69 of 100 trials of the X Y Z _ AB concept that met this representative test. Notice

that the two trials that formed the only 2 and 3 node trees were removed. Even when only

the more representative training sets are considered, the average error of trees of size 4 is

greater than the average error of size 5 trees.

r
o
r
r
E

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

2

4

100

80

60

40

20

0

s
l
a
i
r
T

f
o

r
e
b
m
u
N

Error
Trials

14

16

18

20

8

6
12
Node Cardinality

10

Figure 2. Error rate of consistent trees from representative training sets as a function of

node cardinality.

By regrouping the results of 100 trials for the X Y Z _ AB concept so that trials with the

same minimum-sized trees are grouped together, a set of (cid:12)ve curves, each associated with

a subgroup, was formed (Figure 3). The intent of the grouping is to allow us to determine

whether the minimum-sized trees for any given trial are on average more accurate than

larger trees.

r
o
r
r
E

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

2

4

6

Min. Tree Size = 2
Min. Tree Size = 4
Min. Tree Size = 5
Min. Tree Size = 6
Min. Tree Size = 7

14

16

18

20

8

10
Node Cardinality

12

Figure 3. Error as a function of node cardinality for the X Y Z _ AB concept when (cid:12)rst

grouped by minimum-sized trees built.

Note that in Figure 3, for most minimum tree sizes, error is not a monotonically increas-

ing function of node cardinality. Furthermore, the average error of the smallest trees found

is not the most accurate when the smallest tree has 4 or 6 nodes. In addition, regardless

261

 
 
Murphy & Pazzani

of the size of the smallest tree found, the average accuracy of trees of size 8 (the size of the

smallest correct tree) rarely has the minimum average error.

Another interesting (cid:12)nding becomes apparent with this way of viewing the data: the

average error rates of trees for training sets that allow creation of smaller consistent trees

tends to be higher than for those training sets that can only form larger trees. For example,

the error rate for those training sets whose minimum-sized trees have 4 nodes is higher than

the error rate on trials whose minimum-sized trees has 7 nodes.

r
o
r
r
E

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

2

4

100

80

60

40

20

0

s
l
a
i
r
T

f
o

r
e
b
m
u
N

Error
Trials

14

16

18

20

8

6
12
Node Cardinality

10

Figure 4. Error rate of consistent trees with 2 examples per leaf of some correct 8 node tree

as a function of node cardinality.

The de(cid:12)nition of representative that we used earlier in this section used global char-

acteristics of the training data to determine representativeness. Here, we consider a more

detailed view of representativeness that takes the structure of the correct concept into ac-

count. It is unreasonable to expect a decision tree learner to learn an accurate concept if

there are no examples that correspond to some of the leaves of some correct decision tree.

To generate training data for the next experiment, we (cid:12)rst randomly selected one of the

72 trees with 8 nodes that is consistent with all the data. Next, for each leaf of the tree,

we randomly selected two examples (if possible) to include in the training set.

If a leaf

only had one example, that example was included in the training set. Finally, we randomly

selected from the remaining examples so that there were 20 training examples and 12 test

examples. We had anticipated that with representative training sets formed in this manner,

very small consistent trees would be rare and perhaps the error rate would monotonically

increase with node cardinality. However, the results of 100 trials, as displayed in Figure 4,

indicate the same general pattern as before. In particular, the average error of trees with 7

nodes is substantially less than the average error of those with 6 nodes. Another experiment

with one randomly selected example per leaf had similar results.

3.2 Training Set Size and Concept Complexity

The minimum-sized decision tree for the concept X Y Z _ AB has 8 tests and 9 leaves. Since

the correct tree does not provide much compression

of a set of 20 examples used to induce

3

3. The exact amount of compression provided depends upon the particular scheme chosen for encoding the

training data. See (Quinlan & Rivest, 1989; Wallace & Patrick, 1993) for two such schemes.

262

 
 
Exploring the Decision Forest

the tree, one might argue that the sample used was too small for this complex a concept.

Therefore, we increased the number of training examples to the maximum possible. Figure

5 plots the average error of 32 trials in which we formed all decision trees consistent with 31

examples. Each tree was evaluated on the remaining unseen example. Figure 5 shows that

the smaller trees formed from samples of size 31 have more error than the slightly larger

trees. Since the minimum correct decision tree has 8 nodes and the consistent trees classify

all 31 training examples correctly, any decision tree with fewer than 8 nodes classi(cid:12)es the

test example incorrectly.

r
o
r
r
E

1.0

0.8

0.6

0.4

0.2

0.0

0

2

4

6

Error
Trials

32
28
24
20
16

12
8

4
0

s
l
a
i
r
T

f
o
r
e
b
m
u
N

10

12
8
16
Node Cardinality

14

18

20

22

24

26

Figure 5. Error rate of consistent trees with leave-one-out testing as a function of node

cardinality.

To refute further the hypothesis that the results obtained so far were based on using

too small a training set for a given concept complexity, we considered two less complex

concepts.

In particular, we investigated a single attribute discrimination, A with four

irrelevant features (Figure 6) and a simple conjunction, AB with three irrelevant features

(Figure 7).

r
o
r
r
E

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

2

4

6

100

80

60

40

s
l
a
i
r
T

f
o

r
e
b
m
u
N

Error
Trials

14

16

18

20

8

10
Node Cardinality

12

Figure 6. Error as a function of node cardinality for the single attribute discrimination A

concept.

263

 
 
 
 
Murphy & Pazzani

r
o
r
r
E

0.4

0.3

0.2

0.1

0.0

0

2

4

6

100

80

60

40

20

0

s
l
a
i
r
T

f
o

r
e
b
m
u
N

Error
Trials

8

10
Node Cardinality

12

14

16

18

20

Figure 7. Error as a function of node cardinality for the simple conjunction AB concept.

For each concept, 100 trials were run in which 20 examples were used for training and

the remaining 12 for testing. For these simpler concepts, though the smallest trees are the

most accurate, error again is not a monotonically increasing function of node cardinality.

3.3 Training and Testing using the Same Probability Distribution.

In our previous experiments, we used a methodology that is typical in empirical evaluations

of machine learning systems: the training data and the test data are disjoint. In contrast,

most theoretical work on the PAC model (Valiant, 1984) assumes that the training and

test data are generated from the same probability distribution over the examples. For

this section, we ran an experiment in which training and test examples were selected with

replacement from the same distribution to ensure that our results were not dependent on a

particular experimental methodology.

r
o
r
r
E

0.20

0.15

0.10

0.05

0.00

0

2

4

6

8

10

14
Node Cardinality

12

100

80

60

40

20

0

s
l
a
i
r
T

f
o

r
e
b
m
u
N

Error
Trials

16

18

20

22

Figure 8. Error as a function of node cardinality when the training and test examples are

generated by the same distribution for the X Y Z _ AB concept.

Once again, the target concept was X Y Z _ AB . By randomly choosing 31 training

examples with replacement from the set of 32 possible instances, on average approximately

20 distinct training examples are selected. Error is estimated by randomly choosing 1000

264

 
 
 
 
Exploring the Decision Forest

examples with replacement from the set of possible instances. Figure 8 graphs the mean

error (averaged over 100 trials) as a function of node cardinality.

This testing methodology produces much smaller values for the proportion of test ex-

amples misclassi(cid:12)ed than the disjoint training and test set methodology because those test

examples which also were training examples are always classi(cid:12)ed correctly. However, the

same basic pattern of results is observed. Error is not at a minimum for the smallest de-

cision trees nor at decision trees with 8 nodes (the minimum-sized correct tree). Error

monotonically increases starting at trees with 7 nodes and then begins to decrease again

for very large node cardinalities. Note that on some trials, it is possible to build decision

trees with up to 21 nodes since some training sets contained 22 distinct examples.

3.4 Average Path Length

The information gain metric of ID3 is intended to minimize the number of tests required to

classify an example. Figure 9 reanalyzes the data from Figure 1 by graphing average error

as a function of average path length for the X Y Z _ AB concept.

0.4

0.3

0.2

r
o
r
r
E

0.1

1.0

100

80

60

40

20

0

s
l
a
i
r
T

f
o
r
e
b
m
u
N

Error
Trials

2.0

3.0
Average Path Length

4.0

5.0

Figure 9. Error as a function of average path length for the X Y Z _ AB concept.

The results are similar to those obtained when relating the number of test nodes to the

error rate: error is not a monotonically increasing function of average path length. Similar

analyses were performed and similar results have been obtained for other concepts which

are presented in the Appendix.

4. The Minimum-Sized Decision Tree Policy

A designer of a learning algorithm either explicitly or implicitly must decide which hypoth-

esis to prefer when multiple hypotheses are consistent with the training data. As Table 1

shows, there can be many consistent decision trees. Should the learner always prefer the

smallest consistent decision tree? A learner that adopts this strategy can be said to be

following the minimum-sized decision tree policy.

265

 
 
Murphy & Pazzani

In this section, we present results from additional experiments to evaluate this policy.

In particular, we gather evidence to address two related questions:

(cid:15) Given any two consistent decision trees with di(cid:11)erent node cardinalities, what is the

probability that the smaller decision tree is more accurate?

(cid:15) Given the minimum-sized decision tree and a larger consistent decision tree, what is

the probability that the smallest decision tree is more accurate?

The (cid:12)rst question is of more interest to the current practice of decision tree induction

since, for e(cid:14)ciency reasons, no algorithm attempts to (cid:12)nd the smallest consistent decision

tree for large data sets. Nonetheless, most algorithms are biased toward favoring trees with

fewer nodes.

y
t
i
l
i

b
a
b
o
r
P

0.8

0.6

0.4

0.2

0.0

0

1000

800

600

400

200

s
l
a
i
r
t

f
o
r
e
b
m
u
N

0

0

Prob(Smaller > Larger)
Prob(Larger > Smaller)
Prob(Smaller = Larger)

14

16

14

16

4

2
12
Difference in Node Cardinality

10

6

8

4

2
12
Difference in Node Cardinality

10

6

8

Figure 10. The probability that the accuracy of a smaller decision tree is greater than,

equal to, or less than the accuracy of a larger tree as a function of the di(cid:11)erence of node

cardinalities for the X Y Z _ AB concept (upper). The number of trials out of 1000 on which

at least 2 trees had a given di(cid:11)erence in node cardinality (lower).

To address the question of whether a learner should prefer the smaller of two randomly

selected consistent trees, we ran 1000 trials of learning the concept X Y Z _ AB from 20

training examples. For each trial, we recorded the node cardinality and accuracy (on the

12 test examples) of every consistent tree. For each pair of consistent trees (with di(cid:11)erent

266

 
 
Exploring the Decision Forest

node cardinalities), we computed the di(cid:11)erence in node cardinality and indicated whether

the accuracy of the smaller tree was greater than, equal to, or less than the accuracy of the

larger tree. From this data, we computed the observed probability that one decision tree

was more accurate than another as a function of the di(cid:11)erence in node cardinalities (see

Figure 10 upper). The graph shows that on this concept, the probability that the smaller

of two randomly chosen consistent decision trees will be more accurate is greater than the

probability that the larger tree will be more accurate. Furthermore, the probability that

the smaller tree is more accurate increases as the di(cid:11)erence in node cardinality increases.

An exception to this trend occurs for very large di(cid:11)erences in node cardinality. However,

as Figure 10 lower shows, these exceptions are quite rare. Consistent decision trees whose

node cardinalities di(cid:11)ered by 16 were found in only 6 of the 1000 trials.

The results of

4

this experiment indicate that on average, a learner that prefers the smaller of two randomly

selected decision trees has a higher probability of being more accurate on this concept than

a learner that selects the larger tree.

y
t
i
l
i

b
a
b
o
r
P

0.8

0.6

0.4

0.2

0.0

Prob(Smallest > Larger)
Prob (Larger > Smallest)
Prob(Smallest = Larger)

0

2

4

6

8

10

12

14

16

Difference in Node Cardinality

Figure 11. The probability that the accuracy of a minimum-sized decision is greater than,

equal to, or less than the accuracy of a larger tree as a function of the di(cid:11)erence of node

cardinalities for the X Y Z _ AB concept.

To address the question of whether a learner should prefer the smallest consistent deci-

sion over a randomly selected consistent tree with more test nodes, we reanalyzed the data

from the previous experiment. Figure 11 graphs the observed probability that a consistent

decision tree with the minimum node cardinality is more accurate than a larger tree as a

function of the di(cid:11)erence in node cardinalities between the two trees. The graph shows that

a learner that chooses randomly among the consistent decision trees with minimum node

cardinalities is more likely to (cid:12)nd a tree that is more accurate than a learner that randomly

selects among larger trees.

5

Figure 11 clearly shows that for this particular concept, preferring the minimum-sized

decision tree policy is on average a better policy than preferring a decision tree that is any

4. Four trials had minimum-sized trees with 2 nodes and maximally sized trees with 18 nodes. Two trials

had minimum-sized trees with 3 nodes and maximally sized trees with 19 nodes.

5. Except for the rare case when an extremely small and an extremely large decision trees is found on the

same trial.

267

Murphy & Pazzani

(cid:12)xed size larger than the smallest decision tree. However, it is not clear that the minimum-

sized decision tree is the best possible policy for this concept. Indeed, by looking at the

data from Figure 3, it is apparent that a better strategy for this concept would be to (cid:12)nd

the minimum-sized tree and then decide whether to return the minimum-sized tree or a tree

of a di(cid:11)erent node cardinality as a function of the node cardinality of the minimum-sized

consistent tree. Table 2 shows which node cardinality has the highest probability of being

most accurate as a function of the minimally sized tree, together with the number of trials

(out of 1000) on which the minimum-sized tree had a particular node cardinality.

Minimum

Preferred

Number of

Node Cardinality Node Cardinality

Trials

2

2

49

3

5

17

4

5

300

5

5

351

6

8

211

7

8

71

8

8

1

Table 2. A policy of returning a larger decision tree as a function of the minimum-sized

tree for the X Y Z _ AB concept.

Figure 11 provides some of the data that illustrates that the policy in Table 2 will

perform better than preferring the minimum-sized decision tree on this concept. Figure

12 graphs the observed probability that a consistent decision tree with a minimum node

cardinality of 5 (upper), 6 (middle), or 7 (lower) is more accurate than a larger tree as a

function of the di(cid:11)erence in node cardinalities between the two trees. The graph shows

that when the minimum-sized decision tree has 5 nodes, the probability that a larger tree

is more accurate is less than the probability that the smaller tree is more accurate for all

node cardinalities. This is particularly interesting because it shows that giving a decision

tree learner the size of the correct tree and having the decision tree learner produce an

hypothesis of this size is not the best strategy for this concept. However, when the smallest

consistent tree has 6 nodes, there is a 0.560 probability that a randomly chosen tree with

8 nodes will be more accurate and a 0.208 probability that a tree with 8 test nodes will

have the same accuracy. In addition, when the minimum-sized tree has 7 test nodes, the

probability that a tree with 8 nodes is more accurate is 0.345 while the probability that it

is less accurate is 0.312.

Note that we do not believe that the policy in Table 2 is uniformly superior to preferring

the minimum-sized decision tree. Rather, there is probably some interaction between the

complexity of the concept to be learned, the number of training examples, and the size of

the smallest consistent decision tree. Furthermore, a learner should not be tuned to learn a

particular concept, but should perform well on a variety of concepts. Clearly, if extremely

simple concepts are to be learned su(cid:14)ciently frequently, the minimum-sized decision tree

policy will be better than the policy in Table 2. Indeed, the minimum-sized decision tree

268

Exploring the Decision Forest

policy would work well on the simple concepts A and AB discussed in Section 3.2. However,

if simple concepts are rarely encountered, there may be better policies. The best policy must

depend upon the distribution of concepts that are encountered. Clearly, if the only concept

Minimum Node Cardinality = 5

y
t
i
l
i
b
a
b
o
r
P

y
t
i
l
i

b
a
b
o
r
P

y
t
i
l
i

b
a
b
o
r
P

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

0.0

0

5

10

15

Difference in Node Cardinality

Minimum Node Cardinality = 6

0

5

10

15

Difference in Node Cardinality

Minimum Node Cardinality = 7

Prob(Smallest>Larger)
Prob(Larger>Smallest)
Prob(Smallest=Larger)

Prob(Smallest>Larger)
Prob(Larger>Smallest)
Prob(Smallest=Larger)

Prob(Smallest>Larger)
Prob(Larger>Smallest)
Prob(Smallest=Larger)

0

5

10

15

Difference in Node Cardinality

Figure 12. The probability that the accuracy of a minimum-sized decision tree is greater

than, equal to, or less than the accuracy of a larger tree as a function of the di(cid:11)erence of

node cardinalities for the X Y Z _ AB concept when the minimum-sized decision tree has 5

(upper), 6 (middle), or 7 (lower) test nodes.

269

Murphy & Pazzani

to be learned is X Y Z _ AB , the best policy would be to ignore the training data and return

the decision tree representation for X Y Z _ AB .

It may be that Occam's razor should

be viewed as a philosophical statement about the distribution of concepts one is likely

to encounter. Occam's razor has not been shown to be a guarantee that when learning

a complex concept, the simplest hypothesis consistent with the data is likely to be more

accurate than the randomly-chosen more complex hypothesis consistent with the training

data.

5. Analysis

Scha(cid:11)er (1992, 1993) presents a series of experiments on over(cid:12)tting avoidance algorithms.

Over(cid:12)tting avoidance algorithms prefer simpler decision trees over more complex ones, even

though the simpler decision trees are less accurate on the training data, in hopes that the

trees will be more accurate on the test data. Scha(cid:11)er shows that these over(cid:12)tting avoidance

algorithms are a form of bias. Rather than uniformly improving performance, the over(cid:12)tting

avoidance algorithms improve performance on some distributions of concepts and worsen

performance on other distributions of concepts.

The results of our experiments go a step further than Scha(cid:11)er's. We have shown that

for some concepts, the preference for simpler decision trees does not result in an increase

in predictive accuracy on unseen test data, even when the simple trees are consistent with

the training data. Like Scha(cid:11)er, we do not dispute the theoretical results on Occam's

razor (Blumer, Ehrenfeucht, Haussler, & Warmuth, 1987), minimum description length

(Quinlan & Rivest, 1989; Muggleton et al., 1992), or minimizing the number of leaves of

a decision tree (Fayyad & Irani, 1990). Rather, we point out that for a variety of reasons,

the assumptions behind these theoretical results mean that the results do not apply to the

experiments reported here. For example, (Blumer et al., 1987) indicates that if one (cid:12)nds an

hypothesis in a su(cid:14)ciently small hypothesis space (and simpler hypotheses are one example

of a small hypothesis space) and this hypothesis is consistent with a su(cid:14)ciently large sample

of training data, one can be fairly con(cid:12)dent that it will be fairly accurate on unseen data

drawn from the same distribution of examples. However, it does not say that on average

this hypothesis will be more accurate than other consistent hypotheses not in this small

hypothesis space.

The (Fayyad & Irani, 1990) paper explicitly states that the results on minimizing the

number of leaves of decision trees are worst case results and should not be used to make

absolute statements concerning improvements in performances. Nonetheless, informal argu-

ments in the paper state: \This may then serve as a basis for provably establishing that one

method for inducing decision trees is better than another by proving that one algorithm

always produces a tree with a smaller number of leaves, given the same training data.""

Furthermore, other informal arguments imply that this result is probabilistic because of the

existence of \pathological training sets."" However, as we have shown in Figures 2 and 4

(as well as a reanalysis of the mux6 data in the Appendix), eliminating pathological (i.e.,

unrepresentative) training sets does not change the qualitative result that on some concepts,

the smaller trees are less accurate predictors than slightly larger trees.

270

Exploring the Decision Forest

6. Conclusion

We have reported on a series of experiments in which we generated all decision trees on

a variety of arti(cid:12)cial concepts and two naturally occurring data sets. We found that for

many of the concepts, the consistent decision trees that had a smaller number of nodes were

less accurate on unseen data than the slightly larger ones. These results do not contradict

existing theoretical results. Rather, they serve to remind us to be cautious when informally

using the intuitions derived from theoretical results on problems that are not covered by the

theorems or when using intuitions derived from worst-case results to predict average-case

performance.

We stress that our results are purely experimental. Like the reader, we too would be

pleased if there were theoretical results that indicated, for a given sample of training data,

which decision tree is likely to be most accurate. However, it is not clear whether this can be

done without knowledge of the distribution of concepts one is likely to encounter (Scha(cid:11)er,

1994).

We also note that our results may be due to the small size of the training sets relative to

the size of the correct tree. We tried to rule out this possibility by using larger training sets

(31 of the 32 possible examples) and by testing simpler concepts. For the simpler concepts,

the smallest decision trees were the most accurate, but error did not monotonically increase

with node cardinality. Since most decision tree learners that greedily build decision trees

do not return the smallest decision tree, our results may be of practical interest even for

simple concepts. In the future, experiments with more features and more examples could

help to answer this question, but considerably more complex problems cannot be handled

even by future generations of parallel supercomputers.

In addition, we note that in our

experiments, we did not build decision trees in which a test did not partition the training

data. This explains why we found relatively few extremely large decision trees and may

explain why very large trees made few errors. To our knowledge, all decision tree algorithms

have this constraint. However, the theoretical work on learning does not make use of this

information. We could rerun all of our experiments without this constraint, but we would

prefer that some future theoretical work take this constraint into account.

Although we have found situations in which the smallest consistent decision tree is not

on average the most accurate and cases in which there is a greater than 0.5 probability that a

larger decision tree is more accurate than the smallest, we believe that learning algorithms

(and people) with no relevant knowledge of the concept and no information about the

distribution of concepts that are likely to be encountered should prefer simpler hypotheses.

This bias is appropriate for learning simple concepts. For more complex concepts, the

opposite bias, preferring the more complex hypotheses, is unlikely to produce an accurate

hypothesis (Blumer et al., 1987) and (Fayyad & Irani, 1990) due to the large number of

consistent complex hypotheses. We believe that the only way to learn complex hypotheses

reliably is to have some bias (e.g., prior domain knowledge) which favors particular complex

hypotheses such as combinations of existing hypotheses learned inductively as in OCCAM

(Pazzani, 1990).

Indeed, (Valiant, 1984) advocates a similar position: \If the class of

learnable concepts is as severely limited as suggested by our results, then it would follow

that the only way of teaching more complex concepts is to build them up from simpler

ones.""

271

Murphy & Pazzani

Acknowledgements

We thank Ross Quinlan, Geo(cid:11)rey Hinton, Michael Cameron-Jones, Cullen Scha(cid:11)er, Den-

nis Kibler, Steve Hampson, Jason Catlett, Haym Hirsh, Anselm Blumer, Steve Minton,

Michael Kearns, Tom Dietterich, Pat Langley, and David Schulenburg for commenting on

various aspects of this research. The research reported here was supported in part by

NSF infrastructure grant number MIP-9205737, NSF Grant INT-9201842, AFOSR grant

F49620-92-J-0430, and AFOSR AASERT grant F49620-93-1-0569.

Appendix A. Experiments on Additional Problems

In this appendix, we provide data on experiments which we ran on additional problems.

The experiments show that the basic (cid:12)ndings in this paper are not unique to the arti(cid:12)cial

concept, X Y Z _ AB .

Mux6

The multiplexor concept we consider, mux6, has a total of 8 binary features. Six features

represent the functionality of a multiplexor and 2 features are irrelevant. The minimum sized

tree has 7 nodes. This particular concept was chosen because it is di(cid:14)cult for a top-down

inductive decision tree learner with limited look ahead to (cid:12)nd a small hypothesis (Quinlan,

1993). On each trial, we selected 20 examples randomly and tested on the remaining

examples. Since most of the computational cost of building consistent trees is for larger

node cardinalities and we are primarily interested in trees with small node cardinalities, we

only computed consistent trees with up to 10 nodes for 10 trials and up to 8 nodes for 340

Error
Trials

r
o
r
r
E

0.45

0.40

0.35

0.30

0.25

350

300

250

200

150

100

50

0

s
l
a
i
r
T

f
o

r
e
b
m
u
N

2

4

6
Node Cardinality

8

10

Figure 13. Error as a function of node cardinality for the mux6 concept.

trials. Figure 13 presents the average error as a function of the node cardinality for these

trials. This graph again shows that average error does not monotonically increase with node

cardinality. Trees of 4 nodes are on the average 4% less accurate than trees of 5 nodes.

272

 
 
Exploring the Decision Forest

Lenses

The lenses domain has one 3-valued and three binary features, three classes, and 24 in-

stances. Since the lenses domain has one non-binary feature, trees with a range of leaf

cardinalities are possible for a particular node cardinality. The minimum-sized tree has

6 nodes and 9 leaves. Separate analyses for leaf and node cardinalities were performed.

We used training set sizes of 8, 12, and 18 for this domain, built all consistent trees, and

measured the error rate on all unseen examples.

r
o
r
r
E

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

0

2

4

6

8

12
Node Cardinality

10

r
o
r
r
E

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

Size = 8
Size = 12
Size = 18

14

16

18

0

2

4

Size = 8
Size = 12
Size = 18

8

6
10
Leaf Cardinality

12

14

16

18

20

Figure 14. Error as a function of node cardinality (left) and error as a function of leaf

cardinality (right).

Figure 14 (left) shows the error as a function of the node cardinality for the 3 training

set sizes averaged over 50 trials. These curves indicate that the smallest consistent trees are

not always the most accurate. When observing the larger node cardinalities for the training

set sizes 12 and 18, error monotonically decreases with increasing node cardinality. Similar

statements can be said for the curve in Figure 14 (right), which relates average error as a

function of leaf cardinality.

Shuttle Landing

The shuttle landing domain has four binary and two 4-valued features, two classes, and 277

instances. The minimum-sized consistent tree has 7 nodes and 14 leaves. We used training

sets of size 20, 50, and 100 for the shuttle domain, generating all consistent decision trees

with fewer than 8, 10, and 12 nodes, and measured the error of these trees on all unseen

examples. Figure 15 presents the error as a function of leaf cardinality, averaged over

273

Murphy & Pazzani

10 trials. For this domain, there is a monotonically increasing relationship between node

cardinality and error.

size = 20
size = 50
size = 100

r
o
r
r
E

0.4

0.3

0.2

0.1

0.0

0

2

4

6

8

10

12

14

Node Cardinality

Figure 15. Error as a function of node cardinality for the Shuttle concept.

References

Blumer, A., Ehrenfeucht, A., Haussler, D., & Warmuth, M. (1987). Occam's razor. Infor-

mation Processing Letters, 24, 377{380.

Breiman, L., Friedman, J., Olshen, R., & Stone, C. (1984). Classi(cid:12)cation and Regression

Trees. Paci(cid:12)c Grove, CA: Wadsworth & Brooks.

Fayyad, U., & Irani, K. (1990). What should be minimized in a decision tree?. In Proceedings

of the Eighth National Conference on Arti(cid:12)cial Intel ligence, AAAI-90.

Fayyad, U., & Irani, K. (1992). The attribute selection problem in decision tree generation..

In Proceedings of the Tenth National Conference on Arti(cid:12)cial Intel ligence, AAAI-92.

Muggleton, S., Srinivasan, A., & Bain, M. (1992). Compression, signi(cid:12)cance and accuracy.

In Machine Learning: Proceedings of the Ninth International Workshop.

Murphy, P., & Aha, D. (1994). UCI Repository of machine learning databases [Machine-

readable data repository]. Irvine, CA: University of California, Department of Infor-

mation and Computer Science.

Pazzani, M. (1990). Creating a memory of causal relationships: An integration of empirical

and explanation-based learning methods. Hillsdale, NJ: Lawrence Erlbaum Associates.

Quinlan, J. (1986). Induction of decision trees. Machine Learning, 1 (1), 81{106.

Quinlan, J. (1993). C4.5 Programs for Machine Learning. San Mateo,CA: Morgan Kauf-

mann.

Quinlan, J., & Rivest, R. (1989). Inferring decision trees using the minimum description

length principle. Information and Computation, 80, 227{248.

274

Exploring the Decision Forest

Rissanen, J. (1978). Modeling by shortest data description. Automatica, 14, 465{471.

Scha(cid:11)er, C. (1992). Sparse data and the e(cid:11)ect of over(cid:12)tting avoidance in decision tree

induction. In Proceedings of the Tenth National Conference on Arti(cid:12)cial Intel ligence,

AAAI-92.

Scha(cid:11)er, C. (1993). Over(cid:12)tting avoidance as bias. Machine Learning, 10 (2), 153{178.

Scha(cid:11)er, C. (1994). A conservation law for generalization performance. Unpublished

Manuscript.

Valiant, L. (1984). A theory of the learnable. Communications of the ACM, 27 (11), 1134{

1142.

Wallace, C., & Patrick, J. (1993). Coding decision trees. Machine Learning, 11 (1), 7{22.

275

","We report on a series of experiments in which all decision trees consistent
with the training data are constructed. These experiments were run to gain an
understanding of the properties of the set of consistent decision trees and the
factors that affect the accuracy of individual trees. In particular, we
investigated the relationship between the size of a decision tree consistent
with some training data and the accuracy of the tree on test data. The
experiments were performed on a massively parallel Maspar computer. The results
of the experiments on several artificial and two real world problems indicate
that, for many of the problems investigated, smaller consistent decision trees
are on average less accurate than the average accuracy of slightly larger
trees."
